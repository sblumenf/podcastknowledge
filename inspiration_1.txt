Okay, here's the transcript of the video:

(0:00) Speaker (Greg Kamradt): I built an AI app that extracted 10,000 insights from one of the biggest business podcasts in the world: My First Million.

(0:07) Shaan Puri (clip): Check this out, Dharmesh. Have you seen this? Go to MFMVault.com.

(0:11) Dharmesh Shah (clip): MFMVault.com.

(0:12) Shaan Puri (clip): My friend Greg is building this. Alright. He was like, I don't just want summaries. I want... he's like, you know, I like MFM because it's got ideas, it's got frameworks, it's got stories. And those are like the big things that he really cared about. And AI is basically extracting those from every episode. It's like searching for frameworks, searching for stories, etc.

(0:29) Speaker (Greg Kamradt): That's Shaan Puri. He runs a podcast with Sam Parr called My First Million. It's loaded with dense content around frameworks, stories, insights, and strategies on how founders made their first million dollars. However, there's a problem. It's within 600 different podcast episodes. It's way too much for one person to go through. So, I built an AI app that extracted all of it. You see, I've led data science teams at Salesforce and I've taught over 10,000 developers how to build AI applications. But this time, I needed a new tool for myself. So I built MFMvault.com. It has AI in nearly every single piece of the product. And I'm going to show you exactly how I built it so you can build your own. And as Sam Altman says, "The best way to learn something is by actually doing the thing in question." So the entire thing was built with AI at its core, using these six AI engineering principles. Now, if you're building AI apps that read unstructured data, you're going to want to watch this video. I also made a lot of mistakes along the way. So if you get to the end of this video, you may end up avoiding some of them yourselves. If you're more of a hands-on learner, I have a doc with all the prompts that I used in this app that I've shared with friends and family. Links in the description. Or if you want more one-on-one attention, I have a few consulting clients that I've stepped through how to build this app. Let's get into it.

(1:42) Speaker (Greg Kamradt): Awesome. Now here's an overview about how the entire application works. Now, we're going to go through the five most important topics. Uh, don't worry, we'll get into these one-on-one. The first place why I want to start is actually on the video side. So when you build an, uh, an unstructured insights extraction application, you need to have a source of truth of your data. Now, that may be podcasts, that may be documents, that may be whatever you want for your specific use case. In this example, I wanted videos to be my source of truth. Now, the reason why I chose this is because, well, videos are visual, but then also is YouTube has really good support for in-context timestamps, and that becomes really important later. So let me show you what this video schema looks like in our database.

(2:19) Speaker (Greg Kamradt): Okay, here we have all the videos that have been extracted from the My First Million YouTube page. Now you can see here we have the video ID, and this matches what is on YouTube itself. But then we also have the title, "How This Billionaire Founder Finds +$20B Business Ideas." Okay, cool. And we have the regular information that comes in from the YouTube, uh, uh, video itself. So we have a description on what it is. We have all the thumbnail, um, information, and we also have the duration, so we know how long it is. Now, I noticed that some YouTube, uh, videos that were coming in that they were shorts. You can see this one's only 21 seconds long. So I don't want to process this one, and this is just a regular flag here, but I do want to process full-length episodes.

(2:53) Speaker (Greg Kamradt): Now where I'm getting this data is I'm actually using PyTube for this one. I'm doing just a simple search, and I'm using the My First Million channel ID. So this is where you go get your list of episodes right here. The next important part is how to download the video itself, because I want to actually get the audio that comes with it. So here we have a download video, and we're actually going to use yt-dlp. Now, this is the YouTube downloader from here, and then we're going to be able to extract the audio that comes from there. And that's how you get the source of truth for your videos, which is really nice.

(3:18) Speaker (Greg Kamradt): So now that we have our videos as the source of truth and we have the audio that comes with it, if we're going to understand the text within the video, well, we need to get the transcription that comes with it. So I'm using Deepgram for the transcription for this one, but you can use whatever one you'd like, whether that be Assembly or whether it be from Google itself. There's a lot of different strategies for how to do this here. On the Deepgram side of the house, I just created a quick Deepgram service, and we're going to transcribe the audio. Now, this is going to be with a file path. And so this is a local file, which is the YouTube audio that we've had beforehand. And we're going to read that in, we're going to send over the bytes. And I'm using their Nova 2 model, which is the, um, uh, best and cheapest model that they currently have right now. And I'm doing diarization equals true. This means that they're going to split out the different speakers, which is very important. And then also with dictation and punctuation, so we can get a little bit more, uh, cleaner text out the other end.

(4:02) Speaker (Greg Kamradt): Then if we jump back over to Supabase, you can see that I have a transcriptions table right here. So here we have the video ID, remember that's our source of truth, and we get the actual transcript that comes out of Deepgram from here. So let me make this bigger. Let's go take a look at this thing here. So we get, uh, all the words that were said on the podcast. The important part is that we get a speaker per word. Here's the headline, millionaire, working. Okay, cool. This is all speaker zero, so it's all, it's all the, um, speaker that we would want to look at. Now, if we scroll down to the bottom here, we can see that they start to actually give us some speaker segments as well. So here's all the different segments that come with it. But utilizing the words and knowing what speaker spoke it, we can start to turn these into what I call segments. And that's the next piece we'll look at.

(4:42) Speaker (Greg Kamradt): All right, the next piece I want to talk about is segments. This is down here in the middle here. Now, segments are going to be individual speaking sections that a single speaker had. So it could be a monologue, for example, or it could be in response to something. Just think of it as like what a sentence says.

(4:56) Speaker (Greg Kamradt): Here's an example of what one looks like. And so it's Shaan saying, "OKRs is somewhat similar to it, which is like a goal-setting system." And if we were to reload this, we can see that it snaps right to when he talks about it. (Audio clip plays: "OKRs is somewhat similar to it...") And we can talk about how those timestamps come out from there. Now, there's a lot of really important parts to make a good hydrated segment. And when I say hydrated segment, I mean we want to add information to it to make it a little bit easier for a human to understand. Now, number one is each segment is going to have a title. And this is using AI to look at the different segment and give it a title. Now, if we come out over here, now we have a separate table that is the segments table. Now, the reason why we have an entirely new table for this is, well, I wanted each segment to have its own page that we could then pull up. So on the segments table, not only do we have video ID, but we also get the start time and the end time. This comes out of Deepgram from the transcription, which is really nice. Then what we get is we get the text that comes from Deepgram. You can see here it's not capitalized and there's no punctuation in here. That's because that's how it comes out of Deepgram in the first place. However, the first thing that we do is we use an LLM to make it a display text is what I call it. And this is what we'd actually show the end user. So you can see here there's a little bit of punctuation, there's a period and the 'a' is capitalized. And so it's just a little bit more easy to read. And we actually send that through, uh, an LLM, uh, to, uh, tell it to clean it up for us. Now, the interesting thing about this too is Dwarkesh, uh, who runs the Dwarkesh podcast, he said, he said some of the same needs. So he shared his own, um, uh, prompt to improve transcriptions as well. If you want my prompt, or you want the link to Dwarkesh's, uh, links in the description. The other thing that each segment has is a title. So we ask an LLM, hey, if you were to give a title to this segment, what would it actually be? So, "Sales Work versus Content Work," "Live Startup Investment Calls," "Financial Goals and Earnings Comparison," "This Kid's Going Places." Nice, that's great. And these are good titles for it. And again, if we go back to My First Million, this is where the title actually comes through. So it just makes for a rich process. Now, it's really awesome is there are over, I think, 90,000 different segments. And so these are 90,000 different speaking, uh, slots that, um, are on My First Million that we can now link and reference in context in place in case you wanted to share something with one of your friends.

(6:55) Speaker (Greg Kamradt): So that is not only segments, but that's also segment hydration, which is when you get the title and the format that comes with it. The next thing I want to talk about is search, and let's look at a quick demo. So now that we're on My First Million right here, let's try to search for something. Um, making money. Wow, so you see how quick that search was? Well, that is because we're using Meilisearch in the background, and they're a very fast search index. So here, you can see all the different segments of where somebody said "making money." Or if we wanted to find all the instances of "small boy stuff," then you can go and get that. "No small boy stuff." Now, what's interesting is because we have the segment timestamp from Deepgram for this, what we can do is we can search not only for when somebody said this, but then we can go and hear it. So if we click this little speaker icon, (Audio clip plays: "no small boy stuff...") So he says "no small boy stuff." Let's look at another one. (Audio clip plays: "no small boy stuff...") Wow. So, okay, cool. So now we're searching all these different YouTube videos. So here's another one with Shaan, "no small boy stuff, dude." Let's see what he says. (Audio clip plays: "no small boy stuff, dude...") So, it's interesting. You can see that we can now search for this. And what's awesome is, let me see if we can pull this out here. There are over 10,000 extracted insights and segments. So, we'll get into these insights in a second here, but you can also search across insights. And then the other nice thing about Meilisearch is they give you different filters to, uh, click through. So you can filter by episode or filter by speaker. So, say we wanted to see when Andrew Wilkinson talks about the stock market. Let's see what he says. "Asking a billionaire." All right, let's open this up. And there's Andrew Wilkinson talking about the stock market, which is nice and interesting for us. That's on the search side, and that's run by, uh, Meilisearch. They do a really awesome job of bringing this all together. And in fact, this is just hosted right on Railway. And in fact, let me show you that. So here's my Railway instance, which is where this entire app is deployed. And they had actually a Meilisearch template, which is really nice. So what I was able to do is just spin up a new template and then all of a sudden I'm self-hosting Meilisearch in about five minutes. Really easy. You grab an API key and you start uploading documents and you're good to go.

(8:40) Speaker (Greg Kamradt): Now, the last thing that I want to talk about with My First Million is one of my favorite parts. I think it's one of the coolest. And this is the finding the insights that come from each podcast episode. So each podcast episode is going to be loaded with frameworks and stories and products and, uh, quotes said by Sam and Shaan and their guests on My First Million. But we want to extract those. I don't want to read the entire, let's call it five-minute monologue. I just want to have a dense summary of what the topic was talked about. So it could be a business idea, or it could be the time that Shaan went home to Thanksgiving and he had some like business, uh, uh, revelation or something like that, who knows. And so we want to ask AI to go through each episode and look at it.

(9:16) Speaker (Greg Kamradt): Now, a good starting point for this is actually the prompt that we're going to be using. So I said, "Please read all the following podcast transcript and extract all the key insights discussed. Organize them into the following categories." And we talk about these different frameworks that came from here. Now, I actually use meta-prompting to even find these categories in the first place. That means that I did a pre-prompt exercise going over to Claude and asking, "Hey, what do you think that I should, or what are the categories you think that I should extract from this podcast?" It gives me those, I put a little human flavor on top of it, and we get the output that we see here. Now, one tip that I had for this is if you put in the entire podcast, you're not going to get that many extracted insights. And that's because really the language model has the context length limit, and it's not going to do a great job. So actually, what I choose to do is chunk up the individual transcript, and then I run this prompt over every single chunk. Then I do a de-duplication exercise because you're going to sometimes find overlap and go between there, so you want to de-duplicate them. Then we get our final list of insights. Let's see what that looks like.

(10:07) Speaker (Greg Kamradt): So now we're on our insights page. And again, we have our video ID, that's a source of truth. And we have an insight type of a quote. Now, this quote is going to be when someone on the show recites a quote from somebody they admire, or it's generally somebody they admire or something they said or something they want to, uh, bring up. Now, the person who said this is going to be Sam Parr, that's the person ID. And then here's the, uh, and then the context is the quote that was actually shared. But let's take this ID and let's actually go see what it says on the front end first. All right, let me paste in that ID just right up at the top here. And so "Money is just a way of keeping score." Let's see what Sam says about this. Let me first zoom in. So that's Sam reciting a quote from someone else, which is really cool. So, money's just a way of keeping score, we have that quote up at the top here. And if we, and if we take a look back here, there's the quote, and then we have some metadata around it, who said the actual quote, and when was it updated and, you know, here's a title for that quote because of course all good quotes are going to have titles and, um, then we have embeddings for, uh, doing vector search a little bit later for there. Now, there's a lot of different types of insights that can come from here. So quote is just one of them. But here's one where Alex Chung said a story, or Sam Parr said a product. Now, these product ones are kind of interesting because if we take a look at this, let's go look at this example here. In fact, let's just go to products section over here. This is on the website. So, coop.farm. In this podcast segment, Sam actually brought up coop.farm. Let's see what this one says. There we go, coop.farm. So a product was mentioned in the podcast itself. And then what what we did here is we actually, uh, extracted that mention of it, but then I used, uh, either I used both Perplexity and Exa in order to extract the product information from there. So we can actually link out to it. So, the smart chicken coop by coop.farm. Let's go and click on this. And here it is. Shop Coop, coop.farm, there's a new coop in town. All right. So, now we have extra metadata about this in the first place. And again, this was all AI generated, which is really nice. So now we have deep linking to the actual products themselves in case people want to go check them out.

(11:51) Speaker (Greg Kamradt): All right, the last thing I want to show you is actually the timestamps. I think it's actually very important. So, let's go back to, uh, Sam's quote around, um, money is just a way of keeping score. Well, what we can do is we can ask the LLM, "Hey, LLM, when was this quote said in the podcast?" Then we pass it the transcript, and with the transcript has timestamps in it, we can see that the start time is 808 and we can see the end time is 814. And these are actually seconds. So then what's really interesting is we can pass this 808 to the URL that we pass over to YouTube for the embedded player, and then all of a sudden you've got, you've jump start right to where the video starts playing itself. So let's do another example on here. So here, I want to find one where Shaan Puri is telling a story. Let's see what he's telling a story about. A discussion about Vladimir Putin's surprise rise from an unremarkable career. Maybe let's, let's do this one. I like Elad Gil. So he's talking about Naval, which is really cool. So I'm going to copy this one, and I'm going to go over to My First Million again, and I'm going to go to Stories. Let me paste in that, uh, ID. So he's talking about Naval, and it jumps right to the spot, which is 20:03. Now, let me show you how this works here. Awesome. So what we have here is, now we have the front-end code for that, uh, for that story page. And here we have a video player. Now, this video player has a start time and end time. Let me actually show you where what this looks like real quick. So, now here we're going to actually have the URL that we, uh, embed the YouTube video in. So we have our video ID, which makes sense, that's the episode it was. But then here's the cool part, we have the start time and we have the end time plus two seconds because I figured you want to make sure you leave some buffer on the end for that. So with the start time and end time, we can then go and, um, uh, bring the user right to that point in the video, and they can go see it in context themselves.

(13:27) Speaker (Greg Kamradt): Now, the last thing I want to talk about is actually a low-key really cool feature. So, I wanted to understand and make recommendations around other types of content that the user should go look at as they're looking at this piece, because if they're interested in this one, maybe they want to look at other ones. So here we have a related insights tab. So here we're talking about Naval's career model. Well, maybe you want to look at Naval's leverage. Let's go look at that one real quick. So here's Codie Sanchez talking about Naval. So now we've just connected two different clips across two different episodes for semantically similar content. Now, how do we do this? Do we use a vector store? Well, not really. So as you're seeing, I was using, uh, Supabase as my, uh, database. And if we look back at each insight over here, let me show you, each insight actually has an embedding. So you can see that we have a whole bunch of different, uh, material here. Now, we want to, uh, measure which two embeddings are most closely similar related to each other. Now, when you're using, uh, Postgres, you can just use PGVector and throw this in here right now. And I did that using a custom function within Supabase. Let me show you that. So over here we want to go to database, and then we want to go to functions. And I have a bunch of different functions right here, but here's, uh, uh, find similar insights. And so let's just edit the function. Let me show you what this looks like. So we're actually going to bring this out here. So we have a query embedding. We're going to select the embedding in into query embeddings. Okay, cool. And then we want to return, then we're basically just doing a similarity metric between the two, which is really nice. So, on the fly, I can call this custom function, and then Supabase will go get me the similar insights that come from there. Let me show you what this looks like on the Python side. Here we have the get related insights function. Now, this is going to be using Supabase, like I was mentioning. And we're going to use the RPC, uh, uh, function that they have. And the function name is going to be "find_similar_insights," which matched what I looked at before. And there's a few parameters, which is n equals 5, so go get me five of them. And then here's the insight ID, that's the subject one, that's when the ones we want to find, uh, that's, um, closest to it. Then if there's data, go give me the data and return done if not.

(15:11) Speaker (Greg Kamradt): Now, the last thing that I'll say is all this different information is available via API as well, in case you want to use it for your own apps. However, the API is a paid support. So if you're building a business generation, uh, product, you're going to want a lot of information that comes from My First Million. Or if you want to have more stories or anything like that. Now, uh, if anybody has any questions about this, feel free to contact me, you can come get it. Here we have My First Million Vault or MFM Vault. It's going to be ideas, frameworks, and strategies for My First Million fans.

(15:37) Speaker (Greg Kamradt): Now, that is a very quick overview of MFM Vault, and really we are just scratching the surface. But if you want to get some of those prompts that I used, I shared those with friends and family already, and those the link for that is in the description. And also, if you want me to actually handhold you through building your own version of MFM Vault, and if you want me to save you, like, literally weeks of work of mistakes that I've already made, maybe it's you or your engineering team, I'm happy to tell you about my lessons learned and if you contact me, we can set up a scoping call to figure it out. All right, crew. Well, I enjoyed this. I hope you enjoyed it as well. We will see you later. Bye.