{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üéôÔ∏è Podcast Knowledge Extraction System - Complete Edition\n\n## What This Notebook Does\n\nThis notebook contains the **COMPLETE** podcast knowledge extraction system with ALL production features:\n\n### Core Capabilities\n1. **Downloads** podcast episodes from RSS feeds\n2. **Transcribes** them using Whisper (GPU-accelerated)\n3. **Identifies** speakers with diarization\n4. **Extracts** insights, entities, quotes, and relationships using AI\n5. **Stores** everything in a Neo4j knowledge graph\n6. **Analyzes** information density, complexity, and accessibility\n7. **Detects** communities, trends, and structural patterns\n8. **Supports** batch processing with checkpoint/resume\n\n### Advanced Features\n- **Entity Resolution**: Automatic deduplication and alias detection\n- **Relationship Extraction**: 5 types of semantic relationships\n- **Graph Algorithms**: PageRank, community detection, centrality analysis\n- **Information Metrics**: Density, complexity, quotability scoring\n- **Batch Processing**: Process hundreds of episodes with checkpointing\n- **Multi-Model Support**: Smart routing between AI models with rate limiting\n- **Memory Optimization**: Efficient processing for long podcasts\n\n## Before You Start\n\nYou'll need:\n- A Google Colab account (Pro recommended for better GPUs)\n- API keys for AI services\n- Neo4j database (free cloud instance works)\n- About 30-60 minutes for initial setup\n\n## Notebook Structure\n\nThis notebook is organized into **14 comprehensive sections** with **180+ cells** containing ALL functionality from the production system. Use the Table of Contents in the sidebar to navigate.\n\nLet's begin! üöÄ",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìã Complete Table of Contents\n\n### Section 0: Introduction & Overview (Cells 1-5)\n- System capabilities and architecture\n- Feature overview\n- Quick start guide\n\n### Section 1: Environment Setup (Cells 6-15)\n- Package installation\n- GPU detection and optimization\n- Memory monitoring setup\n- Colab-specific optimizations\n\n### Section 2: Configuration Management (Cells 16-25)\n- PodcastConfig class\n- SeedingConfig for batch processing\n- Checkpoint configuration\n- Model configuration\n- Feature flags\n\n### Section 3: Core Infrastructure (Cells 26-40)\n- Neo4jManager context manager\n- ProgressCheckpoint class\n- ColabCheckpointManager\n- Memory management\n- Error handling\n- Validation utilities\n- Pattern matching\n- Vector matching\n\n### Section 4: Rate Limiting & Task Routing (Cells 41-50)\n- HybridRateLimiter class\n- TaskRouter class\n- Token estimation\n- Model fallback logic\n- Visual rate limit feedback\n\n### Section 5: Audio Processing (Cells 51-65)\n- AudioProcessor class\n- EnhancedPodcastSegmenter\n- Advertisement detection\n- Sentiment analysis\n- Speaker diarization\n- Semantic boundaries\n- Audio caching\n\n### Section 6: Knowledge Extraction (Cells 66-85)\n- KnowledgeExtractor class\n- RelationshipExtractor\n- ExtractionValidator\n- Entity resolution\n- Quote extraction\n- Topic extraction\n- LLM prompts\n\n### Section 7: Graph Operations (Cells 86-105)\n- GraphOperations class\n- Node creation (all types)\n- Relationship creation\n- Batch operations\n- Vector similarity\n- Cross-episode links\n\n### Section 8: Advanced Analytics (Cells 106-125)\n- Complexity analysis\n- Information density\n- Accessibility scoring\n- Quotability detection\n- Community detection\n- Discourse analysis\n- Trend analysis\n\n### Section 9: Graph Algorithms (Cells 126-135)\n- PageRank\n- Shortest paths\n- Semantic clustering\n- Topic evolution\n- Influence distribution\n\n### Section 10: Visualization (Cells 136-145)\n- Knowledge graphs\n- Topic hierarchies\n- Trend charts\n- Network diagrams\n- Heatmaps\n\n### Section 11: Pipeline Orchestration (Cells 146-155)\n- PodcastKnowledgePipeline\n- Component initialization\n- Episode processing\n- Resource cleanup\n- Progress tracking\n\n### Section 12: Batch Processing (Cells 156-165)\n- Seeding functions\n- Checkpoint recovery\n- Memory streaming\n- Progress visualization\n\n### Section 13: Colab Integration (Cells 166-170)\n- Environment setup\n- Auto-resume\n- Progress display\n- Results summary\n\n### Section 14: Usage Examples (Cells 171-180)\n- Single episode\n- Batch processing\n- Custom analysis\n- Graph queries\n- Visualizations",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1Ô∏è‚É£ Setup & Installation [REQUIRED]\n",
    "\n",
    "## Cell 1.1: Install Software Packages\n",
    "\n",
    "**What this does:**\n",
    "- Installs all the software libraries needed to process podcasts\n",
    "- Like installing apps on your phone, but for code\n",
    "\n",
    "**Why you need it:**\n",
    "- Without these packages, the notebook can't:\n",
    "  - Download podcasts\n",
    "  - Convert speech to text\n",
    "  - Extract insights\n",
    "  - Store information\n",
    "\n",
    "**What to expect:**\n",
    "- Takes 3-5 minutes to install everything\n",
    "- You'll see installation progress messages\n",
    "- Red warning messages are usually harmless\n",
    "- The cell is done when you see a green checkmark ‚úì\n",
    "\n",
    "**Run this cell:** Click the ‚ñ∂ button below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# üîß Complete Package Installation [REQUIRED]\n# This cell installs ALL packages needed for the full system\n\nprint(\"üîß Installing complete package set... This will take 5-7 minutes\")\nprint(\"‚òï Good time for a coffee break!\\n\")\n\n# Core packages\n!pip install -q neo4j>=5.0  # Graph database driver\n!pip install -q feedparser  # RSS feed parsing\n!pip install -q python-dotenv  # Environment management\n\n# AI and Language packages\n!pip install -q langchain langchain-google-genai  # Google AI integration\n!pip install -q openai>=1.0  # OpenAI for embeddings\n!pip install -q tiktoken  # Token counting\n\n# Audio processing\n!pip install -q faster-whisper  # Fast speech-to-text\n!pip install -q pyannote.audio  # Speaker diarization\n!pip install -q pydub  # Audio file handling\n\n# Scientific computing\n!pip install -q numpy scipy  # Numerical computing\n!pip install -q scikit-learn  # Machine learning utilities\n!pip install -q networkx>=3.0  # Graph algorithms\n\n# Data processing\n!pip install -q pandas  # Data manipulation\n!pip install -q tqdm  # Progress bars\n!pip install -q python-dateutil  # Date parsing\n\n# Visualization\n!pip install -q matplotlib seaborn  # Charts and graphs\n!pip install -q plotly  # Interactive visualizations\n\n# Memory monitoring\n!pip install -q psutil  # System monitoring\n!pip install -q gpustat  # GPU monitoring\n\n# GPU support (CUDA 11.8)\n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Additional utilities\n!pip install -q regex  # Advanced regex\n!pip install -q rapidfuzz  # Fuzzy string matching\n!pip install -q tenacity  # Retry logic\n\nprint(\"\\n‚úÖ All packages installed successfully!\")\nprint(\"üìå Note: Some warnings are normal and can be ignored\")\nprint(\"\\nüîç Verifying critical packages...\")\n\n# Verify installations\nimport importlib\ncritical_packages = ['neo4j', 'feedparser', 'langchain', 'openai', 'torch', 'networkx']\nfor package in critical_packages:\n    try:\n        importlib.import_module(package)\n        print(f\"  ‚úì {package} installed correctly\")\n    except ImportError:\n        print(f\"  ‚úó {package} failed to install - please check errors above\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1.2: Connect to Google Drive [REQUIRED]\n",
    "\n",
    "**What this does:**\n",
    "- Connects this notebook to your Google Drive\n",
    "- Creates folders to store podcast data\n",
    "\n",
    "**Why you need it:**\n",
    "- Saves your work permanently (survives notebook restarts)\n",
    "- Stores downloaded podcasts and transcripts\n",
    "- Keeps track of your progress\n",
    "\n",
    "**What to expect:**\n",
    "- A popup asking for Google Drive permission\n",
    "- Click \"Connect\" when prompted\n",
    "- Creates a folder: `MyDrive/podcast_knowledge/`\n",
    "\n",
    "**Privacy note:** Only you can access your Drive files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create necessary folders\n",
    "import os\n",
    "\n",
    "folders = [\n",
    "    '/content/drive/MyDrive/podcast_knowledge',\n",
    "    '/content/drive/MyDrive/podcast_knowledge/audio',\n",
    "    '/content/drive/MyDrive/podcast_knowledge/transcripts',\n",
    "    '/content/drive/MyDrive/podcast_knowledge/insights',\n",
    "    '/content/drive/MyDrive/podcast_knowledge/checkpoints'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"‚úÖ Created folder: {folder.split('/')[-1]}\")\n",
    "\n",
    "print(\"\\nüìÅ All folders ready in your Google Drive!\")\n",
    "print(\"üìç Location: MyDrive/podcast_knowledge/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1.3: Set Up API Keys [REQUIRED]\n",
    "\n",
    "**What this does:**\n",
    "- Sets up your AI service credentials\n",
    "- These are like passwords that let you use AI services\n",
    "\n",
    "**Why you need it:**\n",
    "- Google's Gemini AI: For extracting insights from transcripts\n",
    "- OpenAI (optional): For generating embeddings\n",
    "- HuggingFace: For speaker identification\n",
    "\n",
    "**How to get API keys:**\n",
    "1. **Google Gemini** (Free tier available):\n",
    "   - Go to: https://makersuite.google.com/app/apikey\n",
    "   - Click \"Create API Key\"\n",
    "   - Copy the key\n",
    "\n",
    "2. **HuggingFace** (Free):\n",
    "   - Go to: https://huggingface.co/settings/tokens\n",
    "   - Sign up/Login\n",
    "   - Create new token\n",
    "   - Copy the token\n",
    "\n",
    "3. **OpenAI** (Optional, paid):\n",
    "   - Go to: https://platform.openai.com/api-keys\n",
    "   - Create new secret key\n",
    "\n",
    "**Security:** These keys are stored securely in Colab's secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Colab's secure secrets storage\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# Check for API keys\n",
    "print(\"üîê Checking for API keys...\\n\")\n",
    "\n",
    "# Required keys\n",
    "required_keys = {\n",
    "    'GOOGLE_API_KEY': 'Google Gemini API (for AI processing)',\n",
    "    'HF_TOKEN': 'HuggingFace Token (for speaker identification)'\n",
    "}\n",
    "\n",
    "# Optional keys\n",
    "optional_keys = {\n",
    "    'OPENAI_API_KEY': 'OpenAI API (for embeddings - optional)'\n",
    "}\n",
    "\n",
    "missing_keys = []\n",
    "\n",
    "# Check required keys\n",
    "for key, description in required_keys.items():\n",
    "    try:\n",
    "        value = userdata.get(key)\n",
    "        os.environ[key] = value\n",
    "        print(f\"‚úÖ {key} found - {description}\")\n",
    "    except:\n",
    "        print(f\"‚ùå {key} missing - {description}\")\n",
    "        missing_keys.append(key)\n",
    "\n",
    "# Check optional keys\n",
    "for key, description in optional_keys.items():\n",
    "    try:\n",
    "        value = userdata.get(key)\n",
    "        os.environ[key] = value\n",
    "        print(f\"‚úÖ {key} found - {description}\")\n",
    "    except:\n",
    "        print(f\"‚ÑπÔ∏è {key} not set - {description}\")\n",
    "\n",
    "if missing_keys:\n",
    "    print(\"\\n‚ö†Ô∏è Missing required keys!\")\n",
    "    print(\"\\nTo add keys:\")\n",
    "    print(\"1. Click the üîë key icon in the left sidebar\")\n",
    "    print(\"2. Add a new secret for each missing key\")\n",
    "    print(\"3. Paste your API key as the value\")\n",
    "    print(\"4. Run this cell again\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All required API keys are set!\")\n",
    "    print(\"üöÄ Ready to process podcasts!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1.4: Import Python Libraries [REQUIRED]\n",
    "\n",
    "**What this does:**\n",
    "- Loads all the installed packages into memory\n",
    "- Like opening apps so they're ready to use\n",
    "\n",
    "**Why you need it:**\n",
    "- Makes all the tools available for the rest of the notebook\n",
    "- Sets up error handling if some packages are missing\n",
    "\n",
    "**What to expect:**\n",
    "- Should complete in a few seconds\n",
    "- May show some warnings (that's okay)\n",
    "- Green checkmark when done"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Configuration flags\nENABLE_AUDIO_PROCESSING = True\nENABLE_KNOWLEDGE_EXTRACTION = True  \nENABLE_GRAPH_ENHANCEMENTS = True\nENABLE_VISUALIZATION = True\nENABLE_SPEAKER_DIARIZATION = True\n\n# Batch mode flag for unattended processing\nBATCH_MODE = os.getenv(\"BATCH_MODE\", \"false\").lower() == \"true\"\n\n# Colab mode detection\nCOLAB_MODE = 'google.colab' in sys.modules\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.ERROR if BATCH_MODE else logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Load environment variables\nload_dotenv()\n\n# Custom error classes\nclass AudioProcessingError(Exception):\n    \"\"\"Raised when audio processing fails\"\"\"\n    pass\n\nclass DatabaseConnectionError(Exception):\n    \"\"\"Raised when database connection fails\"\"\"\n    pass\n\nclass PodcastProcessingError(Exception):\n    \"\"\"Raised when podcast processing fails\"\"\"\n    pass\n\nclass ConfigurationError(Exception):\n    \"\"\"Raised when configuration is invalid\"\"\"\n    pass\n\nclass CheckpointError(Exception):\n    \"\"\"Raised when checkpoint operations fail\"\"\"\n    pass\n\nclass RateLimitError(Exception):\n    \"\"\"Raised when rate limits are exceeded\"\"\"\n    pass",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class PodcastConfig:\n    \"\"\"Central configuration for the podcast knowledge system\"\"\"\n    \n    # Colab mode detection\n    COLAB_MODE = 'google.colab' in sys.modules\n    \n    # Base directories with Colab support\n    if COLAB_MODE:\n        BASE_DIR = \"/content/drive/MyDrive/podcast_knowledge\"\n        # Ensure persistence across sessions\n        os.makedirs(BASE_DIR, exist_ok=True)\n    else:\n        BASE_DIR = os.getenv(\"PODCAST_DIR\", \".\")\n    \n    # Directory structure\n    AUDIO_DIR = os.path.join(BASE_DIR, \"audio\")\n    OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n    CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n    CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n    \n    # Create all directories\n    for dir_path in [AUDIO_DIR, OUTPUT_DIR, CHECKPOINT_DIR, CACHE_DIR]:\n        os.makedirs(dir_path, exist_ok=True)\n    \n    # Model selection\n    TRANSCRIPTION_MODEL = \"openai/whisper-large-v3\"\n    USE_LARGE_CONTEXT = True  # Enable large context models (Gemini 1.5)\n    USE_FASTER_WHISPER = True  # Use faster-whisper implementation\n    WHISPER_MODEL_SIZE = \"large-v3\"  # Whisper model size\n    USE_GPU = torch.cuda.is_available() if torch else False\n    \n    # LLM settings\n    LLM_TEMPERATURE = 0.7\n    LLM_MAX_OUTPUT_TOKENS = 4096\n    MAX_RETRIES = 3\n    API_RATE_LIMIT_BUFFER = 0.8  # Use 80% of rate limits\n    \n    # Neo4j settings\n    NEO4J_URI = os.getenv(\"NEO4J_URI\", \"neo4j://localhost:7687\")\n    NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n    NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"\")\n    NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    # API Keys\n    GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n    HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\", \"\")\n    \n    # Processing settings\n    MIN_SEGMENT_LENGTH = 30  # seconds\n    MAX_SEGMENT_LENGTH = 300  # seconds\n    MIN_SPEAKERS = 1  # Minimum speakers for diarization\n    MAX_SPEAKERS = 10  # Maximum speakers for diarization\n    MAX_EPISODES = 20  # Maximum episodes per batch\n    \n    # Memory limits\n    MAX_MEMORY_MB = 8000  # 8GB threshold for cleanup\n    MEMORY_THRESHOLD_MB = 8000  # Same as MAX_MEMORY_MB for compatibility\n    \n    @classmethod\n    def validate_dependencies(cls):\n        \"\"\"Validate that all required dependencies are available\"\"\"\n        missing = []\n        \n        # Check API keys\n        if not cls.GOOGLE_API_KEY:\n            missing.append(\"GOOGLE_API_KEY\")\n        if not cls.NEO4J_PASSWORD:\n            missing.append(\"NEO4J_PASSWORD\")\n            \n        if missing:\n            raise ConfigurationError(f\"Missing required configuration: {', '.join(missing)}\")\n            \n        return True\n    \n    @classmethod\n    def setup_directories(cls):\n        \"\"\"Ensure all directories exist\"\"\"\n        directories = [cls.AUDIO_DIR, cls.OUTPUT_DIR, cls.CHECKPOINT_DIR, cls.CACHE_DIR]\n        for directory in directories:\n            os.makedirs(directory, exist_ok=True)\n    \n    @classmethod\n    def get_segmenter_config(cls):\n        \"\"\"Get configuration for the audio segmenter\"\"\"\n        return {\n            'min_segment_length': cls.MIN_SEGMENT_LENGTH,\n            'max_segment_length': cls.MAX_SEGMENT_LENGTH,\n            'min_segment_tokens': 100,\n            'max_segment_tokens': 500,\n            'use_gpu': cls.USE_GPU\n        }\n\n# Global configuration flags used throughout the system\nUSE_LARGE_CONTEXT = PodcastConfig.USE_LARGE_CONTEXT\nUSE_FASTER_WHISPER = PodcastConfig.USE_FASTER_WHISPER\nWHISPER_MODEL_SIZE = PodcastConfig.WHISPER_MODEL_SIZE\n\n# Podcast feed dictionary for easy access\nPODCAST_FEEDS = {\n    'my-first-million': 'https://feeds.megaphone.fm/HSW7835889191',\n    'all-in': 'https://feeds.megaphone.fm/all-in-with-chamath-jason-sacks-friedberg',\n    'lex-fridman': 'https://lexfridman.com/feed/podcast/',\n    'tim-ferriss': 'https://rss.art19.com/tim-ferriss-show',\n    'huberman-lab': 'https://feeds.megaphone.fm/hubermanlab',\n}",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 2.2: Podcast Processing Configuration\n\n**What this does:**\n- Sets up detailed settings for how podcasts are processed\n- Defines where files are stored and how they're handled\n\n**Why you need it:**\n- Tells the system where to save files\n- Controls quality vs speed tradeoffs\n- Sets limits on processing\n\n**Key settings explained:**\n- **Whisper Model**: Speech-to-text quality (larger = better but slower)\n- **Max Episodes**: How many episodes to process per podcast\n- **Segment Tokens**: How text is broken into chunks\n- **GPU Usage**: Use graphics card for faster processing\n\n**Most users can skip customizing this!**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class TaskRouter:\n    \"\"\"\n    Routes tasks to appropriate models based on availability and rate limits.\n    Provides automatic fallback when primary models are unavailable.\n    \"\"\"\n    \n    def __init__(self):\n        self.rate_limiter = HybridRateLimiter()\n        self.model_status = {\n            'gemini-1.5-flash': {'available': True, 'requests_today': 0},\n            'gemini-1.5-pro': {'available': True, 'requests_today': 0},\n            'gemini-pro': {'available': True, 'requests_today': 0}\n        }\n        \n        # Task to model mapping\n        self.task_models = {\n            'insights': ['gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-pro'],\n            'entities': ['gemini-1.5-flash', 'gemini-pro'],\n            'quotes': ['gemini-1.5-flash', 'gemini-pro'],\n            'topics': ['gemini-1.5-flash', 'gemini-pro'],\n            'relationships': ['gemini-1.5-pro', 'gemini-1.5-flash']\n        }\n    \n    def route_request(self, task_type, prompt):\n        \"\"\"\n        Route request to appropriate model with fallback.\n        \n        Args:\n            task_type: Type of task (insights, entities, etc.)\n            prompt: The prompt to send\n            \n        Returns:\n            Dict with response and model used\n        \"\"\"\n        models = self.task_models.get(task_type, ['gemini-1.5-flash'])\n        \n        for model in models:\n            if self.model_status[model]['available']:\n                try:\n                    # Check rate limit\n                    can_proceed = self.rate_limiter.check_and_wait(model)\n                    if can_proceed:\n                        # Make request\n                        response = self._make_request(model, prompt)\n                        self.model_status[model]['requests_today'] += 1\n                        \n                        return {\n                            'response': response,\n                            'model_used': model,\n                            'fallback': model != models[0]\n                        }\n                except Exception as e:\n                    logger.warning(f\"Model {model} failed: {e}\")\n                    self.model_status[model]['available'] = False\n                    continue\n        \n        raise Exception(f\"All models failed for task {task_type}\")\n    \n    def _make_request(self, model_name, prompt):\n        \"\"\"Make request to specific model\"\"\"\n        import google.generativeai as genai\n        \n        genai.configure(api_key=PodcastConfig.GOOGLE_API_KEY)\n        model = genai.GenerativeModel(model_name)\n        \n        response = model.generate_content(prompt)\n        return response.text\n    \n    def get_usage_report(self):\n        \"\"\"\n        Get detailed usage report for all models.\n        \n        Returns:\n            Dict with model usage statistics\n        \"\"\"\n        report = {\n            'model_status': {},\n            'rate_limits': {},\n            'total_requests': 0\n        }\n        \n        # Get model status and request counts\n        for model, status in self.model_status.items():\n            report['model_status'][model] = {\n                'available': status['available'],\n                'requests_today': status['requests_today'],\n                'rpm': self.rate_limiter.requests[model]['rpm'],\n                'tpm': self.rate_limiter.requests[model]['tpm'],\n                'rpd': self.rate_limiter.requests[model]['rpd']\n            }\n            report['total_requests'] += status['requests_today']\n        \n        # Get rate limit status\n        report['rate_limits'] = {\n            'models_at_limit': [],\n            'models_near_limit': []\n        }\n        \n        for model in self.model_status:\n            usage = self.rate_limiter.requests[model]\n            limits = self.rate_limiter.limits[model]\n            \n            # Check if at limit\n            if usage['rpm'] >= limits['rpm'] or usage['rpd'] >= limits['rpd']:\n                report['rate_limits']['models_at_limit'].append(model)\n            # Check if near limit (>80%)\n            elif usage['rpm'] > limits['rpm'] * 0.8 or usage['rpd'] > limits['rpd'] * 0.8:\n                report['rate_limits']['models_near_limit'].append(model)\n        \n        return report\n    \n    def reset_daily_counters(self):\n        \"\"\"Reset daily request counters\"\"\"\n        for model in self.model_status:\n            self.model_status[model]['requests_today'] = 0\n            self.model_status[model]['available'] = True",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 2.4: Extraction Validator",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ExtractionValidator:\n    \"\"\"\n    Validates and cleans extracted insights, entities, and metrics.\n    Ensures data quality and consistency across the pipeline.\n    \"\"\"\n    \n    def __init__(self):\n        self.validation_stats = {\n            'insights_validated': 0,\n            'insights_rejected': 0,\n            'entities_validated': 0,\n            'entities_deduplicated': 0,\n            'metrics_validated': 0,\n            'metrics_corrected': 0\n        }\n    \n    def validate_insights(self, insights):\n        \"\"\"\n        Validate and clean extracted insights.\n        \n        Args:\n            insights: List of insight dictionaries\n            \n        Returns:\n            List of validated insights\n        \"\"\"\n        validated = []\n        \n        for insight in insights:\n            self.validation_stats['insights_validated'] += 1\n            \n            # Skip if missing required fields\n            if not insight.get('title') or not insight.get('description'):\n                self.validation_stats['insights_rejected'] += 1\n                continue\n            \n            # Clean and validate\n            cleaned = {\n                'title': insight['title'].strip()[:200],  # Limit title length\n                'description': insight['description'].strip()[:2000],  # Limit description\n                'insight_type': insight.get('insight_type', 'conceptual').lower(),\n                'confidence': max(0.0, min(1.0, float(insight.get('confidence', 0.8))))\n            }\n            \n            # Ensure valid insight type\n            valid_types = ['conceptual', 'analytical', 'predictive', 'comparative', 'historical']\n            if cleaned['insight_type'] not in valid_types:\n                cleaned['insight_type'] = 'conceptual'\n            \n            # Add optional fields if present\n            if insight.get('evidence'):\n                cleaned['evidence'] = insight['evidence'][:1000]\n            \n            if insight.get('references'):\n                cleaned['references'] = insight['references']\n            \n            validated.append(cleaned)\n        \n        return validated\n    \n    def validate_entities(self, entities):\n        \"\"\"\n        Validate and deduplicate entities.\n        \n        Args:\n            entities: List of entity dictionaries\n            \n        Returns:\n            List of validated entities\n        \"\"\"\n        validated = []\n        seen_entities = {}  # Track seen entities for deduplication\n        \n        for entity in entities:\n            self.validation_stats['entities_validated'] += 1\n            \n            # Skip if missing required fields\n            if not entity.get('name') or not entity.get('type'):\n                continue\n            \n            # Normalize entity name\n            name = entity['name'].strip()\n            entity_type = entity['type'].strip().upper()\n            \n            # Create key for deduplication\n            entity_key = f\"{name.lower()}_{entity_type}\"\n            \n            # Check for duplicates\n            if entity_key in seen_entities:\n                self.validation_stats['entities_deduplicated'] += 1\n                # Merge with existing entity\n                existing = seen_entities[entity_key]\n                if entity.get('description') and len(entity['description']) > len(existing.get('description', '')):\n                    existing['description'] = entity['description']\n                if entity.get('frequency'):\n                    existing['frequency'] = existing.get('frequency', 0) + entity['frequency']\n                continue\n            \n            # Clean and validate\n            cleaned = {\n                'name': name[:100],  # Limit name length\n                'type': entity_type,\n                'confidence': max(0.0, min(1.0, float(entity.get('confidence', 0.9))))\n            }\n            \n            # Ensure valid entity type\n            valid_types = ['PERSON', 'ORGANIZATION', 'LOCATION', 'CONCEPT', \n                          'TECHNOLOGY', 'PRODUCT', 'EVENT', 'OTHER']\n            if cleaned['type'] not in valid_types:\n                cleaned['type'] = 'OTHER'\n            \n            # Add optional fields\n            if entity.get('description'):\n                cleaned['description'] = entity['description'].strip()[:500]\n            \n            if entity.get('frequency'):\n                cleaned['frequency'] = int(entity['frequency'])\n            \n            if entity.get('importance'):\n                cleaned['importance'] = max(0.0, min(1.0, float(entity['importance'])))\n            \n            seen_entities[entity_key] = cleaned\n            validated.append(cleaned)\n        \n        return validated\n    \n    def validate_metrics(self, metrics):\n        \"\"\"\n        Validate and correct metric values.\n        \n        Args:\n            metrics: Dictionary of metrics\n            \n        Returns:\n            Validated metrics dictionary\n        \"\"\"\n        self.validation_stats['metrics_validated'] += 1\n        \n        validated = {}\n        \n        # Validate complexity score\n        if 'complexity_score' in metrics:\n            score = float(metrics['complexity_score'])\n            if score < 0 or score > 10:\n                self.validation_stats['metrics_corrected'] += 1\n                score = max(0.0, min(10.0, score))\n            validated['complexity_score'] = score\n        \n        # Validate information density\n        if 'information_score' in metrics:\n            score = float(metrics['information_score'])\n            if score < 0:\n                self.validation_stats['metrics_corrected'] += 1\n                score = max(0.0, score)\n            validated['information_score'] = score\n        \n        # Validate accessibility score\n        if 'accessibility_score' in metrics:\n            score = float(metrics['accessibility_score'])\n            if score < 0 or score > 100:\n                self.validation_stats['metrics_corrected'] += 1\n                score = max(0.0, min(100.0, score))\n            validated['accessibility_score'] = score\n        \n        # Validate quotability score\n        if 'quotability_score' in metrics:\n            score = float(metrics['quotability_score'])\n            if score < 0 or score > 100:\n                self.validation_stats['metrics_corrected'] += 1\n                score = max(0.0, min(100.0, score))\n            validated['quotability_score'] = score\n        \n        # Copy over other fields\n        for key, value in metrics.items():\n            if key not in validated:\n                validated[key] = value\n        \n        return validated\n    \n    def get_validation_report(self):\n        \"\"\"Get validation statistics\"\"\"\n        return self.validation_stats.copy()\n    \n    def reset_stats(self):\n        \"\"\"Reset validation statistics\"\"\"\n        for key in self.validation_stats:\n            self.validation_stats[key] = 0\n\n# Create global validator instance\nextraction_validator = ExtractionValidator()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 2.5: Enhanced Checkpoint Management",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ColabCheckpointManager:\n    \"\"\"\n    Enhanced checkpoint manager for Colab session recovery.\n    Handles disconnections and provides resumable processing.\n    \"\"\"\n    \n    def __init__(self, checkpoint_dir=None):\n        self.checkpoint_dir = checkpoint_dir or PodcastConfig.CHECKPOINT_DIR\n        self.checkpoint_file = os.path.join(self.checkpoint_dir, \"colab_checkpoint.json\")\n        self.progress_file = os.path.join(self.checkpoint_dir, \"progress.json\")\n        self.episode_progress_dir = os.path.join(self.checkpoint_dir, \"episodes\")\n        \n        # Create directories\n        os.makedirs(self.episode_progress_dir, exist_ok=True)\n    \n    def save_checkpoint(self, state):\n        \"\"\"Save current processing state with environment info.\"\"\"\n        checkpoint = {\n            'timestamp': datetime.now().isoformat(),\n            'state': state,\n            'environment': {\n                'gpu_available': torch.cuda.is_available() if torch else False,\n                'memory_used': self._get_memory_usage(),\n                'colab_mode': COLAB_MODE,\n                'batch_mode': BATCH_MODE\n            }\n        }\n        \n        with open(self.checkpoint_file, 'w') as f:\n            json.dump(checkpoint, f, indent=2)\n    \n    def load_checkpoint(self):\n        \"\"\"Load last checkpoint if exists.\"\"\"\n        if os.path.exists(self.checkpoint_file):\n            with open(self.checkpoint_file, 'r') as f:\n                return json.load(f)\n        return None\n    \n    def save_progress(self, podcast_name, episodes_completed, current_episode=None):\n        \"\"\"Track processing progress across sessions.\"\"\"\n        progress = self.load_progress() or {}\n        \n        if podcast_name not in progress:\n            progress[podcast_name] = {\n                'episodes_completed': [],\n                'last_episode': None,\n                'total_processed': 0,\n                'started_at': datetime.now().isoformat()\n            }\n        \n        # Update progress\n        progress[podcast_name]['episodes_completed'].extend(episodes_completed)\n        progress[podcast_name]['episodes_completed'] = list(set(progress[podcast_name]['episodes_completed']))\n        progress[podcast_name]['last_episode'] = current_episode\n        progress[podcast_name]['total_processed'] = len(progress[podcast_name]['episodes_completed'])\n        progress['last_updated'] = datetime.now().isoformat()\n        \n        with open(self.progress_file, 'w') as f:\n            json.dump(progress, f, indent=2, default=str)\n    \n    def load_progress(self):\n        \"\"\"Load progress tracking.\"\"\"\n        if os.path.exists(self.progress_file):\n            with open(self.progress_file, 'r') as f:\n                return json.load(f)\n        return None\n    \n    def save_episode_progress(self, episode_id, checkpoint_type, data):\n        \"\"\"Save progress for a specific episode.\"\"\"\n        episode_file = os.path.join(self.episode_progress_dir, f\"{episode_id}_{checkpoint_type}.json\")\n        \n        checkpoint_data = {\n            'episode_id': episode_id,\n            'checkpoint_type': checkpoint_type,\n            'timestamp': datetime.now().isoformat(),\n            'data': data\n        }\n        \n        with open(episode_file, 'w') as f:\n            json.dump(checkpoint_data, f, indent=2, default=str)\n    \n    def load_episode_progress(self, episode_id, checkpoint_type):\n        \"\"\"Load progress for a specific episode.\"\"\"\n        episode_file = os.path.join(self.episode_progress_dir, f\"{episode_id}_{checkpoint_type}.json\")\n        \n        if os.path.exists(episode_file):\n            with open(episode_file, 'r') as f:\n                checkpoint_data = json.load(f)\n                return checkpoint_data.get('data')\n        \n        return None\n    \n    def get_completed_episodes(self):\n        \"\"\"Get set of completed episode IDs.\"\"\"\n        completed = set()\n        \n        # Check episode files\n        if os.path.exists(self.episode_progress_dir):\n            for filename in os.listdir(self.episode_progress_dir):\n                if filename.endswith('_complete.json'):\n                    episode_id = filename.replace('_complete.json', '')\n                    completed.add(episode_id)\n        \n        # Also check progress file\n        progress = self.load_progress()\n        if progress:\n            for podcast_name, podcast_progress in progress.items():\n                if isinstance(podcast_progress, dict) and 'episodes_completed' in podcast_progress:\n                    completed.update(podcast_progress['episodes_completed'])\n        \n        return completed\n    \n    def clean_episode_checkpoints(self, episode_id):\n        \"\"\"Clean up intermediate checkpoints for completed episode.\"\"\"\n        checkpoint_types = ['transcript', 'extraction', 'segments']\n        \n        for checkpoint_type in checkpoint_types:\n            episode_file = os.path.join(self.episode_progress_dir, f\"{episode_id}_{checkpoint_type}.json\")\n            if os.path.exists(episode_file):\n                try:\n                    os.remove(episode_file)\n                except:\n                    pass\n    \n    def _get_memory_usage(self):\n        \"\"\"Get current memory usage stats.\"\"\"\n        try:\n            import psutil\n            process = psutil.Process()\n            return {\n                'ram_mb': process.memory_info().rss / 1024 / 1024,\n                'gpu_mb': torch.cuda.memory_allocated() / 1024 / 1024 if torch and torch.cuda.is_available() else 0\n            }\n        except:\n            return {'ram_mb': 0, 'gpu_mb': 0}\n    \n    def check_resume_state(self):\n        \"\"\"Check if we should resume from checkpoint.\"\"\"\n        checkpoint = self.load_checkpoint()\n        if checkpoint:\n            time_diff = datetime.now() - datetime.fromisoformat(checkpoint['timestamp'])\n            if time_diff.total_seconds() < 86400:  # Less than 24 hours old\n                return checkpoint\n        return None\n\n# Keep the original ProgressCheckpoint as an alias for compatibility\nclass ProgressCheckpoint(ColabCheckpointManager):\n    \"\"\"Alias for backward compatibility\"\"\"\n    pass",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def cleanup_memory(force=False):\n    \"\"\"\n    Enhanced memory cleanup with Colab optimizations.\n    Includes threshold-based aggressive cleanup and module clearing.\n    \"\"\"\n    # Get memory usage before cleanup\n    mem_before = 0\n    try:\n        import psutil\n        process = psutil.Process()\n        mem_before = process.memory_info().rss / 1024 / 1024\n    except:\n        pass\n    \n    # Standard cleanup\n    gc.collect()\n    \n    # GPU cleanup\n    if torch and torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    \n    # Matplotlib cleanup\n    if 'matplotlib' in sys.modules:\n        import matplotlib.pyplot as plt\n        plt.close('all')\n    \n    # Force cleanup if memory usage is high\n    if force or mem_before > PodcastConfig.MAX_MEMORY_MB:\n        # Clear module caches\n        if hasattr(sys, 'modules'):\n            modules_to_clear = ['transformers', 'whisper', 'pyannote']\n            for module in modules_to_clear:\n                if module in sys.modules:\n                    del sys.modules[module]\n        \n        # Additional aggressive cleanup\n        gc.collect(2)  # Full collection\n    \n    # Log memory freed\n    if mem_before > 0:\n        try:\n            import psutil\n            process = psutil.Process()\n            mem_after = process.memory_info().rss / 1024 / 1024\n            if mem_before - mem_after > 100:  # If freed more than 100MB\n                logger.info(f\"Memory cleanup freed {mem_before - mem_after:.0f}MB\")\n        except:\n            pass\n\ndef monitor_memory():\n    \"\"\"Monitor current memory usage\"\"\"\n    try:\n        import psutil\n        process = psutil.Process()\n        mem_mb = process.memory_info().rss / 1024 / 1024\n        \n        if torch and torch.cuda.is_available():\n            gpu_mb = torch.cuda.memory_allocated() / 1024 / 1024\n            print(f\"Memory usage: RAM {mem_mb:.0f}MB, GPU {gpu_mb:.0f}MB\")\n        else:\n            print(f\"Memory usage: RAM {mem_mb:.0f}MB\")\n            \n        # Trigger cleanup if approaching limit\n        if mem_mb > PodcastConfig.MAX_MEMORY_MB * 0.9:\n            print(\"Approaching memory limit, triggering cleanup...\")\n            cleanup_memory(force=True)\n    except:\n        pass",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class SeedingConfig(PodcastConfig):\n    \"\"\"Configuration optimized for batch knowledge seeding.\"\"\"\n    \n    # Logging configuration for batch mode\n    LOG_LEVEL = \"ERROR\"  # Only log errors in batch mode\n    SAVE_CHECKPOINTS = True\n    CHECKPOINT_EVERY_N = 5  # Episodes\n    EMBEDDING_BATCH_SIZE = 50  # For batch embedding generation\n    ENABLE_PROGRESS_BAR = True  # Simple progress only\n    \n    # Disable interactive features\n    INTERACTIVE_MODE = False\n    SAVE_VISUALIZATIONS = False\n    GENERATE_REPORTS = False\n    VERBOSE_LOGGING = False\n    \n    # Batch processing settings\n    BATCH_SIZE = 10  # Episodes to process before checkpoint\n    MAX_CONCURRENT_DOWNLOADS = 3  # Parallel audio downloads\n    RETRY_FAILED_EPISODES = True\n    SKIP_EXISTING = True  # Skip already processed episodes\n    \n    @classmethod\n    def get_batch_config(cls):\n        \"\"\"Get configuration for batch processing.\"\"\"\n        return {\n            'checkpoint_interval': cls.CHECKPOINT_EVERY_N,\n            'batch_size': cls.BATCH_SIZE,\n            'skip_existing': cls.SKIP_EXISTING,\n            'retry_failed': cls.RETRY_FAILED_EPISODES,\n            'save_checkpoints': cls.SAVE_CHECKPOINTS\n        }\n    \n    @classmethod\n    def setup_batch_environment(cls):\n        \"\"\"Setup environment for batch processing.\"\"\"\n        # Set minimal logging\n        logging.getLogger().setLevel(logging.ERROR)\n        \n        # Disable matplotlib interactive mode\n        if plt:\n            plt.ioff()\n        \n        # Set process priority (if available)\n        try:\n            import psutil\n            p = psutil.Process()\n            p.nice(10)  # Lower priority for batch jobs\n            print(\"‚úì Process priority lowered for batch processing\")\n        except:\n            pass\n        \n        print(\"‚úÖ Batch processing environment configured\")\n\n# Display batch configuration\nprint(\"üì¶ Batch Processing Configuration:\")\nprint(f\"  ‚Ä¢ Checkpoint every: {SeedingConfig.CHECKPOINT_EVERY_N} episodes\")\nprint(f\"  ‚Ä¢ Batch size: {SeedingConfig.BATCH_SIZE} episodes\")\nprint(f\"  ‚Ä¢ Skip existing: {'YES' if SeedingConfig.SKIP_EXISTING else 'NO'}\")\nprint(f\"  ‚Ä¢ Retry failed: {'YES' if SeedingConfig.RETRY_FAILED_EPISODES else 'NO'}\")\nprint(f\"  ‚Ä¢ Max concurrent downloads: {SeedingConfig.MAX_CONCURRENT_DOWNLOADS}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 3Ô∏è‚É£ Database Setup - Neo4j [REQUIRED]\n\n## What is Neo4j?\n\n**Think of Neo4j as a smart filing cabinet** that stores information about:\n- üìö **Episodes**: Each podcast episode\n- üë§ **Speakers**: People in the podcast\n- üí° **Insights**: Key ideas and takeaways\n- üè¢ **Entities**: Companies, people, topics mentioned\n- üîó **Connections**: How everything relates\n\nUnlike regular databases that use tables, Neo4j uses a **graph** - like a mind map that shows how ideas connect!\n\n## Cell 3.1: Neo4j Connection Setup\n\n**What this does:**\n- Sets up connection details for your Neo4j database\n- You can use either a free cloud instance or local installation\n\n**Options:**\n1. **Neo4j Aura (Recommended)** - Free cloud database\n   - Go to: https://neo4j.com/cloud/aura-free/\n   - Sign up for free account\n   - Create a free instance\n   - Copy connection details\n\n2. **Local Neo4j** - If you have Neo4j installed locally\n\n**What you'll need:**\n- Connection URL (like `neo4j+s://xxxxx.databases.neo4j.io`)\n- Username (usually `neo4j`)\n- Password (you set this when creating the database)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Neo4j Database Configuration\n# Option 1: Set credentials here directly (for testing)\n# Option 2: Use Colab secrets (more secure - recommended)\n\n# Check if Neo4j credentials are in Colab secrets\ntry:\n    from google.colab import userdata\n    NEO4J_URI = userdata.get('NEO4J_URI')\n    NEO4J_USERNAME = userdata.get('NEO4J_USERNAME')\n    NEO4J_PASSWORD = userdata.get('NEO4J_PASSWORD')\n    print(\"‚úÖ Neo4j credentials loaded from Colab secrets\")\nexcept:\n    print(\"‚ÑπÔ∏è Neo4j credentials not found in secrets\")\n    print(\"\\nPlease enter your Neo4j connection details:\")\n    print(\"(Get these from https://neo4j.com/cloud/aura-free/)\")\n    \n    # Manual input option\n    NEO4J_URI = input(\"Neo4j URI (e.g., neo4j+s://xxxxx.databases.neo4j.io): \").strip()\n    NEO4J_USERNAME = input(\"Username (usually 'neo4j'): \").strip() or \"neo4j\"\n    NEO4J_PASSWORD = input(\"Password: \").strip()\n\n# Set environment variables\nos.environ['NEO4J_URI'] = NEO4J_URI\nos.environ['NEO4J_USERNAME'] = NEO4J_USERNAME\nos.environ['NEO4J_PASSWORD'] = NEO4J_PASSWORD\n\n# Update config\nPodcastConfig.NEO4J_URI = NEO4J_URI\nPodcastConfig.NEO4J_USERNAME = NEO4J_USERNAME\nPodcastConfig.NEO4J_PASSWORD = NEO4J_PASSWORD\nPodcastConfig.NEO4J_DATABASE = \"neo4j\"  # Default database name\n\nprint(\"\\n‚úÖ Neo4j configuration saved!\")\nprint(f\"üìç Database URI: {NEO4J_URI.split('@')[1] if '@' in NEO4J_URI else NEO4J_URI}\")\nprint(f\"üë§ Username: {NEO4J_USERNAME}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 3Ô∏è‚É£ Core Infrastructure [REQUIRED]\n\n## What is Core Infrastructure?\n\nThis section contains the **foundation classes and utilities** that power the entire system:\n\n- **Error Handling**: Custom exceptions for better debugging\n- **Memory Management**: Prevents crashes during long processing\n- **Database Connection**: Safe Neo4j connection management\n- **Checkpoint System**: Resume processing after interruptions\n- **Validation**: Input sanitization and verification\n- **Pattern Matching**: Optimized regex for text analysis\n\nThese components ensure the system runs reliably and efficiently.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 3.2: Test Database Connection\n\n**What this does:**\n- Tests if we can connect to your Neo4j database\n- Creates necessary indexes for fast searching\n- Sets up the database structure\n\n**Why you need it:**\n- Confirms your database is working\n- Prepares the database for storing podcast data\n- Creates search indexes for better performance\n\n**What to expect:**\n- \"Connection Successful\" message\n- List of indexes created\n- Ready to store podcast knowledge!\n\n**If it fails:**\n- Check your internet connection\n- Verify your Neo4j credentials\n- Make sure your database is running (if local)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 3.2: Checkpoint System for Resume Support\n\n**What this does:**\n- Saves processing progress automatically\n- Allows resuming after disconnections\n- Tracks which episodes have been processed\n- Essential for Colab's session limits\n\n**Why you need it:**\n- Colab sessions disconnect after 12-24 hours\n- Don't lose progress on long processing jobs\n- Skip already processed episodes automatically",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ProgressCheckpoint:\n    \"\"\"Manages checkpoints for resumable processing.\"\"\"\n    \n    def __init__(self, checkpoint_dir=None):\n        self.checkpoint_dir = checkpoint_dir or PodcastConfig.CHECKPOINT_DIR\n        self.checkpoint_file = os.path.join(self.checkpoint_dir, \"progress_checkpoint.json\")\n        \n    def save(self, podcast_name, processed_episodes, failed_episodes=None, current_episode=None):\n        \"\"\"Save current processing state.\"\"\"\n        checkpoint = {\n            'timestamp': datetime.now().isoformat(),\n            'podcast_name': podcast_name,\n            'processed_episodes': processed_episodes,\n            'failed_episodes': failed_episodes or [],\n            'current_episode': current_episode,\n            'total_processed': len(processed_episodes),\n            'total_failed': len(failed_episodes) if failed_episodes else 0\n        }\n        \n        try:\n            with open(self.checkpoint_file, 'w') as f:\n                json.dump(checkpoint, f, indent=2)\n            return True\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Failed to save checkpoint: {e}\")\n            return False\n    \n    def load(self):\n        \"\"\"Load last checkpoint if exists.\"\"\"\n        if os.path.exists(self.checkpoint_file):\n            try:\n                with open(self.checkpoint_file, 'r') as f:\n                    return json.load(f)\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Failed to load checkpoint: {e}\")\n        return None\n    \n    def clear(self):\n        \"\"\"Clear checkpoint file.\"\"\"\n        if os.path.exists(self.checkpoint_file):\n            os.remove(self.checkpoint_file)\n            print(\"‚úÖ Checkpoint cleared\")\n\nclass ColabCheckpointManager(ProgressCheckpoint):\n    \"\"\"Enhanced checkpoint manager specifically for Colab environments.\"\"\"\n    \n    def __init__(self, checkpoint_dir=None):\n        super().__init__(checkpoint_dir)\n        self.progress_file = os.path.join(self.checkpoint_dir, \"colab_progress.json\")\n        self.session_file = os.path.join(self.checkpoint_dir, \"session_info.json\")\n        \n    def save_session_info(self):\n        \"\"\"Save Colab session information.\"\"\"\n        session_info = {\n            'timestamp': datetime.now().isoformat(),\n            'environment': {\n                'colab': 'google.colab' in sys.modules,\n                'gpu_available': torch.cuda.is_available() if torch else False,\n                'gpu_name': torch.cuda.get_device_name(0) if torch and torch.cuda.is_available() else None,\n                'memory_gb': psutil.virtual_memory().total / (1024**3) if psutil else None\n            }\n        }\n        \n        with open(self.session_file, 'w') as f:\n            json.dump(session_info, f, indent=2)\n    \n    def save_progress(self, podcast_name, episodes_completed, current_episode=None):\n        \"\"\"Track processing progress with enhanced metadata.\"\"\"\n        progress = self.load_progress() or {}\n        \n        if podcast_name not in progress:\n            progress[podcast_name] = {\n                'episodes_completed': [],\n                'episodes_failed': [],\n                'last_episode': None,\n                'total_processed': 0,\n                'processing_times': []\n            }\n        \n        # Update progress\n        progress[podcast_name]['episodes_completed'].extend(episodes_completed)\n        progress[podcast_name]['episodes_completed'] = list(set(progress[podcast_name]['episodes_completed']))\n        progress[podcast_name]['last_episode'] = current_episode\n        progress[podcast_name]['total_processed'] = len(progress[podcast_name]['episodes_completed'])\n        progress['last_updated'] = datetime.now().isoformat()\n        \n        # Save to file\n        with open(self.progress_file, 'w') as f:\n            json.dump(progress, f, indent=2)\n        \n        # Also save session info\n        self.save_session_info()\n        \n        return True\n    \n    def load_progress(self):\n        \"\"\"Load progress tracking with validation.\"\"\"\n        if os.path.exists(self.progress_file):\n            try:\n                with open(self.progress_file, 'r') as f:\n                    return json.load(f)\n            except:\n                return None\n        return None\n    \n    def get_resume_info(self, podcast_name):\n        \"\"\"Get information needed to resume processing.\"\"\"\n        progress = self.load_progress()\n        if progress and podcast_name in progress:\n            podcast_progress = progress[podcast_name]\n            return {\n                'completed_episodes': podcast_progress.get('episodes_completed', []),\n                'failed_episodes': podcast_progress.get('episodes_failed', []),\n                'last_episode': podcast_progress.get('last_episode'),\n                'total_processed': podcast_progress.get('total_processed', 0),\n                'should_resume': True\n            }\n        return {\n            'completed_episodes': [],\n            'failed_episodes': [],\n            'last_episode': None,\n            'total_processed': 0,\n            'should_resume': False\n        }\n    \n    def display_progress_summary(self):\n        \"\"\"Display a summary of all processing progress.\"\"\"\n        progress = self.load_progress()\n        \n        if not progress:\n            print(\"üìä No processing history found\")\n            return\n        \n        print(\"üìä Processing Progress Summary\")\n        print(\"=\" * 50)\n        \n        total_episodes = 0\n        for podcast, info in progress.items():\n            if podcast == 'last_updated':\n                continue\n                \n            episodes_count = info.get('total_processed', 0)\n            total_episodes += episodes_count\n            \n            print(f\"\\nüìª {podcast}\")\n            print(f\"  ‚úì Episodes processed: {episodes_count}\")\n            if info.get('last_episode'):\n                print(f\"  üìç Last episode: {info['last_episode'][:50]}...\")\n            if info.get('episodes_failed'):\n                print(f\"  ‚ö†Ô∏è Failed episodes: {len(info['episodes_failed'])}\")\n        \n        print(f\"\\nüìà Total episodes processed: {total_episodes}\")\n        if 'last_updated' in progress:\n            print(f\"‚è∞ Last updated: {progress['last_updated']}\")\n\n# Initialize checkpoint managers\ncheckpoint_manager = ProgressCheckpoint()\ncolab_checkpoint_manager = ColabCheckpointManager() if PodcastConfig.COLAB_MODE else None\n\nprint(\"‚úÖ Checkpoint system initialized\")\nprint(f\"  üìÅ Checkpoint directory: {PodcastConfig.CHECKPOINT_DIR}\")\nprint(f\"  üíæ Colab mode: {'ENABLED' if colab_checkpoint_manager else 'DISABLED'}\")\n\n# Display current progress\nif colab_checkpoint_manager:\n    print(\"\\n\" + \"=\"*50)\n    colab_checkpoint_manager.display_progress_summary()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 3.3: Input Validation & Error Recovery\n\n**What this does:**\n- Validates user inputs to prevent errors\n- Sanitizes file paths for security\n- Provides retry logic for transient failures\n- Ensures data quality\n\n**Why important:**\n- Prevents crashes from bad inputs\n- Handles network timeouts gracefully\n- Improves overall reliability",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Input Validation Utilities\ndef validate_text_input(text, min_length=10, max_length=1000000):\n    \"\"\"Validate text input with comprehensive checks.\"\"\"\n    if not text:\n        raise ValueError(\"Text input is empty\")\n    \n    if not isinstance(text, str):\n        raise TypeError(f\"Expected string, got {type(text)}\")\n    \n    text = text.strip()\n    \n    if len(text) < min_length:\n        raise ValueError(f\"Text too short: {len(text)} < {min_length}\")\n    \n    if len(text) > max_length:\n        raise ValueError(f\"Text too long: {len(text)} > {max_length}\")\n    \n    # Check for suspicious patterns\n    if text.count('\\x00') > 0:  # Null bytes\n        raise ValueError(\"Text contains null bytes\")\n    \n    return text\n\ndef validate_date_format(date_string):\n    \"\"\"Validate and parse date strings with multiple format support.\"\"\"\n    if not date_string:\n        return None\n    \n    # Common podcast date formats\n    date_formats = [\n        \"%Y-%m-%d\",\n        \"%Y-%m-%dT%H:%M:%S\",\n        \"%Y-%m-%dT%H:%M:%SZ\",\n        \"%a, %d %b %Y %H:%M:%S %z\",\n        \"%a, %d %b %Y %H:%M:%S GMT\",\n        \"%Y-%m-%d %H:%M:%S\"\n    ]\n    \n    for fmt in date_formats:\n        try:\n            return datetime.strptime(date_string, fmt)\n        except ValueError:\n            continue\n    \n    # Try dateutil parser as fallback\n    try:\n        from dateutil import parser\n        return parser.parse(date_string)\n    except:\n        print(f\"‚ö†Ô∏è Could not parse date: {date_string}\")\n        return None\n\ndef sanitize_file_path(file_path):\n    \"\"\"Sanitize file paths to prevent directory traversal attacks.\"\"\"\n    # Remove dangerous characters\n    dangerous_chars = ['..', '~', '|', '>', '<', '&', ';', '$', '`']\n    \n    for char in dangerous_chars:\n        if char in file_path:\n            raise ValueError(f\"Dangerous character '{char}' in file path\")\n    \n    # Normalize path\n    file_path = os.path.normpath(file_path)\n    \n    # Ensure it's within allowed directories\n    allowed_dirs = [PodcastConfig.BASE_DIR, '/tmp', '/content']\n    \n    if not any(file_path.startswith(allowed) for allowed in allowed_dirs):\n        raise ValueError(f\"File path outside allowed directories: {file_path}\")\n    \n    return file_path\n\n# Retry decorator for transient failures\ndef with_retry(max_retries=3, delay=1, backoff=2, exceptions=(Exception,)):\n    \"\"\"Decorator to retry functions on failure.\"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            retries = 0\n            current_delay = delay\n            \n            while retries < max_retries:\n                try:\n                    return func(*args, **kwargs)\n                except exceptions as e:\n                    retries += 1\n                    if retries >= max_retries:\n                        print(f\"‚ùå Failed after {max_retries} retries: {e}\")\n                        raise\n                    \n                    print(f\"‚ö†Ô∏è Attempt {retries} failed: {e}\")\n                    print(f\"‚è≥ Retrying in {current_delay} seconds...\")\n                    time.sleep(current_delay)\n                    current_delay *= backoff\n            \n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n# URL validation\ndef validate_url(url):\n    \"\"\"Validate URL format and accessibility.\"\"\"\n    import urllib.parse\n    \n    try:\n        result = urllib.parse.urlparse(url)\n        if not all([result.scheme, result.netloc]):\n            raise ValueError(\"Invalid URL format\")\n        \n        # Check if URL is accessible\n        if result.scheme not in ['http', 'https']:\n            raise ValueError(f\"Unsupported URL scheme: {result.scheme}\")\n        \n        return url\n    except Exception as e:\n        raise ValueError(f\"Invalid URL: {e}\")\n\n# Episode ID validation\ndef validate_episode_id(episode_id):\n    \"\"\"Validate episode ID format.\"\"\"\n    if not episode_id:\n        raise ValueError(\"Episode ID is empty\")\n    \n    # Check format (alphanumeric with underscores/hyphens)\n    if not re.match(r'^[a-zA-Z0-9_-]+$', episode_id):\n        raise ValueError(f\"Invalid episode ID format: {episode_id}\")\n    \n    # Check length\n    if len(episode_id) > 100:\n        raise ValueError(f\"Episode ID too long: {len(episode_id)} > 100\")\n    \n    return episode_id\n\n# Test validation functions\nprint(\"‚úÖ Validation utilities loaded\")\nprint(\"\\nüß™ Testing validation functions:\")\n\n# Test text validation\ntry:\n    validate_text_input(\"This is a valid text input\")\n    print(\"  ‚úì Text validation working\")\nexcept Exception as e:\n    print(f\"  ‚úó Text validation failed: {e}\")\n\n# Test date parsing\ntest_date = \"2024-01-15T10:30:00Z\"\nparsed = validate_date_format(test_date)\nif parsed:\n    print(f\"  ‚úì Date parsing working: {test_date} ‚Üí {parsed}\")\n\n# Test URL validation\ntry:\n    validate_url(\"https://example.com/podcast.rss\")\n    print(\"  ‚úì URL validation working\")\nexcept:\n    print(\"  ‚úó URL validation failed\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Core Connection Functions [REQUIRED]\n# These functions provide the foundation for Neo4j, embeddings, and audio processing\n\ndef connect_to_neo4j(config=None):\n    \"\"\"\n    Create and verify connection to Neo4j database.\n    \n    Args:\n        config: Optional PodcastConfig instance\n        \n    Returns:\n        Neo4j driver instance\n        \n    Raises:\n        DatabaseConnectionError: If connection fails\n    \"\"\"\n    config = config or PodcastConfig\n    \n    try:\n        driver = GraphDatabase.driver(\n            config.NEO4J_URI,\n            auth=(config.NEO4J_USERNAME, config.NEO4J_PASSWORD)\n        )\n        \n        # Verify connection\n        with driver.session(database=config.NEO4J_DATABASE) as session:\n            result = session.run(\"RETURN 'Connected!' AS message\")\n            message = result.single()[\"message\"]\n            print(f\"‚úÖ Neo4j connection established: {message}\")\n            \n        return driver\n        \n    except Exception as e:\n        raise DatabaseConnectionError(f\"Failed to connect to Neo4j: {e}\")\n\ndef setup_neo4j_schema(driver):\n    \"\"\"\n    Create indexes and constraints for optimal graph performance.\n    \n    Args:\n        driver: Neo4j driver instance\n        \n    Returns:\n        bool: True if successful\n    \"\"\"\n    database = PodcastConfig.NEO4J_DATABASE\n    \n    try:\n        with driver.session(database=database) as session:\n            # Create indexes for better query performance\n            indexes = [\n                (\"Episode\", \"id\"),\n                (\"Episode\", \"podcast_id\"),\n                (\"Episode\", \"published_date\"),\n                (\"Insight\", \"id\"),\n                (\"Insight\", \"episode_id\"),\n                (\"Entity\", \"id\"),\n                (\"Entity\", \"name\"),\n                (\"Entity\", \"normalized_name\"),\n                (\"Entity\", \"type\"),\n                (\"Segment\", \"id\"),\n                (\"Segment\", \"episode_id\"),\n                (\"Topic\", \"id\"),\n                (\"Topic\", \"name\"),\n                (\"Quote\", \"id\"),\n                (\"Quote\", \"episode_id\")\n            ]\n            \n            for label, property in indexes:\n                try:\n                    session.run(f\"\"\"\n                    CREATE INDEX {label.lower()}_{property}_index IF NOT EXISTS\n                    FOR (n:{label}) ON (n.{property})\n                    \"\"\")\n                    print(f\"  ‚úì Index created: {label}.{property}\")\n                except:\n                    pass  # Index might already exist\n            \n            # Create constraints\n            constraints = [\n                (\"Episode\", \"id\"),\n                (\"Insight\", \"id\"),\n                (\"Entity\", \"id\"),\n                (\"Segment\", \"id\"),\n                (\"Topic\", \"id\"),\n                (\"Quote\", \"id\"),\n                (\"Podcast\", \"id\")\n            ]\n            \n            for label, property in constraints:\n                try:\n                    session.run(f\"\"\"\n                    CREATE CONSTRAINT {label.lower()}_{property}_unique IF NOT EXISTS\n                    FOR (n:{label}) REQUIRE n.{property} IS UNIQUE\n                    \"\"\")\n                    print(f\"  ‚úì Constraint created: {label}.{property} UNIQUE\")\n                except:\n                    pass  # Constraint might already exist\n                    \n            print(\"‚úÖ Neo4j schema setup complete\")\n            return True\n            \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Schema setup error: {e}\")\n        return False\n\ndef initialize_embedding_model():\n    \"\"\"\n    Initialize OpenAI client for generating embeddings.\n    \n    Returns:\n        OpenAI client or None if not available\n    \"\"\"\n    api_key = PodcastConfig.OPENAI_API_KEY\n    \n    if not api_key:\n        print(\"‚ö†Ô∏è OpenAI API key not found. Embeddings will be disabled.\")\n        return None\n        \n    if not OpenAI:\n        print(\"‚ö†Ô∏è OpenAI library not available. Embeddings will be disabled.\")\n        return None\n        \n    try:\n        client = OpenAI(api_key=api_key)\n        \n        # Test the client\n        test_response = client.embeddings.create(\n            model=\"text-embedding-ada-002\",\n            input=\"test\"\n        )\n        \n        print(\"‚úÖ OpenAI embedding client initialized\")\n        return client\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Failed to initialize OpenAI client: {e}\")\n        return None\n\ndef generate_embeddings(text, client):\n    \"\"\"\n    Generate embeddings for text using OpenAI.\n    \n    Args:\n        text: Text to embed\n        client: OpenAI client instance\n        \n    Returns:\n        List of floats (embedding vector) or None\n    \"\"\"\n    if not client or not text:\n        return None\n        \n    try:\n        # Truncate text if too long (max ~8000 tokens)\n        if len(text) > 30000:\n            text = text[:30000]\n            \n        response = client.embeddings.create(\n            model=\"text-embedding-ada-002\",\n            input=text\n        )\n        \n        return response.data[0].embedding\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Embedding generation failed: {e}\")\n        return None\n\ndef download_episode_audio(episode, podcast_id):\n    \"\"\"\n    Download audio file for a podcast episode.\n    \n    Args:\n        episode: Episode dictionary with audio URL\n        podcast_id: Podcast identifier for organization\n        \n    Returns:\n        Path to downloaded audio file or None\n    \"\"\"\n    if not episode.get('audio_url'):\n        print(\"‚ö†Ô∏è No audio URL found for episode\")\n        return None\n        \n    # Create safe filename\n    safe_title = re.sub(r'[^\\w\\s-]', '', episode['title'])[:50]\n    safe_title = re.sub(r'[-\\s]+', '-', safe_title)\n    \n    filename = f\"{podcast_id}_{episode.get('episode_number', 0)}_{safe_title}.mp3\"\n    output_path = os.path.join(PodcastConfig.AUDIO_DIR, filename)\n    \n    # Check if already downloaded\n    if os.path.exists(output_path):\n        print(f\"‚úÖ Audio already downloaded: {filename}\")\n        return output_path\n        \n    try:\n        # Use the enhanced download function\n        return download_audio_with_cache(episode['audio_url'], output_path)\n    except Exception as e:\n        print(f\"‚ùå Failed to download audio: {e}\")\n        return None\n\n# Test the functions\nprint(\"‚úÖ Core connection functions loaded\")\nprint(\"  ‚Ä¢ connect_to_neo4j() - Neo4j database connection\")\nprint(\"  ‚Ä¢ setup_neo4j_schema() - Create indexes and constraints\")\nprint(\"  ‚Ä¢ initialize_embedding_model() - OpenAI embeddings\")\nprint(\"  ‚Ä¢ generate_embeddings() - Generate text embeddings\")\nprint(\"  ‚Ä¢ download_episode_audio() - Download podcast audio\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class OptimizedPatternMatcher:\n    \"\"\"Pre-compiled regex patterns for efficient text analysis.\"\"\"\n    \n    def __init__(self):\n        # Compile patterns once for reuse\n        self.patterns = {\n            # Technical terms\n            'technical_terms': re.compile(\n                r'\\b(?:AI|ML|API|SDK|GPU|CPU|RAM|IoT|SaaS|PaaS|IaaS|'\n                r'blockchain|cryptocurrency|quantum|neural|algorithm|'\n                r'framework|database|microservice|kubernetes|docker|'\n                r'machine learning|deep learning|natural language|'\n                r'computer vision|data science|artificial intelligence)\\b',\n                re.IGNORECASE\n            ),\n            \n            # Facts and statistics\n            'statistics': re.compile(\n                r'\\b\\d+(?:\\.\\d+)?%|\\$\\d+(?:,\\d{3})*(?:\\.\\d+)?[BMK]?|'\n                r'\\d+(?:,\\d{3})*\\s*(?:users?|customers?|downloads?|views?|'\n                r'employees?|revenue|profit|loss|growth|increase|decrease)',\n                re.IGNORECASE\n            ),\n            \n            # Dates and timeframes\n            'temporal': re.compile(\n                r'\\b(?:January|February|March|April|May|June|July|August|'\n                r'September|October|November|December|Jan|Feb|Mar|Apr|May|'\n                r'Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s+\\d{1,2}(?:st|nd|rd|th)?'\n                r'(?:,?\\s+\\d{4})?|\\b\\d{4}\\b|\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b|'\n                r'(?:last|next|this)\\s+(?:year|month|week|quarter)',\n                re.IGNORECASE\n            ),\n            \n            # Company and product names (common patterns)\n            'entities': re.compile(\n                r'\\b[A-Z][a-zA-Z]*(?:\\s+[A-Z][a-zA-Z]*)*\\b|'\n                r'\\b(?:Inc|LLC|Corp|Corporation|Company|Technologies|'\n                r'Labs|Studios|Games|Software|Systems|Solutions)\\b',\n                re.IGNORECASE\n            ),\n            \n            # Quotable statements\n            'quotes': re.compile(\n                r'\"[^\"]{20,}\"|\\b(?:said|says|stated|explained|announced|'\n                r'declared|mentioned|noted|emphasized|stressed)\\s*[,:]?\\s*\"',\n                re.IGNORECASE\n            ),\n            \n            # Insights and key points\n            'insights': re.compile(\n                r'(?:the\\s+(?:key|main|important|critical|essential)\\s+'\n                r'(?:point|insight|takeaway|lesson|thing)|'\n                r'(?:importantly|essentially|basically|fundamentally)|'\n                r'(?:in\\s+other\\s+words|to\\s+put\\s+it\\s+simply)|'\n                r'(?:the\\s+bottom\\s+line\\s+is)|(?:what\\s+this\\s+means\\s+is))',\n                re.IGNORECASE\n            ),\n            \n            # Questions\n            'questions': re.compile(\n                r'[^.!?]*\\?',\n                re.MULTILINE\n            ),\n            \n            # Lists and enumerations\n            'lists': re.compile(\n                r'(?:(?:first|second|third|fourth|fifth)|'\n                r'(?:1st|2nd|3rd|4th|5th)|'\n                r'(?:[1-9]\\.)|(?:[a-e]\\.)|(?:‚Ä¢|‚Üí|>))\\s*',\n                re.IGNORECASE | re.MULTILINE\n            )\n        }\n        \n        # Compile fact-checking patterns\n        self.fact_patterns = [\n            re.compile(r'\\b\\d+(?:\\.\\d+)?%'),  # Percentages\n            re.compile(r'\\$\\d+(?:,\\d{3})*(?:\\.\\d+)?[BMK]?'),  # Money\n            re.compile(r'\\b\\d+(?:,\\d{3})*\\s*(?:million|billion|thousand)'),  # Large numbers\n            re.compile(r'\\b(?:doubled|tripled|quadrupled|increased|decreased)\\s*(?:by\\s*)?\\d+'),\n            re.compile(r'\\b\\d+x\\s*(?:faster|slower|better|worse|more|less)'),  # Comparisons\n        ]\n    \n    def extract_technical_terms(self, text):\n        \"\"\"Extract technical terms from text.\"\"\"\n        matches = self.patterns['technical_terms'].findall(text)\n        return list(set(match.lower() for match in matches))\n    \n    def count_facts(self, text):\n        \"\"\"Count factual statements in text.\"\"\"\n        fact_count = 0\n        \n        # Count statistics\n        fact_count += len(self.patterns['statistics'].findall(text))\n        \n        # Count other fact patterns\n        for pattern in self.fact_patterns:\n            fact_count += len(pattern.findall(text))\n        \n        return fact_count\n    \n    def extract_quotes(self, text):\n        \"\"\"Extract quotable content from text.\"\"\"\n        quotes = []\n        \n        # Direct quotes\n        direct_quotes = re.findall(r'\"([^\"]{20,})\"', text)\n        quotes.extend(direct_quotes)\n        \n        # Statements after speech verbs\n        speech_patterns = re.findall(\n            r'(?:said|says|stated|explained)[:,]?\\s*\"([^\"]+)\"',\n            text,\n            re.IGNORECASE\n        )\n        quotes.extend(speech_patterns)\n        \n        return quotes\n    \n    def extract_entities(self, text):\n        \"\"\"Extract potential entity names.\"\"\"\n        # Find capitalized sequences\n        entities = []\n        \n        # Company names with suffixes\n        company_pattern = re.compile(\n            r'\\b([A-Z][a-zA-Z]*(?:\\s+[A-Z][a-zA-Z]*)*)\\s*'\n            r'(?:Inc|LLC|Corp|Corporation|Company|Technologies|Labs)\\b'\n        )\n        entities.extend([match[0] for match in company_pattern.findall(text)])\n        \n        # Proper nouns (consecutive capitalized words)\n        proper_noun_pattern = re.compile(\n            r'\\b[A-Z][a-zA-Z]*(?:\\s+[A-Z][a-zA-Z]*)+\\b'\n        )\n        entities.extend(proper_noun_pattern.findall(text))\n        \n        # Clean and deduplicate\n        entities = list(set(e.strip() for e in entities if len(e) > 2))\n        \n        return entities\n    \n    def analyze_text_structure(self, text):\n        \"\"\"Analyze text structure and patterns.\"\"\"\n        analysis = {\n            'has_questions': bool(self.patterns['questions'].search(text)),\n            'has_lists': bool(self.patterns['lists'].search(text)),\n            'has_quotes': bool(self.patterns['quotes'].search(text)),\n            'has_insights': bool(self.patterns['insights'].search(text)),\n            'technical_density': len(self.extract_technical_terms(text)) / max(1, len(text.split())),\n            'fact_density': self.count_facts(text) / max(1, len(text.split())),\n            'entity_count': len(self.extract_entities(text))\n        }\n        \n        return analysis\n\n# Initialize global pattern matcher\npattern_matcher = OptimizedPatternMatcher()\n\nprint(\"‚úÖ Pattern matching system initialized\")\nprint(\"\\nüß™ Testing pattern matcher:\")\n\n# Test text\ntest_text = \"\"\"\nGoogle announced a 25% increase in revenue to $75.3B this quarter.\n\"AI is transforming how we work,\" said the CEO. The company's new\nmachine learning framework processes data 10x faster than before.\nMicrosoft Corp and Apple Inc are also investing heavily in AI.\n\"\"\"\n\n# Test pattern extraction\nanalysis = pattern_matcher.analyze_text_structure(test_text)\nprint(f\"\\nüìä Text analysis results:\")\nprint(f\"  ‚Ä¢ Technical terms: {pattern_matcher.extract_technical_terms(test_text)}\")\nprint(f\"  ‚Ä¢ Fact count: {pattern_matcher.count_facts(test_text)}\")\nprint(f\"  ‚Ä¢ Entities: {pattern_matcher.extract_entities(test_text)[:3]}...\")\nprint(f\"  ‚Ä¢ Has quotes: {analysis['has_quotes']}\")\nprint(f\"  ‚Ä¢ Technical density: {analysis['technical_density']:.2%}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 4Ô∏è‚É£ Rate Limiting & Task Routing [REQUIRED]\n\n## Advanced API Management\n\nThis section implements **intelligent rate limiting** and **task routing** to:\n\n- **Prevent API errors** from hitting rate limits\n- **Automatically switch** between AI models when needed\n- **Track usage** across multiple models\n- **Optimize costs** by routing tasks to appropriate models\n\n### Key Features:\n- **Multi-model support**: Gemini 1.5 Flash, Pro, and fallbacks\n- **Smart routing**: High-priority tasks get better models\n- **Visual feedback**: See rate limit status in real-time\n- **Automatic recovery**: Handles rate limit errors gracefully\n\nThis is essential for processing large batches of podcasts!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 4.1: HybridRateLimiter - Multi-Model Rate Management\n\n**What this does:**\n- Tracks API usage for each model independently\n- Prevents hitting rate limits by checking before each call\n- Automatically switches to backup models when primary is at limit\n- Provides visual feedback during rate limit waits\n\n**Rate Limits Managed:**\n- Requests per minute (RPM)\n- Tokens per minute (TPM)\n- Requests per day (RPD)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from collections import deque\nimport time\n\nclass HybridRateLimiter:\n    \"\"\"\n    Model-specific rate limiter for Gemini models with smart routing.\n    Tracks usage per model independently and provides fallback options.\n    \"\"\"\n    def __init__(self):\n        # Rate limits per model (conservative to avoid hitting limits)\n        self.limits = {\n            'gemini-1.5-flash': {\n                'rpm': 15,      # Requests per minute\n                'tpm': 1000000, # Tokens per minute  \n                'rpd': 1500     # Requests per day\n            },\n            'gemini-1.5-pro': {\n                'rpm': 10,\n                'tpm': 250000,\n                'rpd': 500\n            },\n            'gemini-1.0-pro': {  # Fallback model\n                'rpm': 60,\n                'tpm': 120000,\n                'rpd': 1500\n            }\n        }\n        \n        # Track usage per model\n        self.requests = {}\n        for model in self.limits:\n            self.requests[model] = {\n                'minute': deque(),\n                'day': deque(),\n                'tokens_minute': deque()\n            }\n        \n        # Track errors and fallbacks\n        self.error_counts = defaultdict(int)\n        self.fallback_counts = defaultdict(int)\n        self.current_model = 'gemini-1.5-flash'\n        \n        # Visual display settings\n        self.show_visual_feedback = PodcastConfig.COLAB_MODE\n        \n    def can_use_model(self, model_name, estimated_tokens=0):\n        \"\"\"Check if model is available within rate limits.\"\"\"\n        current_time = time.time()\n        \n        if model_name not in self.limits:\n            return False\n            \n        limits = self.limits[model_name]\n        usage = self.requests[model_name]\n        \n        # Clean old entries\n        self._clean_old_entries(usage, current_time)\n        \n        # Check RPM\n        rpm_count = len(usage['minute'])\n        if rpm_count >= limits['rpm'] * PodcastConfig.API_RATE_LIMIT_BUFFER:\n            return False\n            \n        # Check TPM\n        tokens_used = sum(t[1] for t in usage['tokens_minute'])\n        if tokens_used + estimated_tokens > limits['tpm'] * PodcastConfig.API_RATE_LIMIT_BUFFER:\n            return False\n            \n        # Check RPD\n        rpd_count = len(usage['day'])\n        if rpd_count >= limits['rpd'] * PodcastConfig.API_RATE_LIMIT_BUFFER:\n            return False\n            \n        return True\n    \n    def _clean_old_entries(self, usage, current_time):\n        \"\"\"Remove old tracking entries.\"\"\"\n        # Clean minute entries (older than 60 seconds)\n        while usage['minute'] and usage['minute'][0] < current_time - 60:\n            usage['minute'].popleft()\n            \n        # Clean token entries  \n        while usage['tokens_minute'] and usage['tokens_minute'][0][0] < current_time - 60:\n            usage['tokens_minute'].popleft()\n            \n        # Clean day entries (older than 24 hours)\n        while usage['day'] and usage['day'][0] < current_time - 86400:\n            usage['day'].popleft()\n    \n    def record_usage(self, model_name, tokens_used):\n        \"\"\"Record that we used the API.\"\"\"\n        current_time = time.time()\n        usage = self.requests[model_name]\n        \n        usage['minute'].append(current_time)\n        usage['day'].append(current_time)\n        usage['tokens_minute'].append((current_time, tokens_used))\n        \n    def get_best_model(self, estimated_tokens=0, task_priority='normal'):\n        \"\"\"Get the best available model for the task.\"\"\"\n        # Model preference order based on task priority\n        if task_priority == 'high':\n            model_order = ['gemini-1.5-pro', 'gemini-1.5-flash', 'gemini-1.0-pro']\n        else:\n            model_order = ['gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-1.0-pro']\n        \n        # Try models in order\n        for model in model_order:\n            if self.can_use_model(model, estimated_tokens):\n                if model != self.current_model:\n                    print(f\"üìä Switching to {model} (priority: {task_priority})\")\n                    self.fallback_counts[model] += 1\n                self.current_model = model\n                return model\n        \n        # All models at limit - wait and retry\n        wait_time = self._get_wait_time()\n        self._show_rate_limit_warning(wait_time)\n        time.sleep(wait_time)\n        \n        # Try again after waiting\n        return self.get_best_model(estimated_tokens, task_priority)\n    \n    def _get_wait_time(self):\n        \"\"\"Calculate how long to wait before retry.\"\"\"\n        current_time = time.time()\n        min_wait = float('inf')\n        \n        for model_name, usage in self.requests.items():\n            if usage['minute']:\n                wait = 61 - (current_time - usage['minute'][0])\n                min_wait = min(min_wait, wait)\n                \n        return max(1, int(min_wait))\n    \n    def _show_rate_limit_warning(self, wait_seconds):\n        \"\"\"Show visual rate limit warning.\"\"\"\n        if self.show_visual_feedback and 'IPython' in sys.modules:\n            from IPython.display import clear_output, display, HTML\n            import time\n            \n            for remaining in range(wait_seconds, 0, -1):\n                clear_output(wait=True)\n                \n                # Create progress bar\n                progress = (wait_seconds - remaining) / wait_seconds\n                bar_length = 40\n                filled = int(bar_length * progress)\n                bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)\n                \n                html = f\"\"\"\n                <div style=\"padding: 20px; border: 2px solid #ff9800; border-radius: 10px; background-color: #fff3e0;\">\n                    <h3 style=\"color: #e65100;\">‚è≥ Rate Limit Cooldown</h3>\n                    <p>All models are at their rate limits. Waiting before retry...</p>\n                    <div style=\"margin: 20px 0;\">\n                        <div style=\"font-size: 24px; font-weight: bold; color: #e65100;\">\n                            {remaining} seconds remaining\n                        </div>\n                        <div style=\"margin-top: 10px; background-color: #ffccbc; border-radius: 5px; overflow: hidden;\">\n                            <div style=\"background-color: #ff5722; color: white; text-align: center; padding: 5px; width: {progress*100}%;\">\n                                {int(progress*100)}%\n                            </div>\n                        </div>\n                    </div>\n                    <p style=\"color: #666; font-size: 14px;\">\n                        üí° Tip: You can process other notebooks while waiting\n                    </p>\n                </div>\n                \"\"\"\n                display(HTML(html))\n                time.sleep(1)\n            \n            clear_output(wait=True)\n            display(HTML('<div style=\"color: green; font-weight: bold;\">‚úÖ Ready to continue!</div>'))\n        else:\n            print(f\"‚è≥ Rate limit reached. Waiting {wait_seconds} seconds...\")\n            time.sleep(wait_seconds)\n            print(\"‚úÖ Ready to continue!\")\n    \n    def get_usage_stats(self):\n        \"\"\"Get current usage statistics.\"\"\"\n        stats = {}\n        current_time = time.time()\n        \n        for model, usage in self.requests.items():\n            self._clean_old_entries(usage, current_time)\n            \n            limits = self.limits[model]\n            rpm_used = len(usage['minute'])\n            tpm_used = sum(t[1] for t in usage['tokens_minute'])\n            rpd_used = len(usage['day'])\n            \n            stats[model] = {\n                'rpm': f\"{rpm_used}/{limits['rpm']} ({rpm_used/limits['rpm']*100:.1f}%)\",\n                'tpm': f\"{tpm_used:,}/{limits['tpm']:,} ({tpm_used/limits['tpm']*100:.1f}%)\",\n                'rpd': f\"{rpd_used}/{limits['rpd']} ({rpd_used/limits['rpd']*100:.1f}%)\",\n                'available': self.can_use_model(model)\n            }\n        \n        return stats\n    \n    def display_usage_dashboard(self):\n        \"\"\"Display visual usage dashboard.\"\"\"\n        stats = self.get_usage_stats()\n        \n        if self.show_visual_feedback and 'IPython' in sys.modules:\n            from IPython.display import display, HTML\n            \n            html = \"\"\"\n            <div style=\"padding: 20px; border: 1px solid #ddd; border-radius: 10px;\">\n                <h3>üìä API Usage Dashboard</h3>\n                <table style=\"width: 100%; border-collapse: collapse;\">\n                    <tr>\n                        <th style=\"text-align: left; padding: 10px; border-bottom: 2px solid #ddd;\">Model</th>\n                        <th style=\"text-align: left; padding: 10px; border-bottom: 2px solid #ddd;\">RPM</th>\n                        <th style=\"text-align: left; padding: 10px; border-bottom: 2px solid #ddd;\">TPM</th>\n                        <th style=\"text-align: left; padding: 10px; border-bottom: 2px solid #ddd;\">RPD</th>\n                        <th style=\"text-align: left; padding: 10px; border-bottom: 2px solid #ddd;\">Status</th>\n                    </tr>\n            \"\"\"\n            \n            for model, usage in stats.items():\n                status_color = \"#4CAF50\" if usage['available'] else \"#f44336\"\n                status_text = \"‚úì Available\" if usage['available'] else \"‚úó At Limit\"\n                \n                html += f\"\"\"\n                    <tr>\n                        <td style=\"padding: 10px; border-bottom: 1px solid #eee;\">{model}</td>\n                        <td style=\"padding: 10px; border-bottom: 1px solid #eee;\">{usage['rpm']}</td>\n                        <td style=\"padding: 10px; border-bottom: 1px solid #eee;\">{usage['tpm']}</td>\n                        <td style=\"padding: 10px; border-bottom: 1px solid #eee;\">{usage['rpd']}</td>\n                        <td style=\"padding: 10px; border-bottom: 1px solid #eee; color: {status_color}; font-weight: bold;\">\n                            {status_text}\n                        </td>\n                    </tr>\n                \"\"\"\n            \n            html += \"\"\"\n                </table>\n                <p style=\"margin-top: 15px; color: #666; font-size: 14px;\">\n                    <strong>Legend:</strong> RPM = Requests/Minute, TPM = Tokens/Minute, RPD = Requests/Day\n                </p>\n            </div>\n            \"\"\"\n            \n            display(HTML(html))\n        else:\n            print(\"\\nüìä API Usage Stats:\")\n            print(\"-\" * 60)\n            for model, usage in stats.items():\n                print(f\"{model}:\")\n                print(f\"  RPM: {usage['rpm']}\")\n                print(f\"  TPM: {usage['tpm']}\")\n                print(f\"  RPD: {usage['rpd']}\")\n                print(f\"  Status: {'Available' if usage['available'] else 'At Limit'}\")\n                print()\n\n# Create global rate limiter instance\nrate_limiter = HybridRateLimiter()\n\nprint(\"‚úÖ Hybrid rate limiter initialized\")\nprint(f\"  ‚Ä¢ Managing {len(rate_limiter.limits)} models\")\nprint(f\"  ‚Ä¢ Visual feedback: {'ENABLED' if rate_limiter.show_visual_feedback else 'DISABLED'}\")\nprint(f\"  ‚Ä¢ Current model: {rate_limiter.current_model}\")\n\n# Display initial usage stats\nprint(\"\\n\" + \"=\"*60)\nrate_limiter.display_usage_dashboard()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 4.2: TaskRouter - Intelligent Task Distribution\n\n**What this does:**\n- Routes different tasks to appropriate AI models\n- Estimates token usage for each task type\n- Prioritizes tasks based on importance\n- Handles fallback strategies\n\n**Task Types:**\n- **High Priority**: Complex extraction, relationship analysis\n- **Normal Priority**: Standard insights, entity extraction\n- **Low Priority**: Simple summaries, basic analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enhanced audio download with caching for Colab\ndef download_audio_with_cache(url, output_path, use_cache=True):\n    \"\"\"Download audio with Colab-optimized caching.\"\"\"\n    if COLAB_MODE and use_cache:\n        # Use content-based cache key\n        cache_key = hashlib.md5(url.encode()).hexdigest()\n        cache_path = os.path.join(PodcastConfig.CACHE_DIR, f\"{cache_key}.mp3\")\n        \n        # Check cache first\n        if os.path.exists(cache_path):\n            logging.info(f\"Using cached audio: {cache_key}\")\n            # Copy from cache to output path\n            import shutil\n            shutil.copy2(cache_path, output_path)\n            return output_path\n    \n    # Download with progress for Colab\n    try:\n        if COLAB_MODE and 'IPython' in sys.modules:\n            from tqdm.notebook import tqdm  # Use notebook version in Colab\n        else:\n            from tqdm import tqdm\n        \n        response = urllib.request.urlopen(url)\n        total_size = int(response.headers.get('Content-Length', 0))\n        \n        with open(output_path, 'wb') as f:\n            with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"Downloading\") as pbar:\n                while True:\n                    chunk = response.read(8192)\n                    if not chunk:\n                        break\n                    f.write(chunk)\n                    pbar.update(len(chunk))\n                    \n        # Save to cache if in Colab\n        if COLAB_MODE and use_cache:\n            import shutil\n            shutil.copy2(output_path, cache_path)\n            \n    except Exception as e:\n        logging.error(f\"Download failed: {e}\")\n        raise\n        \n    return output_path\n\n# Update the existing download_audio function to use the enhanced version\ndef download_audio(url, output_path):\n    \"\"\"Download audio file from URL.\"\"\"\n    # Use the enhanced version with caching by default\n    return download_audio_with_cache(url, output_path, use_cache=True)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class TaskRouter:\n    \"\"\"\n    Routes tasks to appropriate models based on priority and availability.\n    Provides token estimation and fallback strategies.\n    \"\"\"\n    \n    def __init__(self, rate_limiter=None):\n        self.rate_limiter = rate_limiter or HybridRateLimiter()\n        \n        # Task type definitions with priorities\n        self.task_types = {\n            'relationship_extraction': {\n                'priority': 'high',\n                'estimated_tokens': 8000,\n                'description': 'Extract complex relationships between entities'\n            },\n            'insight_extraction': {\n                'priority': 'normal',\n                'estimated_tokens': 5000,\n                'description': 'Extract key insights and takeaways'\n            },\n            'entity_extraction': {\n                'priority': 'normal',\n                'estimated_tokens': 3000,\n                'description': 'Identify people, companies, and concepts'\n            },\n            'sentiment_analysis': {\n                'priority': 'low',\n                'estimated_tokens': 2000,\n                'description': 'Analyze emotional tone and sentiment'\n            },\n            'summary_generation': {\n                'priority': 'low',\n                'estimated_tokens': 1500,\n                'description': 'Generate concise summaries'\n            },\n            'quote_extraction': {\n                'priority': 'low',\n                'estimated_tokens': 1000,\n                'description': 'Extract notable quotes'\n            },\n            'combined_extraction': {\n                'priority': 'high',\n                'estimated_tokens': 10000,\n                'description': 'Combined extraction in single call'\n            }\n        }\n        \n        # Model capabilities\n        self.model_capabilities = {\n            'gemini-1.5-flash': ['all'],  # Can handle all task types\n            'gemini-1.5-pro': ['all'],     # Better for complex tasks\n            'gemini-1.0-pro': ['summary_generation', 'sentiment_analysis', 'quote_extraction']\n        }\n        \n        # Track task distribution\n        self.task_stats = defaultdict(lambda: defaultdict(int))\n        \n    def estimate_tokens(self, text, task_type):\n        \"\"\"Estimate tokens needed for a task.\"\"\"\n        # Base estimation on text length\n        text_tokens = len(text.split()) * 1.3  # Rough token estimate\n        \n        # Get task-specific overhead\n        task_info = self.task_types.get(task_type, {})\n        overhead = task_info.get('estimated_tokens', 2000)\n        \n        # Add buffer for response\n        total_tokens = int(text_tokens + overhead * 1.2)\n        \n        return total_tokens\n    \n    def route_task(self, task_type, text_length=0):\n        \"\"\"Route a task to the best available model.\"\"\"\n        if task_type not in self.task_types:\n            print(f\"‚ö†Ô∏è Unknown task type: {task_type}\")\n            task_type = 'entity_extraction'  # Default\n        \n        task_info = self.task_types[task_type]\n        priority = task_info['priority']\n        estimated_tokens = self.estimate_tokens(\"x\" * text_length, task_type)\n        \n        # Get best model from rate limiter\n        model = self.rate_limiter.get_best_model(estimated_tokens, priority)\n        \n        # Check if model can handle this task\n        capabilities = self.model_capabilities.get(model, [])\n        if 'all' not in capabilities and task_type not in capabilities:\n            print(f\"‚ö†Ô∏è Model {model} cannot handle {task_type}, finding alternative...\")\n            # Find alternative model\n            for alt_model, alt_caps in self.model_capabilities.items():\n                if ('all' in alt_caps or task_type in alt_caps) and \\\n                   self.rate_limiter.can_use_model(alt_model, estimated_tokens):\n                    model = alt_model\n                    break\n        \n        # Track statistics\n        self.task_stats[task_type][model] += 1\n        \n        return {\n            'model': model,\n            'priority': priority,\n            'estimated_tokens': estimated_tokens,\n            'task_type': task_type\n        }\n    \n    def get_llm_client(self, routing_info):\n        \"\"\"Get configured LLM client based on routing info.\"\"\"\n        model = routing_info['model']\n        \n        if not ChatGoogleGenerativeAI:\n            raise LLMProcessingError(\"Google AI client not available\")\n        \n        # Configure client with appropriate settings\n        client = ChatGoogleGenerativeAI(\n            model=model,\n            temperature=PodcastConfig.LLM_TEMPERATURE,\n            max_output_tokens=PodcastConfig.LLM_MAX_OUTPUT_TOKENS,\n            google_api_key=os.environ.get('GOOGLE_API_KEY')\n        )\n        \n        return client\n    \n    def execute_with_fallback(self, func, *args, **kwargs):\n        \"\"\"Execute a function with automatic fallback on failure.\"\"\"\n        max_retries = 3\n        retry_count = 0\n        last_error = None\n        \n        while retry_count < max_retries:\n            try:\n                # Get routing info from kwargs or use default\n                task_type = kwargs.get('task_type', 'entity_extraction')\n                text_length = len(str(args[0])) if args else 1000\n                \n                routing_info = self.route_task(task_type, text_length)\n                kwargs['routing_info'] = routing_info\n                \n                # Execute function\n                result = func(*args, **kwargs)\n                \n                # Record successful usage\n                self.rate_limiter.record_usage(\n                    routing_info['model'],\n                    routing_info['estimated_tokens']\n                )\n                \n                return result\n                \n            except Exception as e:\n                retry_count += 1\n                last_error = e\n                \n                if \"rate_limit\" in str(e).lower():\n                    print(f\"‚ö†Ô∏è Rate limit hit, switching models...\")\n                    # Force model switch\n                    self.rate_limiter.error_counts[routing_info['model']] += 1\n                elif retry_count < max_retries:\n                    print(f\"‚ö†Ô∏è Attempt {retry_count} failed: {e}\")\n                    time.sleep(2 ** retry_count)  # Exponential backoff\n        \n        raise LLMProcessingError(f\"Task failed after {max_retries} attempts: {last_error}\")\n    \n    def display_task_distribution(self):\n        \"\"\"Display how tasks have been distributed across models.\"\"\"\n        if self.rate_limiter.show_visual_feedback and 'IPython' in sys.modules:\n            from IPython.display import display, HTML\n            \n            html = \"\"\"\n            <div style=\"padding: 20px; border: 1px solid #ddd; border-radius: 10px;\">\n                <h3>üìà Task Distribution</h3>\n                <table style=\"width: 100%; border-collapse: collapse;\">\n                    <tr>\n                        <th style=\"text-align: left; padding: 10px; border-bottom: 2px solid #ddd;\">Task Type</th>\n                        <th style=\"text-align: left; padding: 10px; border-bottom: 2px solid #ddd;\">Priority</th>\n            \"\"\"\n            \n            # Add model columns\n            models = list(self.rate_limiter.limits.keys())\n            for model in models:\n                html += f'<th style=\"text-align: center; padding: 10px; border-bottom: 2px solid #ddd;\">{model}</th>'\n            \n            html += \"</tr>\"\n            \n            # Add task rows\n            for task_type, task_info in self.task_types.items():\n                html += f\"\"\"\n                    <tr>\n                        <td style=\"padding: 10px; border-bottom: 1px solid #eee;\">{task_type}</td>\n                        <td style=\"padding: 10px; border-bottom: 1px solid #eee;\">\n                            <span style=\"color: {'#ff5722' if task_info['priority'] == 'high' else '#ff9800' if task_info['priority'] == 'normal' else '#4CAF50'};\">\n                                {task_info['priority'].upper()}\n                            </span>\n                        </td>\n                \"\"\"\n                \n                for model in models:\n                    count = self.task_stats[task_type][model]\n                    html += f'<td style=\"text-align: center; padding: 10px; border-bottom: 1px solid #eee;\">{count}</td>'\n                \n                html += \"</tr>\"\n            \n            html += \"\"\"\n                </table>\n                <p style=\"margin-top: 15px; color: #666; font-size: 14px;\">\n                    Shows how many times each task type has been routed to each model.\n                </p>\n            </div>\n            \"\"\"\n            \n            display(HTML(html))\n        else:\n            print(\"\\nüìà Task Distribution:\")\n            print(\"-\" * 60)\n            for task_type, stats in self.task_stats.items():\n                print(f\"{task_type}:\")\n                for model, count in stats.items():\n                    print(f\"  {model}: {count} tasks\")\n\n# Create global task router\ntask_router = TaskRouter(rate_limiter)\n\nprint(\"‚úÖ Task router initialized\")\nprint(f\"  ‚Ä¢ Managing {len(task_router.task_types)} task types\")\nprint(f\"  ‚Ä¢ Connected to rate limiter\")\n\n# Display task types\nprint(\"\\nüìã Available task types:\")\nfor task_type, info in task_router.task_types.items():\n    print(f\"  ‚Ä¢ {task_type}: {info['description']} (Priority: {info['priority']})\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 5Ô∏è‚É£ Audio Processing [CORE FEATURE]\n\n## Complete Audio Processing Pipeline\n\nThis section contains the **full audio processing system** with:\n\n- **GPU-accelerated transcription** using Whisper\n- **Speaker diarization** to identify who's speaking\n- **Advertisement detection** to skip ads\n- **Sentiment analysis** per segment\n- **Semantic boundary detection** for smart splitting\n- **Audio caching** to avoid re-processing\n\n### Key Components:\n1. **AudioProcessor**: Main class for transcription and diarization\n2. **EnhancedPodcastSegmenter**: Advanced segmentation with multiple features\n3. **Helper functions**: Download, cache, and process audio files\n\nThis replaces the simplified mock transcription with production-grade audio processing!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 4.1: Helper Functions for Text Processing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def convert_transcript_for_llm(transcript_segments):\n    \"\"\"\n    Convert transcript segments to LLM-friendly format.\n    \n    Args:\n        transcript_segments: List of segment dictionaries\n        \n    Returns:\n        Formatted transcript string\n    \"\"\"\n    formatted_lines = []\n    \n    for i, segment in enumerate(transcript_segments):\n        # Extract speaker or use default\n        speaker = segment.get('speaker', 'Speaker')\n        if speaker == 'SPEAKER_00':\n            speaker = 'Host'\n        elif speaker.startswith('SPEAKER_'):\n            speaker = f'Guest {int(speaker.split(\"_\")[1])}'\n        \n        # Format timestamp\n        start_time = segment.get('start', 0)\n        minutes = int(start_time // 60)\n        seconds = int(start_time % 60)\n        timestamp = f\"[{minutes:02d}:{seconds:02d}]\"\n        \n        # Format text\n        text = segment.get('text', '').strip()\n        \n        # Combine into formatted line\n        formatted_lines.append(f\"{timestamp} {speaker}: {text}\")\n    \n    return \"\\n\\n\".join(formatted_lines)\n\ndef clean_segment_text_for_embedding(text):\n    \"\"\"\n    Clean segment text before generating embeddings.\n    \n    Args:\n        text: Raw text to clean\n        \n    Returns:\n        Cleaned text suitable for embedding\n    \"\"\"\n    # Remove excessive whitespace\n    text = ' '.join(text.split())\n    \n    # Remove special characters that might interfere with embeddings\n    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\']', ' ', text)\n    \n    # Remove URLs\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n    \n    # Remove email addresses\n    text = re.sub(r'\\S+@\\S+', '', text)\n    \n    # Normalize quotes\n    text = text.replace('\"', '\"').replace('\"', '\"').replace(''', \"'\").replace(''', \"'\")\n    \n    # Remove multiple spaces\n    text = re.sub(r'\\s+', ' ', text)\n    \n    return text.strip()\n\ndef generate_stable_segment_id(text, episode_id, segment_number):\n    \"\"\"\n    Generate stable content-based ID for segments.\n    \n    Args:\n        text: Segment text\n        episode_id: Episode identifier\n        segment_number: Segment number\n        \n    Returns:\n        Stable segment ID\n    \"\"\"\n    # Create content hash from first 100 chars of text\n    content_hash = hashlib.md5(text[:100].encode()).hexdigest()[:8]\n    \n    # Combine with episode and segment info\n    segment_id = f\"seg_{episode_id}_{segment_number}_{content_hash}\"\n    \n    return segment_id\n\ndef extract_entity_aliases(name, description):\n    \"\"\"\n    Extract potential aliases for an entity from its description.\n    \n    Args:\n        name: Entity name\n        description: Entity description\n        \n    Returns:\n        List of aliases including the original name\n    \"\"\"\n    aliases = [name]\n    \n    if not description:\n        return aliases\n    \n    # Common patterns for aliases\n    patterns = [\n        r'also known as ([^,\\.]+)',\n        r'formerly ([^,\\.]+)',\n        r'aka ([^,\\.]+)',\n        r'or ([^,\\.]+)',\n        r'abbreviated as ([^,\\.]+)',\n        r'([A-Z]{2,})\\s*\\(',  # Acronyms\n    ]\n    \n    for pattern in patterns:\n        matches = re.findall(pattern, description, re.IGNORECASE)\n        for match in matches:\n            alias = match.strip()\n            if alias and alias not in aliases and len(alias) > 1:\n                aliases.append(alias)\n    \n    # Check for parenthetical names\n    paren_match = re.search(r'\\(([^)]+)\\)', description)\n    if paren_match:\n        potential_alias = paren_match.group(1).strip()\n        if potential_alias not in aliases and len(potential_alias) > 1:\n            aliases.append(potential_alias)\n    \n    return aliases\n\ndef build_insight_extraction_prompt(podcast_name, episode_title, text):\n    \"\"\"\n    Build prompt for extracting insights from a segment.\n    \n    Args:\n        podcast_name: Name of the podcast\n        episode_title: Episode title\n        text: Segment text\n        \n    Returns:\n        Formatted prompt\n    \"\"\"\n    prompt = f\"\"\"\nAnalyze this segment from the podcast \"{podcast_name}\" episode \"{episode_title}\".\n\nExtract key insights that would be valuable for someone who wants to understand the main ideas without listening to the full episode.\n\nSegment:\n{text}\n\nReturn a JSON array of insights with this structure:\n[\n  {{\n    \"title\": \"Brief title of the insight (max 100 chars)\",\n    \"description\": \"Detailed explanation of the insight (2-3 sentences)\",\n    \"insight_type\": \"conceptual|analytical|predictive|comparative|historical\",\n    \"confidence\": 0.0-1.0,\n    \"evidence\": \"Quote or reference from the segment that supports this insight\"\n  }}\n]\n\nFocus on:\n- Key concepts or frameworks discussed\n- Important conclusions or recommendations\n- Surprising facts or counterintuitive ideas\n- Predictions or future trends\n- Comparisons or contrasts made\n\nReturn only the JSON array, no other text.\n\"\"\"\n    return prompt\n\ndef build_entity_extraction_prompt(text):\n    \"\"\"\n    Build prompt for extracting entities from text.\n    \n    Args:\n        text: Text to extract entities from\n        \n    Returns:\n        Formatted prompt\n    \"\"\"\n    prompt = f\"\"\"\nExtract all named entities from this text. Include people, organizations, technologies, products, concepts, and locations.\n\nText:\n{text}\n\nReturn a JSON array with this structure:\n[\n  {{\n    \"name\": \"Entity name\",\n    \"type\": \"PERSON|ORGANIZATION|TECHNOLOGY|PRODUCT|CONCEPT|LOCATION|EVENT\",\n    \"description\": \"Brief description of the entity based on context\",\n    \"confidence\": 0.0-1.0\n  }}\n]\n\nGuidelines:\n- Include full names when available\n- For organizations, use the official name\n- For concepts, include technical terms and frameworks\n- Add a description that explains the entity's relevance in this context\n\nReturn only the JSON array.\n\"\"\"\n    return prompt",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 4.2: Combined Extraction Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_combined_extraction_prompt(podcast_name, episode_title, transcript, use_large_context=True):\n    \"\"\"\n    Build a unified prompt for extracting insights, entities, and quotes in one pass.\n    Optimized for large context models.\n    \n    Args:\n        podcast_name: Name of the podcast\n        episode_title: Title of the episode\n        transcript: Full transcript text\n        use_large_context: Whether to use large context optimizations\n        \n    Returns:\n        Formatted prompt string\n    \"\"\"\n    # Limit transcript length based on context window\n    max_chars = 800000 if use_large_context else 30000\n    if len(transcript) > max_chars:\n        transcript = transcript[:max_chars] + \"\\n\\n[Transcript truncated...]\"\n    \n    prompt = f\"\"\"\nAnalyze this complete podcast transcript from \"{podcast_name}\" - \"{episode_title}\".\n\nExtract comprehensive knowledge including insights, entities, and notable quotes.\n\nTRANSCRIPT:\n{transcript}\n\nEXTRACTION TASK:\nProvide a comprehensive analysis in the following JSON structure:\n\n{{\n  \"insights\": [\n    {{\n      \"title\": \"Concise insight title (max 100 chars)\",\n      \"description\": \"Detailed explanation (2-3 sentences)\",\n      \"insight_type\": \"conceptual|analytical|predictive|comparative|historical\",\n      \"confidence\": 0.0-1.0,\n      \"evidence\": \"Supporting quote from transcript\",\n      \"timestamp_reference\": \"Approximate time reference if available\"\n    }}\n  ],\n  \"entities\": [\n    {{\n      \"name\": \"Entity name\",\n      \"type\": \"PERSON|ORGANIZATION|TECHNOLOGY|PRODUCT|CONCEPT|LOCATION|EVENT\",\n      \"description\": \"Context-based description\",\n      \"frequency\": \"Number of mentions\",\n      \"importance\": 0.0-1.0,\n      \"confidence\": 0.0-1.0\n    }}\n  ],\n  \"quotes\": [\n    {{\n      \"text\": \"Exact quote text\",\n      \"speaker\": \"Speaker name or identifier\",\n      \"impact_score\": 0.0-1.0,\n      \"quote_type\": \"insight|prediction|advice|story|controversial\",\n      \"context\": \"Brief context\"\n    }}\n  ],\n  \"topics\": [\n    {{\n      \"name\": \"Topic name\",\n      \"score\": 0.0-1.0,\n      \"evidence\": \"Why this topic is significant\"\n    }}\n  ]\n}}\n\nGUIDELINES:\n1. Extract 10-20 key insights covering main ideas, frameworks, and conclusions\n2. Identify all significant entities (people, companies, technologies, concepts)\n3. Select 5-10 most impactful or memorable quotes\n4. Identify 5-10 main topics discussed\n5. Ensure all extractions are supported by the transcript\n6. Use confidence scores to indicate certainty\n\nReturn only the JSON object, no other text.\n\"\"\"\n    \n    return prompt\n\ndef parse_combined_llm_response(response_text):\n    \"\"\"\n    Parse the combined JSON response from LLM.\n    Handles potential formatting issues and validates structure.\n    \n    Args:\n        response_text: Raw response from LLM\n        \n    Returns:\n        Dict with parsed insights, entities, quotes, and topics\n    \"\"\"\n    # Clean response text\n    response_text = response_text.strip()\n    \n    # Remove markdown code blocks if present\n    if response_text.startswith('```json'):\n        response_text = response_text[7:]\n    if response_text.startswith('```'):\n        response_text = response_text[3:]\n    if response_text.endswith('```'):\n        response_text = response_text[:-3]\n    \n    # Try to parse JSON\n    try:\n        data = json.loads(response_text)\n    except json.JSONDecodeError as e:\n        # Try to fix common issues\n        logger.warning(f\"JSON parse error: {e}\")\n        \n        # Remove trailing commas\n        response_text = re.sub(r',\\s*}', '}', response_text)\n        response_text = re.sub(r',\\s*]', ']', response_text)\n        \n        try:\n            data = json.loads(response_text)\n        except:\n            # If still failing, return empty structure\n            logger.error(\"Failed to parse LLM response\")\n            return {\n                'insights': [],\n                'entities': [],\n                'quotes': [],\n                'topics': []\n            }\n    \n    # Validate and clean the parsed data\n    result = {\n        'insights': data.get('insights', []),\n        'entities': data.get('entities', []),\n        'quotes': data.get('quotes', []),\n        'topics': data.get('topics', [])\n    }\n    \n    # Ensure all arrays are actually lists\n    for key in result:\n        if not isinstance(result[key], list):\n            result[key] = []\n    \n    return result\n\ndef parse_insights_from_response(response_text):\n    \"\"\"Parse insights from LLM response.\"\"\"\n    try:\n        data = parse_combined_llm_response(response_text)\n        return data.get('insights', [])\n    except:\n        return []\n\ndef parse_entities_from_response(response_text):\n    \"\"\"Parse entities from LLM response.\"\"\"\n    try:\n        data = parse_combined_llm_response(response_text)\n        return data.get('entities', [])\n    except:\n        return []\n\ndef extract_notable_quotes(transcript_segments, llm_client=None):\n    \"\"\"\n    Extract notable quotes from transcript segments.\n    \n    Args:\n        transcript_segments: List of transcript segments\n        llm_client: Optional LLM client for enhanced extraction\n        \n    Returns:\n        List of quote dictionaries\n    \"\"\"\n    quotes = []\n    \n    # If LLM client provided, use it for better extraction\n    if llm_client:\n        transcript_text = convert_transcript_for_llm(transcript_segments)\n        \n        prompt = f\"\"\"\nExtract the most notable, impactful, or memorable quotes from this transcript.\n\n{transcript_text[:50000]}\n\nReturn a JSON array of quotes:\n[\n  {{\n    \"text\": \"Exact quote\",\n    \"speaker\": \"Speaker name\",\n    \"impact_score\": 0.0-1.0,\n    \"quote_type\": \"insight|prediction|advice|story|controversial\",\n    \"estimated_timestamp\": \"MM:SS\"\n  }}\n]\n\nSelect quotes that are:\n- Insightful or thought-provoking\n- Memorable or quotable\n- Represent key ideas\n- Tell compelling stories\n- Make predictions or give advice\n\nReturn only the JSON array.\n\"\"\"\n        \n        try:\n            response = llm_client.invoke(prompt)\n            quotes_data = json.loads(response.content)\n            if isinstance(quotes_data, list):\n                quotes.extend(quotes_data)\n        except:\n            pass\n    \n    # Fallback: Extract quotes based on patterns\n    if not quotes:\n        for segment in transcript_segments:\n            text = segment.get('text', '')\n            speaker = segment.get('speaker', 'Unknown')\n            \n            # Look for quotable patterns\n            quotable_patterns = [\n                r'\"([^\"]{50,300})\"',  # Quoted text\n                r'[A-Z][^.!?]{50,200}[.!?]',  # Complete sentences\n            ]\n            \n            for pattern in quotable_patterns:\n                matches = re.findall(pattern, text)\n                for match in matches[:2]:  # Max 2 per segment\n                    if len(match.split()) >= 10:  # At least 10 words\n                        quotes.append({\n                            'text': match,\n                            'speaker': speaker,\n                            'impact_score': 0.5,\n                            'quote_type': 'general',\n                            'estimated_timestamp': f\"{int(segment.get('start', 0))//60:02d}:{int(segment.get('start', 0))%60:02d}\"\n                        })\n    \n    return quotes[:20]  # Limit to top 20 quotes",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 5.1: Audio Download with Smart Caching\n\n**What this does:**\n- Downloads podcast audio files from URLs\n- Implements content-based caching to avoid re-downloads\n- Shows progress bars during download\n- Handles network errors gracefully\n\n**Caching benefits:**\n- Saves bandwidth and time\n- Persistent across Colab sessions (stored in Drive)\n- Automatic cache management",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@with_retry(max_retries=3, delay=5, exceptions=(urllib.error.URLError, ConnectionError))\ndef download_audio_with_cache(url, output_path, use_cache=True):\n    \"\"\"\n    Download audio file with intelligent caching support.\n    \n    Args:\n        url: URL of the audio file\n        output_path: Where to save the file\n        use_cache: Whether to use caching\n        \n    Returns:\n        Path to the downloaded/cached file\n    \"\"\"\n    if PodcastConfig.COLAB_MODE and use_cache:\n        # Generate cache key from URL\n        cache_key = hashlib.md5(url.encode()).hexdigest()\n        cache_path = os.path.join(PodcastConfig.CACHE_DIR, f\"{cache_key}.mp3\")\n        \n        # Check cache first\n        if os.path.exists(cache_path):\n            file_size = os.path.getsize(cache_path) / (1024 * 1024)  # MB\n            print(f\"‚úÖ Using cached audio: {cache_key} ({file_size:.1f} MB)\")\n            \n            # Copy from cache to output path if different\n            if cache_path != output_path:\n                import shutil\n                shutil.copy2(cache_path, output_path)\n            \n            return output_path\n    \n    # Download with progress bar\n    print(f\"üì• Downloading audio from: {url[:60]}...\")\n    \n    try:\n        # Use notebook-friendly progress bar if in Colab\n        if PodcastConfig.COLAB_MODE and 'IPython' in sys.modules:\n            from IPython.display import clear_output\n            \n            # Open URL and get file size\n            response = urllib.request.urlopen(url)\n            total_size = int(response.headers.get('Content-Length', 0))\n            \n            # Download with visual progress\n            downloaded = 0\n            chunk_size = 8192\n            \n            with open(output_path, 'wb') as f:\n                while True:\n                    chunk = response.read(chunk_size)\n                    if not chunk:\n                        break\n                    \n                    f.write(chunk)\n                    downloaded += len(chunk)\n                    \n                    # Update progress\n                    if total_size > 0:\n                        progress = downloaded / total_size\n                        clear_output(wait=True)\n                        print(f\"üì• Downloading: {progress*100:.1f}% ({downloaded/(1024*1024):.1f}/{total_size/(1024*1024):.1f} MB)\")\n                        \n                        # Visual progress bar\n                        bar_length = 50\n                        filled = int(bar_length * progress)\n                        bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)\n                        print(f\"[{bar}]\")\n            \n            clear_output(wait=True)\n            print(f\"‚úÖ Download complete: {os.path.basename(output_path)} ({downloaded/(1024*1024):.1f} MB)\")\n            \n        else:\n            # Standard download with urllib\n            urllib.request.urlretrieve(url, output_path)\n            file_size = os.path.getsize(output_path) / (1024 * 1024)\n            print(f\"‚úÖ Downloaded: {file_size:.1f} MB\")\n        \n        # Save to cache if enabled\n        if PodcastConfig.COLAB_MODE and use_cache:\n            import shutil\n            os.makedirs(PodcastConfig.CACHE_DIR, exist_ok=True)\n            shutil.copy2(output_path, cache_path)\n            print(f\"üíæ Cached for future use: {cache_key}\")\n            \n    except Exception as e:\n        print(f\"‚ùå Download failed: {e}\")\n        raise AudioProcessingError(f\"Failed to download audio: {e}\")\n        \n    return output_path\n\ndef clean_audio_cache(max_size_gb=10):\n    \"\"\"Clean old cached audio files if cache exceeds size limit.\"\"\"\n    if not os.path.exists(PodcastConfig.CACHE_DIR):\n        return\n    \n    # Get all cache files with their timestamps\n    cache_files = []\n    total_size = 0\n    \n    for filename in os.listdir(PodcastConfig.CACHE_DIR):\n        if filename.endswith('.mp3'):\n            filepath = os.path.join(PodcastConfig.CACHE_DIR, filename)\n            stat = os.stat(filepath)\n            cache_files.append((filepath, stat.st_mtime, stat.st_size))\n            total_size += stat.st_size\n    \n    # Check if cleanup needed\n    total_size_gb = total_size / (1024**3)\n    if total_size_gb <= max_size_gb:\n        return\n    \n    print(f\"üßπ Cache cleanup needed: {total_size_gb:.1f} GB > {max_size_gb} GB limit\")\n    \n    # Sort by modification time (oldest first)\n    cache_files.sort(key=lambda x: x[1])\n    \n    # Remove oldest files until under limit\n    removed_count = 0\n    removed_size = 0\n    \n    for filepath, _, size in cache_files:\n        if total_size_gb <= max_size_gb:\n            break\n            \n        try:\n            os.remove(filepath)\n            removed_count += 1\n            removed_size += size\n            total_size_gb -= size / (1024**3)\n        except:\n            pass\n    \n    print(f\"‚úÖ Removed {removed_count} files ({removed_size/(1024**3):.1f} GB)\")\n\n# Test download function\nprint(\"‚úÖ Audio download system ready\")\nprint(f\"  üìÅ Cache directory: {PodcastConfig.CACHE_DIR}\")\nprint(f\"  üíæ Caching: {'ENABLED' if PodcastConfig.COLAB_MODE else 'DISABLED'}\")\n\n# Clean cache if needed\nif PodcastConfig.COLAB_MODE:\n    clean_audio_cache()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 5.2: Whisper Transcription with GPU Acceleration\n\n**What this does:**\n- Transcribes audio using OpenAI's Whisper model\n- Automatically uses GPU if available (much faster)\n- Supports multiple Whisper model sizes\n- Returns timestamped segments\n\n**Model sizes:**\n- **tiny**: Fastest, least accurate (39M parameters)\n- **base**: Good balance (74M)\n- **small**: Better accuracy (244M)\n- **medium**: High accuracy (769M)\n- **large-v3**: Best accuracy (1550M) - Default",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def transcribe_audio(audio_path, use_faster_whisper=True, whisper_model_size=\"large-v3\"):\n    \"\"\"\n    Transcribe audio using Whisper with GPU acceleration.\n    \n    Args:\n        audio_path: Path to audio file\n        use_faster_whisper: Use faster-whisper implementation\n        whisper_model_size: Model size to use\n        \n    Returns:\n        List of transcript segments with timestamps\n    \"\"\"\n    if not ENABLE_AUDIO_PROCESSING:\n        print(\"‚ö†Ô∏è Audio processing disabled. Returning empty transcript.\")\n        return []\n    \n    print(f\"üéØ Starting transcription with {whisper_model_size} model...\")\n    start_time = time.time()\n    \n    try:\n        if use_faster_whisper and WhisperModel:\n            # Use faster-whisper (recommended)\n            device = \"cuda\" if PodcastConfig.USE_GPU else \"cpu\"\n            compute_type = \"float16\" if PodcastConfig.USE_GPU else \"int8\"\n            \n            print(f\"  ‚Ä¢ Device: {device}\")\n            print(f\"  ‚Ä¢ Compute type: {compute_type}\")\n            \n            # Load model\n            model = WhisperModel(\n                whisper_model_size,\n                device=device,\n                compute_type=compute_type\n            )\n            \n            # Transcribe with progress\n            segments, info = model.transcribe(\n                audio_path,\n                beam_size=5,\n                best_of=5,\n                patience=1,\n                length_penalty=1,\n                temperature=0,\n                compression_ratio_threshold=2.4,\n                log_prob_threshold=-1.0,\n                no_speech_threshold=0.6,\n                condition_on_previous_text=True,\n                initial_prompt=\"This is a podcast transcript with multiple speakers.\",\n                vad_filter=True,  # Voice activity detection\n                vad_parameters=dict(\n                    threshold=0.5,\n                    min_speech_duration_ms=250,\n                    min_silence_duration_ms=100,\n                    speech_pad_ms=30,\n                    window_size_samples=512,\n                )\n            )\n            \n            # Convert to list with progress tracking\n            transcript_segments = []\n            \n            if PodcastConfig.COLAB_MODE:\n                from IPython.display import clear_output\n                \n            for i, segment in enumerate(segments):\n                transcript_segments.append({\n                    'text': segment.text.strip(),\n                    'start': segment.start,\n                    'end': segment.end,\n                    'no_speech_prob': segment.no_speech_prob,\n                    'avg_logprob': segment.avg_logprob\n                })\n                \n                # Show progress every 100 segments\n                if i % 100 == 0 and PodcastConfig.COLAB_MODE:\n                    clear_output(wait=True)\n                    elapsed = time.time() - start_time\n                    print(f\"üéØ Transcribing: {i} segments processed ({elapsed:.1f}s)\")\n            \n            if PodcastConfig.COLAB_MODE:\n                clear_output(wait=True)\n                \n        elif whisper:\n            # Fallback to original whisper\n            device = \"cuda\" if PodcastConfig.USE_GPU else \"cpu\"\n            model = whisper.load_model(whisper_model_size, device=device)\n            \n            result = model.transcribe(\n                audio_path,\n                verbose=False,\n                temperature=0,\n                compression_ratio_threshold=2.4,\n                log_prob_threshold=-1.0,\n                no_speech_threshold=0.6,\n                condition_on_previous_text=True,\n                initial_prompt=\"This is a podcast transcript with multiple speakers.\"\n            )\n            \n            # Convert to segment format\n            transcript_segments = []\n            for segment in result['segments']:\n                transcript_segments.append({\n                    'text': segment['text'].strip(),\n                    'start': segment['start'],\n                    'end': segment['end']\n                })\n        else:\n            print(\"‚ùå No Whisper implementation available\")\n            return []\n        \n        # Calculate statistics\n        duration = time.time() - start_time\n        total_segments = len(transcript_segments)\n        total_words = sum(len(seg['text'].split()) for seg in transcript_segments)\n        \n        print(f\"‚úÖ Transcription complete!\")\n        print(f\"  ‚Ä¢ Time: {duration:.1f} seconds\")\n        print(f\"  ‚Ä¢ Segments: {total_segments}\")\n        print(f\"  ‚Ä¢ Words: {total_words:,}\")\n        print(f\"  ‚Ä¢ Speed: {total_words/duration:.1f} words/second\")\n        \n        # Clean up GPU memory\n        if PodcastConfig.USE_GPU:\n            cleanup_memory()\n        \n        return transcript_segments\n        \n    except Exception as e:\n        print(f\"‚ùå Transcription failed: {e}\")\n        raise AudioProcessingError(f\"Transcription failed: {e}\")\n\n# Validate transcription segments\ndef validate_transcript_segments(segments):\n    \"\"\"Validate and clean transcript segments.\"\"\"\n    valid_segments = []\n    \n    for i, segment in enumerate(segments):\n        # Skip empty segments\n        if not segment.get('text', '').strip():\n            continue\n        \n        # Ensure required fields\n        if 'start' not in segment or 'end' not in segment:\n            print(f\"‚ö†Ô∏è Segment {i} missing timestamp, skipping\")\n            continue\n        \n        # Validate timestamps\n        if segment['end'] <= segment['start']:\n            print(f\"‚ö†Ô∏è Segment {i} has invalid timestamps, fixing\")\n            segment['end'] = segment['start'] + 1.0\n        \n        # Clean text\n        segment['text'] = segment['text'].strip()\n        \n        valid_segments.append(segment)\n    \n    return valid_segments\n\nprint(\"‚úÖ Whisper transcription system ready\")\nprint(f\"  üéôÔ∏è Model: {PodcastConfig.WHISPER_MODEL_SIZE}\")\nprint(f\"  üöÄ GPU: {'ENABLED' if PodcastConfig.USE_GPU else 'DISABLED'}\")\nprint(f\"  ‚ö° Implementation: {'faster-whisper' if WhisperModel else 'original whisper' if whisper else 'NOT AVAILABLE'}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 5.3: Speaker Diarization - Who Said What\n\n**What this does:**\n- Identifies different speakers in the audio\n- Creates a timeline of who spoke when\n- Uses AI to distinguish voices\n- Essential for multi-speaker podcasts\n\n**Requirements:**\n- HuggingFace token (for pyannote access)\n- GPU recommended for faster processing\n\n**Output:**\n- Speaker labels (SPEAKER_00, SPEAKER_01, etc.)\n- Timestamps for each speaker segment",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def diarize_speakers(audio_path, min_speakers=1, max_speakers=10):\n    \"\"\"\n    Perform speaker diarization to identify who speaks when.\n    \n    Args:\n        audio_path: Path to audio file\n        min_speakers: Minimum expected speakers\n        max_speakers: Maximum expected speakers\n        \n    Returns:\n        Dictionary mapping time ranges to speaker IDs\n    \"\"\"\n    if not ENABLE_SPEAKER_DIARIZATION:\n        print(\"‚ö†Ô∏è Speaker diarization disabled.\")\n        return {}\n    \n    if not Pipeline:\n        print(\"‚ö†Ô∏è Pyannote not available. Install with: pip install pyannote.audio\")\n        return {}\n    \n    # Check for HuggingFace token\n    hf_token = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_TOKEN\")\n    if not hf_token:\n        print(\"‚ö†Ô∏è HuggingFace token not found. Speaker diarization requires authentication.\")\n        print(\"   Set HF_TOKEN environment variable or add to Colab secrets.\")\n        return {}\n    \n    print(\"üë• Starting speaker diarization...\")\n    start_time = time.time()\n    \n    try:\n        # Load pretrained pipeline\n        pipeline = Pipeline.from_pretrained(\n            \"pyannote/speaker-diarization-3.1\",\n            use_auth_token=hf_token\n        )\n        \n        # Move to GPU if available\n        if PodcastConfig.USE_GPU:\n            import torch\n            pipeline.to(torch.device(\"cuda\"))\n        \n        # Run diarization\n        diarization = pipeline(\n            audio_path,\n            min_speakers=min_speakers,\n            max_speakers=max_speakers\n        )\n        \n        # Convert to dictionary format\n        speaker_map = {}\n        speakers_found = set()\n        \n        for turn, _, speaker in diarization.itertracks(yield_label=True):\n            start = turn.start\n            end = turn.end\n            speaker_map[f\"{start:.2f}-{end:.2f}\"] = speaker\n            speakers_found.add(speaker)\n        \n        duration = time.time() - start_time\n        print(f\"‚úÖ Diarization complete!\")\n        print(f\"  ‚Ä¢ Time: {duration:.1f} seconds\")\n        print(f\"  ‚Ä¢ Speakers found: {len(speakers_found)}\")\n        print(f\"  ‚Ä¢ Speaker IDs: {', '.join(sorted(speakers_found))}\")\n        \n        # Clean up GPU memory\n        if PodcastConfig.USE_GPU:\n            cleanup_memory()\n        \n        return speaker_map\n        \n    except Exception as e:\n        print(f\"‚ùå Diarization failed: {e}\")\n        if \"401\" in str(e):\n            print(\"   Authentication error. Check your HuggingFace token.\")\n        return {}\n\ndef align_transcript_with_diarization(transcript_segments, speaker_map):\n    \"\"\"\n    Align transcript segments with speaker diarization results.\n    \n    Args:\n        transcript_segments: List of transcript segments\n        speaker_map: Speaker diarization results\n        \n    Returns:\n        Updated transcript segments with speaker labels\n    \"\"\"\n    if not speaker_map:\n        return transcript_segments\n    \n    print(\"üîÑ Aligning transcript with speakers...\")\n    \n    # Convert speaker map to list of tuples for easier searching\n    speaker_timeline = []\n    for time_range, speaker in speaker_map.items():\n        start, end = map(float, time_range.split('-'))\n        speaker_timeline.append((start, end, speaker))\n    \n    # Sort by start time\n    speaker_timeline.sort(key=lambda x: x[0])\n    \n    # Align each segment\n    aligned_count = 0\n    for segment in transcript_segments:\n        seg_start = segment['start']\n        seg_end = segment['end']\n        seg_mid = (seg_start + seg_end) / 2\n        \n        # Find overlapping speaker segments\n        overlaps = []\n        for sp_start, sp_end, speaker in speaker_timeline:\n            # Calculate overlap\n            overlap_start = max(seg_start, sp_start)\n            overlap_end = min(seg_end, sp_end)\n            \n            if overlap_end > overlap_start:\n                overlap_duration = overlap_end - overlap_start\n                overlaps.append((overlap_duration, speaker))\n        \n        # Assign speaker with most overlap\n        if overlaps:\n            overlaps.sort(reverse=True)  # Sort by duration\n            segment['speaker'] = overlaps[0][1]\n            aligned_count += 1\n        else:\n            # Fallback: find nearest speaker\n            best_speaker = None\n            min_distance = float('inf')\n            \n            for sp_start, sp_end, speaker in speaker_timeline:\n                distance = min(abs(seg_mid - sp_start), abs(seg_mid - sp_end))\n                if distance < min_distance:\n                    min_distance = distance\n                    best_speaker = speaker\n            \n            if best_speaker:\n                segment['speaker'] = best_speaker\n                segment['speaker_confidence'] = 'low'\n                aligned_count += 1\n    \n    print(f\"‚úÖ Aligned {aligned_count}/{len(transcript_segments)} segments with speakers\")\n    \n    return transcript_segments\n\n# Test speaker detection\ndef test_speaker_detection():\n    \"\"\"Test if speaker diarization is properly configured.\"\"\"\n    print(\"üß™ Testing speaker diarization setup...\")\n    \n    # Check dependencies\n    if not Pipeline:\n        print(\"  ‚ùå pyannote.audio not installed\")\n        return False\n    \n    # Check token\n    hf_token = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_TOKEN\")\n    if not hf_token:\n        print(\"  ‚ùå HuggingFace token not found\")\n        return False\n    else:\n        print(\"  ‚úÖ HuggingFace token found\")\n    \n    # Check GPU\n    if PodcastConfig.USE_GPU:\n        print(\"  ‚úÖ GPU available for acceleration\")\n    else:\n        print(\"  ‚ÑπÔ∏è GPU not available, will use CPU (slower)\")\n    \n    return True\n\n# Run test\nif test_speaker_detection():\n    print(\"\\n‚úÖ Speaker diarization ready!\")\nelse:\n    print(\"\\n‚ö†Ô∏è Speaker diarization not fully configured\")\n    print(\"  Set ENABLE_SPEAKER_DIARIZATION = False to disable\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 5.4: Complete Audio Processing Classes\n\n**AudioProcessor:**\n- Main class that orchestrates transcription and diarization\n- Handles all audio processing operations\n- Provides clean interface for the pipeline\n\n**EnhancedPodcastSegmenter:**\n- Advanced segmentation with multiple features\n- Advertisement detection\n- Sentiment analysis per segment\n- Post-processing for better quality",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class AudioProcessor:\n    \"\"\"Complete audio processing with transcription and diarization.\"\"\"\n    \n    def __init__(self, config=None):\n        \"\"\"Initialize audio processor with configuration.\"\"\"\n        self.config = config or PodcastConfig\n        self.enable_processing = ENABLE_AUDIO_PROCESSING\n        self.enable_diarization = ENABLE_SPEAKER_DIARIZATION\n        \n    def transcribe_audio(self, audio_path, use_faster_whisper=True, whisper_model_size=\"large-v3\"):\n        \"\"\"Transcribe audio using Whisper.\"\"\"\n        if not self.enable_processing:\n            print(\"Audio processing disabled. Returning empty transcript.\")\n            return []\n            \n        return transcribe_audio(audio_path, use_faster_whisper, whisper_model_size)\n        \n    def diarize_speakers(self, audio_path, min_speakers=1, max_speakers=10):\n        \"\"\"Perform speaker diarization on audio.\"\"\"\n        if not self.enable_diarization:\n            print(\"Speaker diarization disabled. Returning empty speaker map.\")\n            return {}\n            \n        return diarize_speakers(audio_path, min_speakers, max_speakers)\n        \n    def align_transcript_with_diarization(self, transcript_segments, speaker_map):\n        \"\"\"Align transcript segments with speaker diarization results.\"\"\"\n        return align_transcript_with_diarization(transcript_segments, speaker_map)\n    \n    def process_audio_complete(self, audio_path):\n        \"\"\"Complete audio processing pipeline.\"\"\"\n        results = {\n            'transcript': [],\n            'speaker_map': {},\n            'metadata': {}\n        }\n        \n        # Transcribe\n        if self.enable_processing:\n            print(\"\\n\" + \"=\"*60)\n            print(\"üìù TRANSCRIPTION PHASE\")\n            print(\"=\"*60)\n            results['transcript'] = self.transcribe_audio(\n                audio_path,\n                use_faster_whisper=self.config.USE_FASTER_WHISPER,\n                whisper_model_size=self.config.WHISPER_MODEL_SIZE\n            )\n            \n            # Validate segments\n            results['transcript'] = validate_transcript_segments(results['transcript'])\n        \n        # Diarize\n        if self.enable_diarization and results['transcript']:\n            print(\"\\n\" + \"=\"*60)\n            print(\"üë• SPEAKER DIARIZATION PHASE\")\n            print(\"=\"*60)\n            results['speaker_map'] = self.diarize_speakers(\n                audio_path,\n                min_speakers=self.config.MIN_SPEAKERS,\n                max_speakers=self.config.MAX_SPEAKERS\n            )\n            \n            # Align speakers with transcript\n            if results['speaker_map']:\n                results['transcript'] = self.align_transcript_with_diarization(\n                    results['transcript'],\n                    results['speaker_map']\n                )\n        \n        # Add metadata\n        results['metadata'] = {\n            'audio_path': audio_path,\n            'processing_time': datetime.now().isoformat(),\n            'whisper_model': self.config.WHISPER_MODEL_SIZE,\n            'diarization_enabled': self.enable_diarization,\n            'segment_count': len(results['transcript']),\n            'word_count': sum(len(seg['text'].split()) for seg in results['transcript'])\n        }\n        \n        return results\n\nclass EnhancedPodcastSegmenter:\n    \"\"\"Enhanced segmentation with advertisement detection and sentiment analysis.\"\"\"\n    \n    def __init__(self, config=None):\n        \"\"\"Initialize with configuration.\"\"\"\n        # Default configuration\n        default_config = {\n            'min_segment_tokens': 150,\n            'max_segment_tokens': 800,\n            'use_gpu': True,\n            'ad_detection_enabled': True,\n            'use_semantic_boundaries': True,\n            'min_speakers': 1,\n            'max_speakers': 10\n        }\n        \n        # Update with provided config\n        self.config = default_config.copy()\n        if config:\n            self.config.update(config)\n        \n        # Initialize audio processor\n        self.audio_processor = AudioProcessor()\n            \n    def process_audio(self, audio_path):\n        \"\"\"Process audio file through complete pipeline.\"\"\"\n        # Use audio processor for transcription and diarization\n        results = self.audio_processor.process_audio_complete(audio_path)\n        \n        # Post-process segments\n        if results['transcript']:\n            print(\"\\n\" + \"=\"*60)\n            print(\"üîß POST-PROCESSING PHASE\")\n            print(\"=\"*60)\n            results['transcript'] = self._post_process_segments(results['transcript'])\n        \n        return results\n        \n    def _post_process_segments(self, segments):\n        \"\"\"Post-process segments with enhancements.\"\"\"\n        processed_segments = []\n        \n        print(\"üìä Analyzing segments...\")\n        \n        for i, segment in enumerate(tqdm(segments, desc=\"Processing segments\")):\n            # Skip empty segments\n            if not segment.get('text', '').strip():\n                continue\n            \n            # Detect advertisements\n            if self.config['ad_detection_enabled']:\n                segment['is_advertisement'] = self._detect_advertisement(segment)\n            \n            # Add sentiment analysis\n            segment['sentiment'] = self._analyze_segment_sentiment(segment['text'])\n            \n            # Add technical analysis\n            if pattern_matcher:\n                analysis = pattern_matcher.analyze_text_structure(segment['text'])\n                segment['has_technical_content'] = analysis['technical_density'] > 0.05\n                segment['has_facts'] = analysis['fact_density'] > 0.02\n                segment['entity_mentions'] = analysis['entity_count']\n            \n            # Add segment index\n            segment['segment_index'] = i\n            \n            # Calculate segment duration\n            segment['duration'] = segment['end'] - segment['start']\n            \n            processed_segments.append(segment)\n        \n        print(f\"‚úÖ Processed {len(processed_segments)} segments\")\n        \n        # Add segment statistics\n        self._add_segment_statistics(processed_segments)\n        \n        return processed_segments\n        \n    def _detect_advertisement(self, segment):\n        \"\"\"Detect if segment is an advertisement.\"\"\"\n        if not self.config['ad_detection_enabled']:\n            return False\n        \n        text = segment['text'].lower()\n        \n        # Common ad markers\n        ad_markers = [\n            \"sponsor\", \"sponsored by\", \"brought to you by\", \n            \"discount code\", \"promo code\", \"offer code\",\n            \"special offer\", \"limited time offer\",\n            \"visit\", \"go to\", \"check out\",\n            \"use code\", \"save\", \"percent off\",\n            \"free shipping\", \"money back guarantee\"\n        ]\n        \n        # Count markers\n        marker_count = sum(1 for marker in ad_markers if marker in text)\n        \n        # Check for URLs (common in ads)\n        url_pattern = r'(?:www\\.|https?://)\\S+|(?:\\w+\\.com|\\.org|\\.net)'\n        has_url = bool(re.search(url_pattern, text))\n        \n        # Decision logic\n        if marker_count >= 2 or (marker_count >= 1 and has_url):\n            return True\n        \n        # Check segment duration (ads often have specific durations)\n        duration = segment.get('end', 0) - segment.get('start', 0)\n        if 25 <= duration <= 35 or 55 <= duration <= 65:  # Common ad durations\n            if marker_count >= 1:\n                return True\n        \n        return False\n        \n    def _analyze_segment_sentiment(self, text):\n        \"\"\"Analyze sentiment of segment text.\"\"\"\n        # Enhanced sentiment word lists\n        positive_words = set([\n            \"good\", \"great\", \"excellent\", \"amazing\", \"love\", \"best\", \"positive\",\n            \"happy\", \"excited\", \"wonderful\", \"fantastic\", \"superior\", \"beneficial\",\n            \"success\", \"win\", \"achieve\", \"improve\", \"better\", \"awesome\", \"brilliant\",\n            \"outstanding\", \"perfect\", \"beautiful\", \"delightful\", \"enjoyable\"\n        ])\n        \n        negative_words = set([\n            \"bad\", \"terrible\", \"awful\", \"hate\", \"worst\", \"negative\", \"poor\",\n            \"horrible\", \"failure\", \"inadequate\", \"disappointing\", \"problem\",\n            \"difficult\", \"hard\", \"struggle\", \"fail\", \"loss\", \"mistake\", \"wrong\",\n            \"unfortunately\", \"sadly\", \"regret\", \"concern\", \"worry\", \"fear\"\n        ])\n        \n        # Intensity modifiers\n        intensifiers = set([\"very\", \"really\", \"extremely\", \"incredibly\", \"absolutely\"])\n        negations = set([\"not\", \"no\", \"never\", \"neither\", \"nor\", \"wasn't\", \"weren't\", \"isn't\", \"aren't\"])\n        \n        text_lower = text.lower()\n        words = re.findall(r'\\b\\w+\\b', text_lower)\n        \n        # Count sentiment with context\n        positive_score = 0\n        negative_score = 0\n        \n        for i, word in enumerate(words):\n            # Check for negation\n            negated = any(neg in words[max(0, i-3):i] for neg in negations)\n            \n            # Check for intensifier\n            intensified = any(int_word in words[max(0, i-2):i] for int_word in intensifiers)\n            multiplier = 1.5 if intensified else 1.0\n            \n            if word in positive_words:\n                if negated:\n                    negative_score += multiplier\n                else:\n                    positive_score += multiplier\n            elif word in negative_words:\n                if negated:\n                    positive_score += multiplier\n                else:\n                    negative_score += multiplier\n        \n        # Calculate final score\n        total = positive_score + negative_score\n        if total == 0:\n            score = 0\n            polarity = \"neutral\"\n        else:\n            score = (positive_score - negative_score) / total\n            if score > 0.2:\n                polarity = \"positive\"\n            elif score < -0.2:\n                polarity = \"negative\"\n            else:\n                polarity = \"neutral\"\n        \n        return {\n            \"score\": score,\n            \"polarity\": polarity,\n            \"positive_score\": positive_score,\n            \"negative_score\": negative_score,\n            \"confidence\": min(total / len(words), 1.0) if words else 0\n        }\n    \n    def _add_segment_statistics(self, segments):\n        \"\"\"Add statistical metadata to segments.\"\"\"\n        if not segments:\n            return\n        \n        # Calculate speaking rate\n        for segment in segments:\n            words = len(segment['text'].split())\n            duration = segment['duration']\n            segment['words_per_minute'] = (words / duration * 60) if duration > 0 else 0\n        \n        # Find conversation dynamics\n        if 'speaker' in segments[0]:\n            speaker_changes = 0\n            last_speaker = segments[0].get('speaker')\n            \n            for segment in segments[1:]:\n                current_speaker = segment.get('speaker')\n                if current_speaker and current_speaker != last_speaker:\n                    speaker_changes += 1\n                    last_speaker = current_speaker\n            \n            # Add to all segments\n            for segment in segments:\n                segment['total_speaker_changes'] = speaker_changes\n\n# Initialize processors\naudio_processor = AudioProcessor()\npodcast_segmenter = EnhancedPodcastSegmenter(PodcastConfig.get_segmenter_config())\n\nprint(\"‚úÖ Audio processing system initialized\")\nprint(f\"  ‚Ä¢ AudioProcessor ready\")\nprint(f\"  ‚Ä¢ EnhancedPodcastSegmenter ready\")\nprint(f\"  ‚Ä¢ Advertisement detection: {'ON' if podcast_segmenter.config['ad_detection_enabled'] else 'OFF'}\")\nprint(f\"  ‚Ä¢ Sentiment analysis: ON\")\nprint(f\"  ‚Ä¢ Technical content detection: ON\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 6Ô∏è‚É£ Knowledge Extraction [CORE FEATURE]\n\n## Advanced AI-Powered Extraction\n\nThis section contains the **complete knowledge extraction system** with:\n\n- **Insight extraction** with context-aware prompts\n- **Entity recognition** and deduplication\n- **Relationship extraction** (5 types)\n- **Quote extraction** for memorable content\n- **Topic generation** from content\n- **Validation** and quality control\n\n### Key Components:\n1. **LLM Prompt Building**: Optimized prompts for different tasks\n2. **Entity Resolution**: Fuzzy matching and alias detection\n3. **Relationship Extraction**: Complex semantic relationships\n4. **Validation**: Quality control for extracted data\n5. **Batch Processing**: Efficient extraction for large transcripts\n\nThis uses Google's Gemini AI for state-of-the-art extraction!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 6.1: LLM Prompt Building Functions\n\n**What this does:**\n- Creates optimized prompts for different extraction tasks\n- Supports both large context (1M tokens) and standard models\n- Combines multiple extraction tasks for efficiency\n\n**Key features:**\n- Context-aware prompts that use full transcript\n- Task-specific formatting\n- Combined extraction to reduce API calls",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def build_insight_extraction_prompt(podcast_name, episode_title, use_large_context=True):\n    \"\"\"Build the appropriate prompt template for insight extraction.\"\"\"\n    \n    if use_large_context:\n        # Enhanced prompt for large context window models\n        prompt_template = \"\"\"\n        You are a knowledge extraction system analyzing an entire podcast episode.\n        \n        PODCAST: {podcast_name}\n        EPISODE: {episode_title}\n        \n        Your task is to extract structured insights from the complete podcast transcript below.\n        Take advantage of your 1M token context window to identify themes, patterns, and valuable insights\n        that span across the entire conversation. Look for connections between different parts of the episode.\n        \n        Focus on the most valuable and non-obvious information. Consider how ideas evolve throughout the conversation.\n        \n        FULL TRANSCRIPT:\n        {segment_text}\n        \n        Extract 5-15 valuable insights from this episode. For each insight:\n        1. Give it a concise, informative title that captures the core idea\n        2. Write a brief but clear description (1-3 sentences) that explains the insight\n        3. Classify the insight_type as one of:\n           - 'actionable' (practical advice, steps to take, recommendations)\n           - 'conceptual' (explanations, theory, background information)\n           - 'experiential' (personal stories, examples, case studies)\n        4. Include a confidence score (1-10) indicating how strongly this insight is supported in the transcript\n        \n        Format your response as a valid JSON array of objects with these fields:\n        - title (string): Concise, informative title\n        - description (string): Clear, accurate description of the insight\n        - insight_type (string): Must be one of ['actionable', 'conceptual', 'experiential']\n        - confidence (integer): 1-10, how well-supported this insight is\n        \n        Focus on insights that are:\n        - Valuable and non-obvious\n        - Well-supported by the conversation\n        - Practical or thought-provoking\n        - Representative of the episode's key themes\n        \n        Return ONLY the JSON array, no other text.\n        \"\"\"\n    else:\n        # Standard prompt for smaller context windows\n        prompt_template = \"\"\"\n        Extract 3-8 valuable insights from this podcast transcript segment.\n        \n        PODCAST: {podcast_name}\n        EPISODE: {episode_title}\n        \n        TRANSCRIPT SEGMENT:\n        {segment_text}\n        \n        For each insight, provide:\n        - title: Concise, informative title\n        - description: Clear explanation (1-3 sentences)\n        - insight_type: One of ['actionable', 'conceptual', 'experiential']\n        - confidence: 1-10\n        \n        Return as JSON array only.\n        \"\"\"\n    \n    return prompt_template\n\ndef build_combined_extraction_prompt(podcast_name, episode_title, use_large_context=True):\n    \"\"\"Build a combined prompt for extracting insights, entities, and quotes in one call.\"\"\"\n    \n    if use_large_context:\n        prompt_template = \"\"\"\n        You are analyzing a complete podcast episode. Extract structured knowledge including insights, entities, and notable quotes.\n        \n        PODCAST: {podcast_name}\n        EPISODE: {episode_title}\n        \n        FULL TRANSCRIPT:\n        {segment_text}\n        \n        Extract the following:\n        \n        1. INSIGHTS (5-15 items):\n           - title: Concise, informative title\n           - description: Clear explanation (1-3 sentences)\n           - insight_type: One of ['actionable', 'conceptual', 'experiential']\n           - confidence: 1-10\n           - timestamp_reference: Approximate time when discussed (if mentioned)\n        \n        2. ENTITIES (10-30 items):\n           - name: Entity name as mentioned\n           - type: One of ['person', 'company', 'product', 'technology', 'concept', 'location', 'event']\n           - description: Brief description based on context (1-2 sentences)\n           - aliases: Alternative names mentioned (list)\n           - importance: 1-10 based on discussion emphasis\n        \n        3. NOTABLE QUOTES (5-10 items):\n           - text: The exact quote\n           - speaker: Who said it (if identifiable)\n           - context: Brief context (1 sentence)\n           - significance: Why this quote is notable\n        \n        Return as JSON object with three arrays: insights, entities, quotes\n        \n        Focus on:\n        - Cross-episode themes and connections\n        - Key takeaways and actionable advice\n        - Important people, companies, and concepts\n        - Memorable and impactful quotes\n        \n        {format_json}\n        \"\"\"\n    else:\n        prompt_template = \"\"\"\n        Extract insights, entities, and quotes from this podcast segment.\n        \n        PODCAST: {podcast_name}\n        EPISODE: {episode_title}\n        \n        SEGMENT:\n        {segment_text}\n        \n        Return JSON with:\n        - insights: [{title, description, insight_type, confidence}]\n        - entities: [{name, type, description}]\n        - quotes: [{text, speaker, context}]\n        \n        {format_json}\n        \"\"\"\n    \n    format_json = \"\"\"\n    Return ONLY valid JSON in this exact format:\n    {\n        \"insights\": [...],\n        \"entities\": [...],\n        \"quotes\": [...]\n    }\n    \"\"\"\n    \n    return prompt_template.format(format_json=format_json)\n\ndef build_entity_extraction_prompt(use_large_context=True):\n    \"\"\"Build prompt for entity extraction.\"\"\"\n    \n    if use_large_context:\n        return \"\"\"\n        Extract all mentioned entities (people, companies, products, technologies, concepts, locations, events).\n        \n        For each entity provide:\n        - name: As mentioned in the text\n        - type: Category of entity\n        - description: Context from the discussion\n        - aliases: Alternative names used\n        - first_mention: Where first discussed\n        \n        Return as JSON array of entity objects.\n        Focus on entities that are central to the discussion.\n        \"\"\"\n    else:\n        return \"\"\"\n        Extract entities from this text.\n        Return JSON array with: name, type, description\n        \"\"\"\n\n# Test prompt building\nprint(\"‚úÖ Prompt building functions ready\")\nprint(f\"  ‚Ä¢ Large context support: {USE_LARGE_CONTEXT}\")\nprint(f\"  ‚Ä¢ Combined extraction: ENABLED\")\nprint(f\"  ‚Ä¢ Entity extraction: ENABLED\")\n\n# Example prompt preview\nif USE_LARGE_CONTEXT:\n    print(\"\\nüìù Sample prompt structure:\")\n    sample = build_insight_extraction_prompt(\"Test Podcast\", \"Test Episode\", True)\n    print(f\"  ‚Ä¢ Prompt length: {len(sample)} characters\")\n    print(f\"  ‚Ä¢ Optimized for: Large context window (1M tokens)\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 6.2: Entity Resolution & Deduplication\n\n**What this does:**\n- Normalizes entity names for comparison\n- Finds duplicates using fuzzy matching\n- Extracts aliases from descriptions\n- Maintains entity consistency across episodes\n\n**Key features:**\n- Fuzzy name matching with difflib\n- Alias extraction from context\n- Cross-episode entity linking\n- Confidence scoring for matches",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def normalize_entity_name(name):\n    \"\"\"\n    Normalize entity name for comparison.\n    \n    Args:\n        name: Entity name to normalize\n        \n    Returns:\n        Normalized name\n    \"\"\"\n    if not name:\n        return \"\"\n    \n    # Convert to lowercase\n    normalized = name.lower().strip()\n    \n    # Remove common suffixes\n    suffixes = [' inc', ' inc.', ' corp', ' corp.', ' corporation', ' llc', \n                ' ltd', ' ltd.', ' limited', ' co', ' co.', ' company']\n    \n    for suffix in suffixes:\n        if normalized.endswith(suffix):\n            normalized = normalized[:-len(suffix)].strip()\n            break\n    \n    # Remove special characters but keep spaces\n    normalized = re.sub(r'[^\\w\\s]', '', normalized)\n    \n    # Normalize whitespace\n    normalized = ' '.join(normalized.split())\n    \n    return normalized\n\ndef calculate_name_similarity(name1, name2):\n    \"\"\"\n    Calculate similarity between two entity names.\n    \n    Args:\n        name1: First entity name\n        name2: Second entity name\n        \n    Returns:\n        Similarity score (0-1)\n    \"\"\"\n    # Normalize names\n    norm1 = normalize_entity_name(name1)\n    norm2 = normalize_entity_name(name2)\n    \n    if not norm1 or not norm2:\n        return 0.0\n    \n    # Exact match after normalization\n    if norm1 == norm2:\n        return 1.0\n    \n    # Use SequenceMatcher for fuzzy matching\n    from difflib import SequenceMatcher\n    base_similarity = SequenceMatcher(None, norm1, norm2).ratio()\n    \n    # Check for subset relationships (one name contains the other)\n    if norm1 in norm2 or norm2 in norm1:\n        base_similarity = max(base_similarity, 0.8)\n    \n    # Check for common word overlap\n    words1 = set(norm1.split())\n    words2 = set(norm2.split())\n    \n    if words1 and words2:\n        overlap = len(words1.intersection(words2))\n        total = len(words1.union(words2))\n        word_similarity = overlap / total if total > 0 else 0\n        \n        # Weight word similarity higher for multi-word names\n        if len(words1) > 1 or len(words2) > 1:\n            base_similarity = max(base_similarity, word_similarity * 0.9)\n    \n    return base_similarity\n\ndef extract_entity_aliases(entity_description):\n    \"\"\"\n    Extract potential aliases from entity description.\n    \n    Args:\n        entity_description: Description text that might contain aliases\n        \n    Returns:\n        List of potential aliases\n    \"\"\"\n    aliases = []\n    \n    if not entity_description:\n        return aliases\n    \n    # Common alias patterns\n    alias_patterns = [\n        r'also known as ([^,\\.]+)',\n        r'aka ([^,\\.]+)',\n        r'formerly ([^,\\.]+)',\n        r'now called ([^,\\.]+)',\n        r'previously ([^,\\.]+)',\n        r'operating as ([^,\\.]+)',\n        r'doing business as ([^,\\.]+)',\n        r'd\\.b\\.a\\. ([^,\\.]+)',\n        r'\\(([^)]+)\\)',  # Parenthetical names\n    ]\n    \n    for pattern in alias_patterns:\n        matches = re.findall(pattern, entity_description, re.IGNORECASE)\n        aliases.extend(matches)\n    \n    # Clean up aliases\n    cleaned_aliases = []\n    for alias in aliases:\n        alias = alias.strip()\n        # Skip if too short or too generic\n        if len(alias) > 2 and not alias.lower() in ['the', 'a', 'an', 'inc', 'corp']:\n            cleaned_aliases.append(alias)\n    \n    return list(set(cleaned_aliases))  # Remove duplicates\n\ndef find_existing_entity(entity_name, existing_entities, threshold=0.8):\n    \"\"\"\n    Find if an entity already exists in the database.\n    \n    Args:\n        entity_name: Name of entity to check\n        existing_entities: List of existing entities with names and aliases\n        threshold: Similarity threshold for matching\n        \n    Returns:\n        Matched entity or None\n    \"\"\"\n    best_match = None\n    best_score = 0\n    \n    for existing in existing_entities:\n        # Check against main name\n        score = calculate_name_similarity(entity_name, existing['name'])\n        \n        if score > best_score:\n            best_score = score\n            best_match = existing\n        \n        # Check against aliases\n        for alias in existing.get('aliases', []):\n            alias_score = calculate_name_similarity(entity_name, alias)\n            if alias_score > best_score:\n                best_score = alias_score\n                best_match = existing\n    \n    # Return match if above threshold\n    if best_score >= threshold:\n        return best_match\n    \n    return None\n\nclass EntityResolver:\n    \"\"\"Handles entity resolution and deduplication across episodes.\"\"\"\n    \n    def __init__(self, neo4j_session=None):\n        self.neo4j_session = neo4j_session\n        self.entity_cache = {}\n        \n    def load_existing_entities(self, entity_type=None):\n        \"\"\"Load existing entities from Neo4j.\"\"\"\n        if not self.neo4j_session:\n            return []\n        \n        query = \"\"\"\n        MATCH (e:Entity)\n        WHERE $entity_type IS NULL OR e.type = $entity_type\n        RETURN e.id as id, e.name as name, e.type as type, \n               e.aliases as aliases, e.global_id as global_id\n        \"\"\"\n        \n        result = self.neo4j_session.run(query, entity_type=entity_type)\n        entities = []\n        \n        for record in result:\n            entities.append({\n                'id': record['id'],\n                'name': record['name'],\n                'type': record['type'],\n                'aliases': record['aliases'] or [],\n                'global_id': record['global_id']\n            })\n        \n        return entities\n    \n    def resolve_entities(self, new_entities, threshold=0.8):\n        \"\"\"\n        Resolve new entities against existing ones.\n        \n        Args:\n            new_entities: List of new entities to resolve\n            threshold: Similarity threshold\n            \n        Returns:\n            List of resolved entities with match information\n        \"\"\"\n        resolved = []\n        \n        # Load existing entities by type for efficiency\n        entities_by_type = {}\n        \n        for entity in new_entities:\n            entity_type = entity.get('type', 'unknown')\n            \n            # Load entities of this type if not cached\n            if entity_type not in entities_by_type:\n                entities_by_type[entity_type] = self.load_existing_entities(entity_type)\n            \n            # Try to find match\n            existing_match = find_existing_entity(\n                entity['name'],\n                entities_by_type[entity_type],\n                threshold\n            )\n            \n            if existing_match:\n                # Update existing entity\n                entity['matched_to'] = existing_match['id']\n                entity['global_id'] = existing_match['global_id']\n                entity['match_type'] = 'existing'\n                \n                # Merge aliases\n                all_aliases = set(existing_match.get('aliases', []))\n                all_aliases.update(entity.get('aliases', []))\n                # Add the new name as an alias if different\n                if normalize_entity_name(entity['name']) != normalize_entity_name(existing_match['name']):\n                    all_aliases.add(entity['name'])\n                entity['aliases'] = list(all_aliases)\n            else:\n                # New entity\n                entity['match_type'] = 'new'\n                entity['global_id'] = f\"entity_{hashlib.md5(entity['name'].encode()).hexdigest()[:16]}\"\n            \n            resolved.append(entity)\n        \n        return resolved\n\n# Test entity resolution\nprint(\"‚úÖ Entity resolution system ready\")\n\n# Test examples\ntest_entities = [\n    {\"name\": \"OpenAI Inc.\", \"type\": \"company\"},\n    {\"name\": \"OpenAI\", \"type\": \"company\"},\n    {\"name\": \"Google LLC\", \"type\": \"company\"},\n    {\"name\": \"Google\", \"type\": \"company\"},\n]\n\nprint(\"\\nüß™ Testing entity resolution:\")\nfor i in range(0, len(test_entities), 2):\n    name1 = test_entities[i]['name']\n    name2 = test_entities[i+1]['name']\n    similarity = calculate_name_similarity(name1, name2)\n    print(f\"  ‚Ä¢ '{name1}' vs '{name2}': {similarity:.2f} similarity\")\n\n# Test alias extraction\ntest_desc = \"OpenAI (formerly OpenAI LP), also known as Open AI, is an AI research company.\"\naliases = extract_entity_aliases(test_desc)\nprint(f\"\\nüìù Extracted aliases from description: {aliases}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 6.3: Knowledge Extraction Functions\n\n**What this does:**\n- Main extraction function that uses LLM to extract insights\n- Handles both segmented and full-transcript extraction\n- Parses and validates LLM responses\n- Supports combined extraction for efficiency\n\n**Key features:**\n- Smart routing based on transcript size\n- JSON response parsing with error handling\n- Batch processing for large transcripts\n- Progress tracking",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def parse_llm_json_response(response_text):\n    \"\"\"\n    Parse JSON response from LLM with error handling.\n    \n    Args:\n        response_text: Raw response text from LLM\n        \n    Returns:\n        Parsed JSON object or None\n    \"\"\"\n    try:\n        # Try direct JSON parsing\n        return json.loads(response_text)\n    except json.JSONDecodeError:\n        # Try to extract JSON from text\n        import re\n        \n        # Look for JSON array pattern\n        array_match = re.search(r'\\[\\s*\\{.*\\}\\s*\\]', response_text, re.DOTALL)\n        if array_match:\n            try:\n                return json.loads(array_match.group())\n            except:\n                pass\n        \n        # Look for JSON object pattern\n        object_match = re.search(r'\\{\\s*\".*\"\\s*:\\s*.*\\}', response_text, re.DOTALL)\n        if object_match:\n            try:\n                return json.loads(object_match.group())\n            except:\n                pass\n        \n        print(\"Failed to parse LLM response as JSON\")\n        return None\n\ndef extract_insights_from_transcript(transcript_text, podcast_name, episode_title, \n                                   llm_client=None, use_large_context=True):\n    \"\"\"\n    Extract insights from podcast transcript using LLM.\n    \n    Args:\n        transcript_text: Full transcript or segment text\n        podcast_name: Name of the podcast\n        episode_title: Title of the episode\n        llm_client: LLM client for extraction\n        use_large_context: Whether to use large context prompts\n        \n    Returns:\n        List of extracted insights\n    \"\"\"\n    if not llm_client:\n        print(\"No LLM client available for insight extraction\")\n        return []\n    \n    if not ENABLE_KNOWLEDGE_EXTRACTION:\n        print(\"Knowledge extraction disabled\")\n        return []\n    \n    try:\n        # Get prompt template\n        prompt_template = build_insight_extraction_prompt(\n            podcast_name, episode_title, use_large_context\n        )\n        \n        # Format prompt with transcript\n        prompt = prompt_template.format(\n            podcast_name=podcast_name,\n            episode_title=episode_title,\n            segment_text=transcript_text\n        )\n        \n        # Route task through rate limiter\n        routing_info = task_router.route_task('insight_extraction', len(transcript_text))\n        \n        # Update LLM client if needed\n        if hasattr(task_router, 'get_llm_client'):\n            llm_client = task_router.get_llm_client(routing_info)\n        \n        # Call LLM\n        print(f\"ü§ñ Extracting insights using {routing_info['model']}...\")\n        response = llm_client.invoke(prompt)\n        \n        # Record usage\n        rate_limiter.record_usage(\n            routing_info['model'],\n            routing_info['estimated_tokens']\n        )\n        \n        # Parse response\n        insights = []\n        parsed = parse_llm_json_response(response.content)\n        \n        if isinstance(parsed, list):\n            insights = parsed\n        elif isinstance(parsed, dict) and 'insights' in parsed:\n            insights = parsed['insights']\n        \n        # Validate insights\n        valid_insights = []\n        for insight in insights:\n            if isinstance(insight, dict) and 'title' in insight and 'description' in insight:\n                # Ensure required fields\n                insight.setdefault('insight_type', 'conceptual')\n                insight.setdefault('confidence', 7)\n                valid_insights.append(insight)\n        \n        print(f\"‚úÖ Extracted {len(valid_insights)} insights\")\n        return valid_insights\n        \n    except Exception as e:\n        print(f\"Error extracting insights: {e}\")\n        return []\n\ndef extract_entities_from_transcript(transcript_text, llm_client=None, use_large_context=True):\n    \"\"\"Extract entities from transcript using LLM.\"\"\"\n    if not llm_client or not ENABLE_KNOWLEDGE_EXTRACTION:\n        return []\n    \n    try:\n        prompt_template = build_entity_extraction_prompt(use_large_context)\n        prompt = f\"{prompt_template}\\n\\nTRANSCRIPT:\\n{transcript_text}\"\n        \n        # Route task\n        routing_info = task_router.route_task('entity_extraction', len(transcript_text))\n        \n        print(f\"ü§ñ Extracting entities using {routing_info['model']}...\")\n        response = llm_client.invoke(prompt)\n        \n        # Record usage\n        rate_limiter.record_usage(\n            routing_info['model'],\n            routing_info['estimated_tokens']\n        )\n        \n        # Parse response\n        entities = []\n        parsed = parse_llm_json_response(response.content)\n        \n        if isinstance(parsed, list):\n            entities = parsed\n        elif isinstance(parsed, dict) and 'entities' in parsed:\n            entities = parsed['entities']\n        \n        print(f\"‚úÖ Extracted {len(entities)} entities\")\n        return entities\n        \n    except Exception as e:\n        print(f\"Error extracting entities: {e}\")\n        return []\n\ndef extract_combined_knowledge(transcript_text, podcast_name, episode_title, \n                             llm_client=None, use_large_context=True):\n    \"\"\"\n    Extract insights, entities, and quotes in a single LLM call.\n    More efficient than separate calls.\n    \"\"\"\n    if not llm_client or not ENABLE_KNOWLEDGE_EXTRACTION:\n        return {'insights': [], 'entities': [], 'quotes': []}\n    \n    try:\n        # Get combined prompt\n        prompt_template = build_combined_extraction_prompt(\n            podcast_name, episode_title, use_large_context\n        )\n        \n        prompt = prompt_template.format(\n            podcast_name=podcast_name,\n            episode_title=episode_title,\n            segment_text=transcript_text\n        )\n        \n        # Route as high-priority task\n        routing_info = task_router.route_task('combined_extraction', len(transcript_text))\n        \n        print(f\"ü§ñ Performing combined extraction using {routing_info['model']}...\")\n        response = llm_client.invoke(prompt)\n        \n        # Record usage\n        rate_limiter.record_usage(\n            routing_info['model'],\n            routing_info['estimated_tokens']\n        )\n        \n        # Parse response\n        parsed = parse_llm_json_response(response.content)\n        \n        if not parsed:\n            return {'insights': [], 'entities': [], 'quotes': []}\n        \n        # Extract components\n        result = {\n            'insights': parsed.get('insights', []),\n            'entities': parsed.get('entities', []),\n            'quotes': parsed.get('quotes', [])\n        }\n        \n        print(f\"‚úÖ Combined extraction complete:\")\n        print(f\"  ‚Ä¢ Insights: {len(result['insights'])}\")\n        print(f\"  ‚Ä¢ Entities: {len(result['entities'])}\")\n        print(f\"  ‚Ä¢ Quotes: {len(result['quotes'])}\")\n        \n        return result\n        \n    except Exception as e:\n        print(f\"Error in combined extraction: {e}\")\n        return {'insights': [], 'entities': [], 'quotes': []}\n\n# Wrapper function for simplified extraction\ndef extract_knowledge_from_episode(episode_data, llm_client=None):\n    \"\"\"\n    Main function to extract all knowledge from an episode.\n    \n    Args:\n        episode_data: Dictionary with episode information and transcript\n        llm_client: LLM client for extraction\n        \n    Returns:\n        Dictionary with insights, entities, quotes, and relationships\n    \"\"\"\n    podcast_name = episode_data.get('podcast_name', 'Unknown Podcast')\n    episode_title = episode_data.get('title', 'Unknown Episode')\n    transcript = episode_data.get('transcript', '')\n    \n    if not transcript:\n        print(\"No transcript available for extraction\")\n        return {'insights': [], 'entities': [], 'quotes': [], 'relationships': []}\n    \n    # Determine if we can use large context\n    use_large_context = USE_LARGE_CONTEXT and len(transcript) < 900000  # ~900k chars\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"üìö KNOWLEDGE EXTRACTION: {episode_title}\")\n    print(f\"{'='*60}\")\n    print(f\"  ‚Ä¢ Transcript length: {len(transcript):,} characters\")\n    print(f\"  ‚Ä¢ Using {'large' if use_large_context else 'standard'} context mode\")\n    \n    # Perform combined extraction\n    results = extract_combined_knowledge(\n        transcript, podcast_name, episode_title, \n        llm_client, use_large_context\n    )\n    \n    # Extract relationships if we have entities\n    if results['entities'] and len(results['entities']) >= 2:\n        print(\"\\nüîó Extracting relationships...\")\n        relationships = extract_relationships_from_entities(\n            results['entities'], transcript, llm_client\n        )\n        results['relationships'] = relationships\n        print(f\"‚úÖ Extracted {len(relationships)} relationships\")\n    else:\n        results['relationships'] = []\n    \n    # Resolve entities\n    if results['entities']:\n        print(\"\\nüîç Resolving entities...\")\n        resolver = EntityResolver()\n        results['entities'] = resolver.resolve_entities(results['entities'])\n        \n        # Count resolution stats\n        new_entities = sum(1 for e in results['entities'] if e.get('match_type') == 'new')\n        matched_entities = len(results['entities']) - new_entities\n        print(f\"  ‚Ä¢ New entities: {new_entities}\")\n        print(f\"  ‚Ä¢ Matched to existing: {matched_entities}\")\n    \n    # Extract additional metadata\n    for entity in results['entities']:\n        # Extract aliases from description\n        if entity.get('description'):\n            aliases = extract_entity_aliases(entity['description'])\n            if aliases:\n                entity.setdefault('aliases', []).extend(aliases)\n                entity['aliases'] = list(set(entity['aliases']))  # Deduplicate\n    \n    return results\n\nprint(\"‚úÖ Knowledge extraction functions ready\")\nprint(f\"  ‚Ä¢ Combined extraction: ENABLED\")\nprint(f\"  ‚Ä¢ Entity resolution: ENABLED\")\nprint(f\"  ‚Ä¢ Relationship extraction: READY\")\nprint(f\"  ‚Ä¢ Large context support: {USE_LARGE_CONTEXT}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 6.4: Relationship Extraction\n\n**What this does:**\n- Extracts semantic relationships between entities\n- Identifies 5 types of relationships\n- Uses context to determine relationship strength\n- Handles bidirectional and conditional relationships\n\n**Relationship Types:**\n1. **Hierarchical**: Parent-child, ownership, part-of\n2. **Influential**: Advises, mentors, inspires\n3. **Comparative**: Competes with, similar to, contrasts with\n4. **Temporal**: Preceded by, followed by, concurrent with\n5. **Functional**: Uses, enables, depends on",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class RelationshipExtractor:\n    \"\"\"Extracts typed relationships between entities.\"\"\"\n    \n    # Relationship taxonomy\n    RELATIONSHIP_TYPES = {\n        'hierarchical': {\n            'patterns': ['owns', 'parent company', 'subsidiary', 'part of', 'division of',\n                        'founded', 'created', 'established', 'acquired', 'merged with'],\n            'subtypes': ['owns', 'parent_of', 'subsidiary_of', 'part_of', 'founded_by']\n        },\n        'influential': {\n            'patterns': ['advises', 'mentors', 'influenced', 'inspired', 'taught',\n                        'learned from', 'follows', 'looks up to', 'guided by'],\n            'subtypes': ['advises', 'mentors', 'influences', 'inspires', 'teaches']\n        },\n        'comparative': {\n            'patterns': ['competes with', 'rivals', 'alternative to', 'similar to',\n                        'different from', 'better than', 'worse than', 'compared to'],\n            'subtypes': ['competes_with', 'similar_to', 'alternative_to', 'contrasts_with']\n        },\n        'temporal': {\n            'patterns': ['before', 'after', 'preceded', 'followed', 'replaced',\n                        'succeeded', 'evolved into', 'transformed into'],\n            'subtypes': ['preceded_by', 'followed_by', 'replaced_by', 'concurrent_with']\n        },\n        'functional': {\n            'patterns': ['uses', 'relies on', 'depends on', 'enables', 'powers',\n                        'integrates with', 'works with', 'built on', 'based on'],\n            'subtypes': ['uses', 'enables', 'depends_on', 'integrates_with', 'powers']\n        }\n    }\n    \n    def __init__(self):\n        self.compiled_patterns = self._compile_patterns()\n        \n    def _compile_patterns(self):\n        \"\"\"Compile regex patterns for efficient matching.\"\"\"\n        compiled = {}\n        for rel_type, info in self.RELATIONSHIP_TYPES.items():\n            patterns = info['patterns']\n            # Create regex pattern\n            pattern_str = '|'.join(f'\\\\b{p}\\\\b' for p in patterns)\n            compiled[rel_type] = re.compile(pattern_str, re.IGNORECASE)\n        return compiled\n    \n    def extract_relationships(self, entities, context_text, llm_client=None):\n        \"\"\"\n        Extract relationships between entities from context.\n        \n        Args:\n            entities: List of entities to find relationships between\n            context_text: Text context to analyze\n            llm_client: Optional LLM for enhanced extraction\n            \n        Returns:\n            List of extracted relationships\n        \"\"\"\n        relationships = []\n        \n        # First, try pattern-based extraction\n        pattern_relationships = self._extract_pattern_based(entities, context_text)\n        relationships.extend(pattern_relationships)\n        \n        # Then, use LLM for more complex relationships\n        if llm_client and len(entities) >= 2:\n            llm_relationships = self._extract_llm_based(entities, context_text, llm_client)\n            relationships.extend(llm_relationships)\n        \n        # Deduplicate and merge\n        relationships = self._deduplicate_relationships(relationships)\n        \n        return relationships\n    \n    def _extract_pattern_based(self, entities, context_text):\n        \"\"\"Extract relationships using pattern matching.\"\"\"\n        relationships = []\n        entity_names = [e['name'] for e in entities]\n        \n        # Check each pair of entities\n        for i, entity1 in enumerate(entities):\n            for j, entity2 in enumerate(entities[i+1:], i+1):\n                # Find sentences mentioning both entities\n                sentences = self._find_connecting_sentences(\n                    entity1['name'], entity2['name'], context_text\n                )\n                \n                for sentence in sentences:\n                    # Check each relationship type\n                    for rel_type, pattern in self.compiled_patterns.items():\n                        if pattern.search(sentence):\n                            # Determine direction\n                            e1_pos = sentence.lower().find(entity1['name'].lower())\n                            e2_pos = sentence.lower().find(entity2['name'].lower())\n                            \n                            if e1_pos < e2_pos:\n                                source, target = entity1, entity2\n                            else:\n                                source, target = entity2, entity1\n                            \n                            relationships.append({\n                                'source': source['name'],\n                                'target': target['name'],\n                                'type': rel_type,\n                                'evidence': sentence[:200],\n                                'confidence': 0.8,\n                                'extraction_method': 'pattern'\n                            })\n        \n        return relationships\n    \n    def _extract_llm_based(self, entities, context_text, llm_client):\n        \"\"\"Use LLM to extract more complex relationships.\"\"\"\n        try:\n            # Prepare entity list\n            entity_list = [f\"{e['name']} ({e.get('type', 'unknown')})\" for e in entities[:20]]\n            \n            prompt = f\"\"\"\n            Analyze the relationships between these entities based on the context:\n            \n            ENTITIES:\n            {', '.join(entity_list)}\n            \n            CONTEXT:\n            {context_text[:3000]}\n            \n            Extract relationships between entities. For each relationship:\n            - source: Entity name (exactly as listed)\n            - target: Entity name (exactly as listed)\n            - relationship_type: One of [hierarchical, influential, comparative, temporal, functional]\n            - specific_relation: Specific type (e.g., \"owns\", \"competes_with\", \"mentors\")\n            - confidence: 0.0-1.0\n            - evidence: Brief quote supporting this relationship\n            \n            Return as JSON array. Only include relationships strongly supported by the context.\n            \"\"\"\n            \n            # Route through task router\n            routing_info = task_router.route_task('relationship_extraction', len(prompt))\n            response = llm_client.invoke(prompt)\n            \n            # Parse response\n            relationships = []\n            parsed = parse_llm_json_response(response.content)\n            \n            if isinstance(parsed, list):\n                for rel in parsed:\n                    if self._validate_relationship(rel, entity_list):\n                        rel['extraction_method'] = 'llm'\n                        relationships.append(rel)\n            \n            return relationships\n            \n        except Exception as e:\n            print(f\"Error in LLM relationship extraction: {e}\")\n            return []\n    \n    def _find_connecting_sentences(self, entity1, entity2, text):\n        \"\"\"Find sentences that mention both entities.\"\"\"\n        sentences = re.split(r'[.!?]+', text)\n        connecting = []\n        \n        e1_lower = entity1.lower()\n        e2_lower = entity2.lower()\n        \n        for sentence in sentences:\n            sentence_lower = sentence.lower()\n            if e1_lower in sentence_lower and e2_lower in sentence_lower:\n                connecting.append(sentence.strip())\n        \n        return connecting\n    \n    def _validate_relationship(self, rel, valid_entities):\n        \"\"\"Validate that a relationship has required fields.\"\"\"\n        required = ['source', 'target', 'relationship_type']\n        \n        # Check required fields\n        for field in required:\n            if field not in rel:\n                return False\n        \n        # Validate relationship type\n        if rel['relationship_type'] not in self.RELATIONSHIP_TYPES:\n            return False\n        \n        return True\n    \n    def _deduplicate_relationships(self, relationships):\n        \"\"\"Remove duplicate relationships, keeping highest confidence.\"\"\"\n        unique = {}\n        \n        for rel in relationships:\n            # Create unique key\n            key = (rel['source'], rel['target'], rel.get('relationship_type', 'unknown'))\n            \n            if key not in unique or rel.get('confidence', 0) > unique[key].get('confidence', 0):\n                unique[key] = rel\n        \n        return list(unique.values())\n\ndef extract_relationships_from_entities(entities, context_text, llm_client=None):\n    \"\"\"\n    Wrapper function to extract relationships between entities.\n    \n    Args:\n        entities: List of entities\n        context_text: Context to analyze\n        llm_client: LLM client\n        \n    Returns:\n        List of relationships\n    \"\"\"\n    if len(entities) < 2:\n        return []\n    \n    extractor = RelationshipExtractor()\n    relationships = extractor.extract_relationships(entities, context_text, llm_client)\n    \n    # Add global IDs\n    for rel in relationships:\n        rel['id'] = f\"rel_{hashlib.md5(f\\\"{rel['source']}_{rel['target']}_{rel.get('type', '')}\\\".encode()).hexdigest()[:16]}\"\n    \n    return relationships\n\n# Test relationship extraction\nprint(\"‚úÖ Relationship extraction system ready\")\n\n# Show relationship types\nprint(\"\\nüìã Relationship taxonomy:\")\nextractor = RelationshipExtractor()\nfor rel_type, info in extractor.RELATIONSHIP_TYPES.items():\n    print(f\"\\n  ‚Ä¢ {rel_type.upper()}:\")\n    print(f\"    Subtypes: {', '.join(info['subtypes'])}\")\n\n# Test pattern matching\ntest_text = \"Google owns YouTube. Microsoft competes with Google in cloud services.\"\ntest_entities = [\n    {'name': 'Google', 'type': 'company'},\n    {'name': 'YouTube', 'type': 'company'},\n    {'name': 'Microsoft', 'type': 'company'}\n]\n\nprint(\"\\nüß™ Testing pattern-based extraction:\")\ntest_rels = extractor._extract_pattern_based(test_entities, test_text)\nfor rel in test_rels:\n    print(f\"  ‚Ä¢ {rel['source']} ‚Üí {rel['target']} ({rel['type']})\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "def validate_and_enhance_insights(insights, use_large_context=True):\n    \"\"\"Validate and enhance the extracted insights.\"\"\"\n    validated_insights = []\n    \n    for insight in insights:\n        try:\n            # Ensure required fields exist\n            if not all(field in insight for field in [\"title\", \"description\", \"insight_type\"]):\n                print(f\"Warning: Skipping invalid insight missing required fields: {insight}\")\n                continue\n            \n            # Add default values for optional fields\n            if use_large_context:\n                if \"confidence\" not in insight:\n                    insight[\"confidence\"] = 7  # Default mid-high confidence\n                if \"references\" not in insight:\n                    insight[\"references\"] = []\n            \n            # Validate insight_type\n            valid_types = [\"actionable\", \"conceptual\", \"experiential\"]\n            if insight[\"insight_type\"] not in valid_types:\n                insight[\"insight_type\"] = \"conceptual\"  # Default type\n            \n            # Ensure strings are not empty\n            if not insight[\"title\"].strip() or not insight[\"description\"].strip():\n                print(f\"Warning: Skipping insight with empty title or description\")\n                continue\n                \n            validated_insights.append(insight)\n            \n        except Exception as e:\n            print(f\"Warning: Error validating insight: {e}\")\n            continue\n    \n    return validated_insights\n\ndef validate_extraction_results(extraction_results):\n    \"\"\"Validate extraction results and provide quality metrics.\"\"\"\n    validation_report = {\n        'total_insights': 0,\n        'total_entities': 0,\n        'total_quotes': 0,\n        'quality_issues': [],\n        'entity_types': {},\n        'insight_types': {},\n        'extraction_quality': 'good'\n    }\n    \n    # Validate insights\n    if 'insights' in extraction_results:\n        insights = extraction_results['insights']\n        validation_report['total_insights'] = len(insights)\n        \n        for insight in insights:\n            insight_type = insight.get('insight_type', 'unknown')\n            validation_report['insight_types'][insight_type] = \\\n                validation_report['insight_types'].get(insight_type, 0) + 1\n            \n            # Check quality\n            if len(insight.get('title', '')) > 50:\n                validation_report['quality_issues'].append(\n                    f\"Insight title too long: {insight['title'][:50]}...\"\n                )\n            if len(insight.get('description', '')) > 500:\n                validation_report['quality_issues'].append(\n                    f\"Insight description too long\"\n                )\n    \n    # Validate entities\n    if 'entities' in extraction_results:\n        entities = extraction_results['entities']\n        validation_report['total_entities'] = len(entities)\n        \n        for entity in entities:\n            entity_type = entity.get('type', 'unknown')\n            validation_report['entity_types'][entity_type] = \\\n                validation_report['entity_types'].get(entity_type, 0) + 1\n            \n            # Check for duplicates (basic check)\n            if not entity.get('name'):\n                validation_report['quality_issues'].append(\n                    \"Entity missing name\"\n                )\n    \n    # Validate quotes\n    if 'quotes' in extraction_results:\n        quotes = extraction_results['quotes']\n        validation_report['total_quotes'] = len(quotes)\n        \n        for quote in quotes:\n            quote_len = len(quote.get('text', '').split())\n            if quote_len < 5 or quote_len > 100:\n                validation_report['quality_issues'].append(\n                    f\"Quote length not optimal: {quote_len} words\"\n                )\n    \n    # Determine overall quality\n    issue_count = len(validation_report['quality_issues'])\n    total_items = (validation_report['total_insights'] + \n                   validation_report['total_entities'] + \n                   validation_report['total_quotes'])\n    \n    if total_items == 0:\n        validation_report['extraction_quality'] = 'poor'\n    elif issue_count > total_items * 0.3:\n        validation_report['extraction_quality'] = 'fair'\n    elif issue_count > total_items * 0.1:\n        validation_report['extraction_quality'] = 'good'\n    else:\n        validation_report['extraction_quality'] = 'excellent'\n    \n    return validation_report\n\ndef consolidate_segment_extractions(segment_extractions):\n    \"\"\"Consolidate extractions from multiple segments.\"\"\"\n    consolidated = {\n        'insights': [],\n        'entities': {},  # Use dict for deduplication\n        'quotes': []\n    }\n    \n    # Consolidate insights\n    for extraction in segment_extractions:\n        if 'insights' in extraction:\n            consolidated['insights'].extend(extraction['insights'])\n    \n    # Consolidate entities with deduplication\n    for extraction in segment_extractions:\n        if 'entities' in extraction:\n            for entity in extraction['entities']:\n                entity_key = (entity.get('name', '').lower(), entity.get('type', ''))\n                if entity_key not in consolidated['entities']:\n                    consolidated['entities'][entity_key] = entity\n                else:\n                    # Update frequency if applicable\n                    existing = consolidated['entities'][entity_key]\n                    existing['frequency'] = existing.get('frequency', 1) + 1\n    \n    # Convert entities dict back to list\n    consolidated['entities'] = list(consolidated['entities'].values())\n    \n    # Consolidate quotes\n    for extraction in segment_extractions:\n        if 'quotes' in extraction:\n            consolidated['quotes'].extend(extraction['quotes'])\n    \n    return consolidated\n\n# Example usage function\ndef validate_extraction_example():\n    \"\"\"Example of validation in action.\"\"\"\n    sample_extraction = {\n        'insights': [\n            {\n                'title': 'Sleep is crucial',\n                'description': 'Getting 7-9 hours of quality sleep improves cognitive function.',\n                'insight_type': 'actionable',\n                'confidence': 8\n            }\n        ],\n        'entities': [\n            {\n                'name': 'Matthew Walker',\n                'type': 'Person',\n                'description': 'Sleep researcher and author',\n                'importance': 9\n            }\n        ],\n        'quotes': [\n            {\n                'text': 'Sleep is the single most effective thing we can do for our health.',\n                'speaker': 'Matthew Walker',\n                'context': 'Discussing sleep importance'\n            }\n        ]\n    }\n    \n    validation = validate_extraction_results(sample_extraction)\n    print(\"Extraction Validation Report:\")\n    print(f\"- Total insights: {validation['total_insights']}\")\n    print(f\"- Total entities: {validation['total_entities']}\")\n    print(f\"- Total quotes: {validation['total_quotes']}\")\n    print(f\"- Quality: {validation['extraction_quality']}\")\n    print(f\"- Issues: {len(validation['quality_issues'])}\")\n    \n    return validation\n\n# Run example\nif __name__ == \"__main__\":\n    validate_extraction_example()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def create_topic_nodes(session, topics, episode_id, podcast_id):\n    \"\"\"\n    Create Topic nodes and relationships in Neo4j.\n    \n    Args:\n        session: Neo4j session\n        topics: List of topic dictionaries\n        episode_id: Episode identifier\n        podcast_id: Podcast identifier\n        \n    Returns:\n        List of created topic names\n    \"\"\"\n    created_topics = []\n    \n    for topic in topics:\n        # Create topic ID (global across podcasts)\n        topic_id = f\"topic_{hashlib.sha256(topic['name'].encode()).hexdigest()[:16]}\"\n        \n        # Create or update topic node\n        session.run(\"\"\"\n        MERGE (t:Topic {id: $id})\n        ON CREATE SET \n            t.name = $name,\n            t.created_at = datetime(),\n            t.episode_count = 0,\n            t.total_score = 0\n        SET t.episode_count = t.episode_count + 1,\n            t.total_score = t.total_score + $score,\n            t.avg_score = t.total_score / t.episode_count,\n            t.last_seen = datetime()\n        \"\"\", {\n            \"id\": topic_id,\n            \"name\": topic['name'],\n            \"score\": topic.get('score', 0.5)\n        })\n        \n        # Create relationship to episode\n        session.run(\"\"\"\n        MATCH (e:Episode {id: $episode_id})\n        MATCH (t:Topic {id: $topic_id})\n        MERGE (e)-[r:HAS_TOPIC]->(t)\n        SET r.score = $score,\n            r.evidence = $evidence\n        \"\"\", {\n            \"episode_id\": episode_id,\n            \"topic_id\": topic_id,\n            \"score\": topic.get('score', 0.5),\n            \"evidence\": topic.get('evidence', '')[:500]\n        })\n        \n        # Create relationship to podcast\n        session.run(\"\"\"\n        MATCH (p:Podcast {id: $podcast_id})\n        MATCH (t:Topic {id: $topic_id})\n        MERGE (p)-[r:COVERS_TOPIC]->(t)\n        ON CREATE SET r.episode_count = 1, r.total_score = $score\n        ON MATCH SET \n            r.episode_count = r.episode_count + 1,\n            r.total_score = r.total_score + $score,\n            r.avg_score = r.total_score / r.episode_count\n        \"\"\", {\n            \"podcast_id\": podcast_id,\n            \"topic_id\": topic_id,\n            \"score\": topic.get('score', 0.5)\n        })\n        \n        created_topics.append(topic['name'])\n    \n    return created_topics\n\ndef update_episode_with_topics(session, episode_id, topics):\n    \"\"\"\n    Update episode node with aggregated topic information.\n    \n    Args:\n        session: Neo4j session\n        episode_id: Episode identifier\n        topics: List of topic dictionaries\n    \"\"\"\n    # Extract primary topics (score > 0.5)\n    primary_topics = [t['name'] for t in topics if t.get('score', 0) > 0.5]\n    all_topic_names = [t['name'] for t in topics]\n    \n    # Update episode with topic metadata\n    session.run(\"\"\"\n    MATCH (e:Episode {id: $episode_id})\n    SET e.primary_topics = $primary_topics,\n        e.all_topics = $all_topics,\n        e.topic_count = size($all_topics),\n        e.topic_diversity = CASE \n            WHEN size($all_topics) > 0 \n            THEN size($primary_topics) * 1.0 / size($all_topics)\n            ELSE 0\n        END\n    \"\"\", {\n        \"episode_id\": episode_id,\n        \"primary_topics\": primary_topics,\n        \"all_topics\": all_topic_names\n    })\n\ndef create_quote_nodes(session, quotes, segment_id, episode_id, embedding_client):\n    \"\"\"\n    Create Quote nodes in Neo4j.\n    \n    Args:\n        session: Neo4j session\n        quotes: List of quote dictionaries\n        segment_id: Parent segment ID\n        episode_id: Parent episode ID\n        embedding_client: Client for generating embeddings\n    \"\"\"\n    for quote in quotes:\n        # Generate quote ID\n        quote_hash = hashlib.sha256(f\"{quote['text']}_{episode_id}\".encode()).hexdigest()[:16]\n        quote_id = f\"quote_{quote_hash}\"\n        \n        # Generate embedding for the quote\n        embedding = None\n        if embedding_client:\n            try:\n                quote_context = f\"{quote.get('quote_type', 'general')} quote: {quote['text']}\"\n                embedding = generate_embeddings(quote_context, embedding_client)\n            except:\n                pass\n        \n        # Create quote node\n        session.run(\"\"\"\n        MERGE (q:Quote {id: $id})\n        SET q.text = $text,\n            q.speaker = $speaker,\n            q.impact_score = $impact_score,\n            q.quote_type = $quote_type,\n            q.estimated_timestamp = $timestamp,\n            q.word_count = $word_count,\n            q.embedding = $embedding,\n            q.episode_id = $episode_id\n        WITH q\n        MATCH (s:Segment {id: $segment_id})\n        MERGE (q)-[:EXTRACTED_FROM]->(s)\n        WITH q, s\n        MATCH (e:Episode {id: $episode_id})\n        MERGE (q)-[:QUOTED_IN]->(e)\n        \"\"\", {\n            \"id\": quote_id,\n            \"text\": quote[\"text\"][:1000],\n            \"speaker\": quote.get(\"speaker\", \"Unknown\"),\n            \"impact_score\": quote.get(\"impact_score\", 0.5),\n            \"quote_type\": quote.get(\"quote_type\", \"general\"),\n            \"timestamp\": quote.get(\"estimated_timestamp\", \"00:00\"),\n            \"word_count\": len(quote[\"text\"].split()),\n            \"embedding\": embedding,\n            \"segment_id\": segment_id,\n            \"episode_id\": episode_id\n        })",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 7.1: Core Graph Creation Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_podcast_nodes(session, podcast_info):\n    \"\"\"Create or update podcast nodes in Neo4j.\"\"\"\n    try:\n        session.run(\"\"\"\n        MERGE (p:Podcast {id: $id})\n        ON CREATE SET \n            p.name = $name,\n            p.description = $description,\n            p.rss_url = $rss_url,\n            p.created_timestamp = datetime()\n        ON MATCH SET \n            p.name = $name,\n            p.description = $description,\n            p.rss_url = $rss_url,\n            p.updated_timestamp = datetime()\n        \"\"\", {\n            \"id\": podcast_info[\"id\"],\n            \"name\": podcast_info[\"title\"],\n            \"description\": podcast_info[\"description\"],\n            \"rss_url\": podcast_info[\"rss_url\"]\n        })\n        print(f\"Created/updated podcast node: {podcast_info['title']}\")\n    except Exception as e:\n        raise DatabaseConnectionError(f\"Failed to create podcast node: {e}\")\n\ndef create_episode_nodes(session, episode, podcast_info, episode_complexity=None, episode_metrics=None):\n    \"\"\"Create or update episode nodes in Neo4j with optional complexity and information density metrics.\"\"\"\n    try:\n        query_params = {\n            \"id\": episode[\"id\"],\n            \"title\": episode[\"title\"],\n            \"description\": episode[\"description\"],\n            \"published_date\": episode[\"published_date\"],\n            \"podcast_id\": podcast_info[\"id\"]\n        }\n        \n        # Add complexity and density metrics if available\n        metrics_set_clause = \"\"\n        if episode_complexity:\n            query_params.update({\n                \"avg_complexity\": episode_complexity.get('average_complexity', 0),\n                \"dominant_level\": episode_complexity.get('dominant_level', 'unknown'),\n                \"technical_density\": episode_complexity.get('technical_density', 0),\n                \"complexity_variance\": episode_complexity.get('complexity_variance', 0),\n                \"is_mixed_complexity\": episode_complexity.get('is_mixed_complexity', False),\n                \"is_technical\": episode_complexity.get('is_technical', False),\n                \"layperson_pct\": episode_complexity.get('complexity_distribution', {}).get('layperson', 0),\n                \"intermediate_pct\": episode_complexity.get('complexity_distribution', {}).get('intermediate', 0),\n                \"expert_pct\": episode_complexity.get('complexity_distribution', {}).get('expert', 0)\n            })\n            metrics_set_clause = \"\"\",\n            e.avg_complexity = $avg_complexity,\n            e.dominant_complexity_level = $dominant_level,\n            e.technical_density = $technical_density,\n            e.complexity_variance = $complexity_variance,\n            e.is_mixed_complexity = $is_mixed_complexity,\n            e.is_technical = $is_technical,\n            e.layperson_percentage = $layperson_pct,\n            e.intermediate_percentage = $intermediate_pct,\n            e.expert_percentage = $expert_pct\"\"\"\n        \n        if episode_metrics:\n            query_params.update({\n                \"avg_information_score\": episode_metrics.get('avg_information_score', 0),\n                \"total_insights\": episode_metrics.get('total_insights', 0),\n                \"total_entities\": episode_metrics.get('total_entities', 0),\n                \"avg_accessibility\": episode_metrics.get('avg_accessibility', 0),\n                \"information_variance\": episode_metrics.get('information_variance', 0),\n                \"has_consistent_density\": episode_metrics.get('has_consistent_density', False)\n            })\n            metrics_set_clause += \"\"\",\n            e.avg_information_score = $avg_information_score,\n            e.total_insights = $total_insights,\n            e.total_entities = $total_entities,\n            e.avg_accessibility = $avg_accessibility,\n            e.information_variance = $information_variance,\n            e.has_consistent_density = $has_consistent_density\"\"\"\n        \n        session.run(f\"\"\"\n        MERGE (e:Episode {{id: $id}})\n        ON CREATE SET \n            e.title = $title,\n            e.description = $description,\n            e.published_date = $published_date,\n            e.podcast_id = $podcast_id,\n            e.created_timestamp = datetime(){metrics_set_clause}\n        ON MATCH SET \n            e.title = $title,\n            e.description = $description,\n            e.published_date = $published_date,\n            e.podcast_id = $podcast_id,\n            e.updated_timestamp = datetime(){metrics_set_clause}\n        WITH e\n        MATCH (p:Podcast {{id: $podcast_id}})\n        MERGE (p)-[:HAS_EPISODE]->(e)\n        \"\"\", query_params)\n        \n        print(f\"Created/updated episode node: {episode['title']}\")\n        if episode_complexity:\n            print(f\"  - Complexity: {episode_complexity['dominant_level']} (avg score: {episode_complexity['average_complexity']:.2f})\")\n    except Exception as e:\n        raise DatabaseConnectionError(f\"Failed to create episode node: {e}\")\n\ndef create_insight_nodes(session, insights, podcast_info, episode, embedding_client, use_large_context=True):\n    \"\"\"Create insight nodes in Neo4j with enhanced properties.\"\"\"\n    try:\n        print(f\"Creating {len(insights)} insight nodes...\")\n        \n        for insight in tqdm(insights, desc=\"Creating insights\"):\n            # Generate insight ID\n            insight_text_for_hash = f\"{podcast_info['id']}_{episode['id']}_{insight.get('insight_type', 'conceptual')}_{insight['title']}\"\n            insight_id = f\"insight_{hashlib.sha256(insight_text_for_hash.encode()).hexdigest()[:28]}\"\n            \n            # Generate embedding\n            embedding = None\n            if embedding_client:\n                try:\n                    # Context-aware embedding: include insight type\n                    insight_type = insight.get('insight_type', 'conceptual')\n                    insight_text = f\"[{insight_type}] {insight['title']}: {insight['description']}\"\n                    embedding = generate_embeddings(insight_text, embedding_client)\n                except Exception as e:\n                    print(f\"Warning: Failed to generate embedding for insight: {e}\")\n            \n            # Prepare properties\n            properties = {\n                \"id\": insight_id,\n                \"title\": insight[\"title\"],\n                \"description\": insight[\"description\"],\n                \"insight_type\": insight.get(\"insight_type\", \"conceptual\"),\n                \"podcast_id\": podcast_info[\"id\"],\n                \"episode_id\": episode[\"id\"],\n                \"embedding\": embedding\n            }\n            \n            # Add large context properties\n            if use_large_context:\n                if \"confidence\" in insight:\n                    properties[\"confidence\"] = insight[\"confidence\"]\n                if \"references\" in insight:\n                    properties[\"references\"] = json.dumps(insight[\"references\"])\n            \n            # Build dynamic query\n            query = _build_insight_query(use_large_context, insight)\n            \n            # Create insight node\n            session.run(query, properties)\n            \n            # Create relationship with episode\n            session.run(\"\"\"\n            MATCH (insight:Insight {id: $insight_id})\n            MATCH (episode:Episode {id: $episode_id})\n            MERGE (insight)-[:EXTRACTED_FROM]->(episode)\n            \"\"\", {\n                \"insight_id\": insight_id,\n                \"episode_id\": episode[\"id\"]\n            })\n            \n        print(f\"Successfully created {len(insights)} insight nodes\")\n    except Exception as e:\n        raise DatabaseConnectionError(f\"Failed to create insight nodes: {e}\")\n\ndef _build_insight_query(use_large_context, insight):\n    \"\"\"Helper function to build dynamic insight query based on context mode.\"\"\"\n    base_fields = \"\"\"\n        i.title = $title,\n        i.description = $description,\n        i.insight_type = $insight_type,\n        i.podcast_id = $podcast_id,\n        i.episode_id = $episode_id,\n        i.embedding = $embedding,\n    \"\"\"\n    \n    extra_fields = \"\"\n    if use_large_context and \"confidence\" in insight:\n        extra_fields += \"i.confidence = $confidence,\\n\"\n    if use_large_context and \"references\" in insight:\n        extra_fields += \"i.references = $references,\\n\"\n    \n    return f\"\"\"\n    MERGE (i:Insight {{id: $id}})\n    ON CREATE SET \n        {base_fields}\n        {extra_fields}\n        i.created_timestamp = datetime()\n    ON MATCH SET \n        {base_fields}\n        {extra_fields}\n        i.updated_timestamp = datetime()\n    \"\"\"\n\ndef create_entity_nodes(session, entities, podcast_info, episode, embedding_client, use_large_context=True):\n    \"\"\"Create entity nodes in Neo4j with enhanced properties and entity resolution.\"\"\"\n    try:\n        print(f\"Processing {len(entities)} entities with deduplication...\")\n        \n        entities_created = 0\n        entities_merged = 0\n        \n        for entity in tqdm(entities, desc=\"Creating entities\"):\n            # Normalize entity name\n            normalized_name = normalize_entity_name(entity['name'])\n            \n            # Check for existing similar entities\n            existing_matches = find_existing_entity(session, entity['name'], entity['type'])\n            \n            if existing_matches and existing_matches[0]['similarity'] >= 0.85:\n                # Use existing entity\n                best_match = existing_matches[0]\n                entity_id = best_match['id']\n                entities_merged += 1\n                \n                # Update the existing entity with any new information\n                # Add the current name as an alias if it's different\n                if best_match['similarity'] < 1.0:\n                    session.run(\"\"\"\n                    MATCH (e:Entity {id: $entity_id})\n                    SET e.aliases = CASE\n                        WHEN $new_name IN COALESCE(e.aliases, [])\n                        THEN e.aliases\n                        ELSE COALESCE(e.aliases, []) + $new_name\n                    END\n                    \"\"\", {\"entity_id\": entity_id, \"new_name\": entity['name']})\n                \n                print(f\"Merged '{entity['name']}' with existing entity '{best_match['name']}' (similarity: {best_match['similarity']:.2f})\")\n            else:\n                # Create new entity with enhanced properties\n                # Generate global ID (for future federation)\n                entity_type = entity['type']\n                global_entity_id = f\"global_entity_{hashlib.sha256(f'{normalized_name}_{entity_type}'.encode()).hexdigest()[:28]}\"\n                # Generate local ID (podcast-specific)\n                podcast_id = podcast_info[\"id\"]\n                entity_name = entity[\"name\"]\n                entity_id = f\"entity_{hashlib.sha256(f'{podcast_id}_{entity_name}_{entity_type}'.encode()).hexdigest()[:28]}\"\n                entities_created += 1\n            \n            # Extract aliases from description\n            aliases = extract_entity_aliases(entity['name'], entity.get('description', ''))\n            \n            # Generate embedding with context-aware text\n            embedding = None\n            if embedding_client:\n                try:\n                    # Context-aware embedding: include type and description\n                    entity_text = f\"{entity['type']}: {entity['name']}\"\n                    if entity.get('description'):\n                        entity_text += f\", {entity['description']}\"\n                    embedding = generate_embeddings(entity_text, embedding_client)\n                except Exception as e:\n                    print(f\"Warning: Failed to generate embedding for entity {entity['name']}: {e}\")\n            \n            # Prepare properties with new fields\n            properties = {\n                \"id\": entity_id,\n                \"global_id\": global_entity_id if 'global_entity_id' in locals() else None,\n                \"name\": entity[\"name\"],\n                \"normalized_name\": normalized_name,\n                \"aliases\": aliases,\n                \"type\": entity[\"type\"],\n                \"podcast_id\": podcast_info[\"id\"],\n                \"episode_id\": episode[\"id\"],\n                \"source_podcasts\": [podcast_info[\"id\"]],  # Track which podcasts mention this entity\n                \"embedding\": embedding,\n                \"confidence\": entity.get(\"confidence\", 0.8)  # Default confidence if not provided\n            }\n            \n            # Add optional properties\n            if entity.get(\"description\"):\n                properties[\"description\"] = entity[\"description\"]\n            if use_large_context and \"frequency\" in entity:\n                properties[\"frequency\"] = entity[\"frequency\"]\n            if use_large_context and \"importance\" in entity:\n                properties[\"importance\"] = entity[\"importance\"]\n            \n            # Build dynamic query\n            query = _build_entity_query(use_large_context, entity)\n            \n            # Create entity node\n            session.run(query, properties)\n            \n            # Create relationship with episode\n            session.run(\"\"\"\n            MATCH (entity:Entity {id: $entity_id})\n            MATCH (episode:Episode {id: $episode_id})\n            MERGE (entity)-[:MENTIONED_IN]->(episode)\n            \"\"\", {\n                \"entity_id\": entity_id,\n                \"episode_id\": episode[\"id\"]\n            })\n            \n        print(f\"Entity deduplication complete: {entities_created} created, {entities_merged} merged with existing\")\n        print(f\"Successfully processed {len(entities)} entities\")\n    except Exception as e:\n        raise DatabaseConnectionError(f\"Failed to create entity nodes: {e}\")\n\ndef _build_entity_query(use_large_context, entity):\n    \"\"\"Helper function to build dynamic entity query with enhanced fields.\"\"\"\n    base_fields = \"\"\"\n        e.name = $name,\n        e.normalized_name = $normalized_name,\n        e.aliases = $aliases,\n        e.type = $type,\n        e.podcast_id = $podcast_id,\n        e.episode_id = $episode_id,\n        e.embedding = $embedding,\n        e.confidence = $confidence,\n    \"\"\"\n    \n    # Handle global_id only if creating new entity\n    global_id_fields = \"\"\"\n        e.global_id = COALESCE(e.global_id, $global_id),\n    \"\"\"\n    \n    # Handle source_podcasts array - append if already exists\n    source_podcast_fields = \"\"\"\n        e.source_podcasts = CASE\n            WHEN $podcast_id IN COALESCE(e.source_podcasts, [])\n            THEN e.source_podcasts\n            ELSE COALESCE(e.source_podcasts, []) + $podcast_id\n        END,\n    \"\"\"\n    \n    optional_fields = \"\"\n    if entity.get(\"description\"):\n        optional_fields += \"e.description = $description,\\n\"\n    \n    extra_fields = \"\"\n    if use_large_context and \"frequency\" in entity:\n        extra_fields += \"\"\"\n        e.frequency = CASE\n            WHEN e.frequency IS NULL OR $frequency > e.frequency THEN $frequency\n            ELSE e.frequency\n        END,\n        \"\"\"\n    if use_large_context and \"importance\" in entity:\n        extra_fields += \"\"\"\n        e.importance = CASE\n            WHEN e.importance IS NULL OR $importance > e.importance THEN $importance\n            ELSE e.importance\n        END,\n        \"\"\"\n    \n    return f\"\"\"\n    MERGE (e:Entity {{id: $id}})\n    ON CREATE SET \n        {base_fields}\n        {global_id_fields}\n        {source_podcast_fields}\n        {optional_fields}\n        {extra_fields}\n        e.created_timestamp = datetime()\n    ON MATCH SET \n        {base_fields}\n        {source_podcast_fields}\n        {optional_fields}\n        {extra_fields}\n        e.updated_timestamp = datetime()\n    \"\"\"",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 7.2: Segment & Relationship Creation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_segment_nodes(session, transcript_segments, episode, embedding_client, \n                        segments_complexity=None, segments_info_density=None, \n                        segments_accessibility=None, segments_quotability=None,\n                        segments_best_of=None):\n    \"\"\"Create segment nodes in Neo4j with progress tracking and optional analysis metrics.\"\"\"\n    try:\n        print(f\"Creating {len(transcript_segments)} segment nodes...\")\n        \n        for i, segment in enumerate(tqdm(transcript_segments, desc=\"Creating segments\")):\n            # Generate stable segment ID\n            segment_id = generate_stable_segment_id(\n                episode['id'], \n                segment['text'], \n                segment['start'],\n                segment.get('speaker')\n            )\n            \n            # Check if segment is an advertisement\n            is_ad = _detect_advertisement_in_segment(segment[\"text\"])\n            \n            # Generate embedding\n            embedding = None\n            if embedding_client:\n                try:\n                    # Clean segment text before embedding\n                    cleaned_text = clean_segment_text_for_embedding(segment[\"text\"])\n                    embedding = generate_embeddings(cleaned_text, embedding_client)\n                except Exception as e:\n                    print(f\"Warning: Failed to generate embedding for segment {i}: {e}\")\n            \n            # Build query parameters\n            query_params = {\n                \"id\": segment_id,\n                \"text\": segment[\"text\"],\n                \"start_time\": segment[\"start\"],\n                \"end_time\": segment[\"end\"],\n                \"speaker\": segment.get(\"speaker\", \"Unknown\"),\n                \"is_ad\": is_ad,\n                \"episode_id\": episode[\"id\"],\n                \"index\": i,\n                \"embedding\": embedding,\n                # Additional metadata for stable IDs\n                \"content_hash\": segment_id.split('_')[-1],\n                \"word_count\": len(segment[\"text\"].split()),\n                \"duration_seconds\": segment[\"end\"] - segment[\"start\"]\n            }\n            \n            # Add all metrics if available\n            metrics_set_clause = \"\"\n            if segments_complexity and i < len(segments_complexity):\n                segment_complexity = segments_complexity[i]\n                query_params.update({\n                    \"complexity_level\": segment_complexity.get('classification', 'unknown'),\n                    \"complexity_score\": segment_complexity.get('complexity_score', 0),\n                    \"technical_density\": segment_complexity.get('technical_density', 0),\n                    \"technical_entity_count\": segment_complexity.get('technical_entity_count', 0)\n                })\n                metrics_set_clause = \"\"\",\n                s.complexity_level = $complexity_level,\n                s.complexity_score = $complexity_score,\n                s.technical_density = $technical_density,\n                s.technical_entity_count = $technical_entity_count\"\"\"\n            \n            if segments_info_density and i < len(segments_info_density):\n                segment_density = segments_info_density[i]\n                query_params.update({\n                    \"information_score\": segment_density.get('information_score', 0),\n                    \"insight_density\": segment_density.get('insight_density', 0),\n                    \"entity_density\": segment_density.get('entity_density', 0),\n                    \"fact_density\": segment_density.get('fact_density', 0)\n                })\n                metrics_set_clause += \"\"\",\n                s.information_score = $information_score,\n                s.insight_density = $insight_density,\n                s.entity_density = $entity_density,\n                s.fact_density = $fact_density\"\"\"\n            \n            if segments_accessibility and i < len(segments_accessibility):\n                segment_accessibility = segments_accessibility[i]\n                query_params.update({\n                    \"accessibility_score\": segment_accessibility.get('accessibility_score', 0),\n                    \"avg_sentence_length\": segment_accessibility.get('avg_sentence_length', 0),\n                    \"jargon_percentage\": segment_accessibility.get('jargon_percentage', 0),\n                    \"explanation_quality\": segment_accessibility.get('explanation_quality', 0),\n                    \"has_analogies\": segment_accessibility.get('has_analogies', False),\n                    \"has_examples\": segment_accessibility.get('has_examples', False)\n                })\n                metrics_set_clause += \"\"\",\n                s.accessibility_score = $accessibility_score,\n                s.avg_sentence_length = $avg_sentence_length,\n                s.jargon_percentage = $jargon_percentage,\n                s.explanation_quality = $explanation_quality,\n                s.has_analogies = $has_analogies,\n                s.has_examples = $has_examples\"\"\"\n            \n            if segments_quotability and i < len(segments_quotability):\n                segment_quotability = segments_quotability[i]\n                query_params.update({\n                    \"quotability_score\": segment_quotability.get('quotability_score', 0),\n                    \"is_quotable\": segment_quotability.get('is_highly_quotable', False)\n                })\n                metrics_set_clause += \"\"\",\n                s.quotability_score = $quotability_score,\n                s.is_quotable = $is_quotable\"\"\"\n            \n            if segments_best_of and i < len(segments_best_of):\n                segment_best_of = segments_best_of[i]\n                query_params[\"best_of_category\"] = segment_best_of.get('category', 'regular')\n                metrics_set_clause += \"\"\",\n                s.best_of_category = $best_of_category\"\"\"\n            \n            # Create segment node\n            session.run(f\"\"\"\n            MERGE (s:Segment {{id: $id}})\n            ON CREATE SET \n                s.text = $text,\n                s.start_time = $start_time,\n                s.end_time = $end_time,\n                s.speaker = $speaker,\n                s.is_advertisement = $is_ad,\n                s.episode_id = $episode_id,\n                s.segment_index = $index,\n                s.embedding = $embedding,\n                s.content_hash = $content_hash,\n                s.word_count = $word_count,\n                s.duration_seconds = $duration_seconds,\n                s.created_timestamp = datetime(){metrics_set_clause}\n            ON MATCH SET \n                s.text = $text,\n                s.start_time = $start_time,\n                s.end_time = $end_time,\n                s.speaker = $speaker,\n                s.is_advertisement = $is_ad,\n                s.episode_id = $episode_id,\n                s.segment_index = $index,\n                s.embedding = $embedding,\n                s.content_hash = $content_hash,\n                s.word_count = $word_count,\n                s.duration_seconds = $duration_seconds,\n                s.updated_timestamp = datetime(){metrics_set_clause}\n            WITH s\n            MATCH (e:Episode {{id: $episode_id}})\n            MERGE (e)-[:HAS_SEGMENT]->(s)\n            \"\"\", query_params)\n            \n        print(f\"Successfully created {len(transcript_segments)} segment nodes\")\n    except Exception as e:\n        raise DatabaseConnectionError(f\"Failed to create segment nodes: {e}\")\n\ndef _detect_advertisement_in_segment(text):\n    \"\"\"Helper function to detect if a segment contains advertisement content.\"\"\"\n    segment_text = text.lower()\n    ad_markers = [\n        \"sponsor\", \"sponsored by\", \"brought to you by\", \"discount code\",\n        \"promo code\", \"offer code\", \"special offer\", \"limited time offer\"\n    ]\n    return any(marker in segment_text for marker in ad_markers)\n\ndef create_cross_references(session, entities, insights, podcast_info, episode, use_large_context=True):\n    \"\"\"Create cross-references between entities and insights.\"\"\"\n    if not use_large_context:\n        return\n        \n    try:\n        print(\"Creating cross-references between entities and insights...\")\n        \n        for entity in tqdm(entities, desc=\"Cross-referencing\"):\n            for insight in insights:\n                # Check if entity is mentioned in insight\n                insight_text = f\"{insight['title']} {insight['description']}\".lower()\n                if entity['name'].lower() in insight_text:\n                    # Use the same entity ID generation logic as in create_entity_nodes\n                    normalized_name = normalize_entity_name(entity['name'])\n                    podcast_id = podcast_info['id']\n                    entity_name = entity['name']\n                    entity_type = entity['type']\n                    entity_id = f\"entity_{hashlib.sha256(f'{podcast_id}_{entity_name}_{entity_type}'.encode()).hexdigest()[:28]}\"\n                    insight_text_for_hash = f\"{podcast_info['id']}_{episode['id']}_{insight.get('insight_type', 'conceptual')}_{insight['title']}\"\n                    insight_id = f\"insight_{hashlib.sha256(insight_text_for_hash.encode()).hexdigest()[:28]}\"\n                    \n                    # Create relationship with relevance score\n                    relevance = min(1.0, len(entity['name']) / len(insight_text) * 10)\n                    \n                    session.run(\"\"\"\n                    MATCH (entity:Entity {id: $entity_id})\n                    MATCH (insight:Insight {id: $insight_id})\n                    MERGE (entity)-[r:RELATED_TO]->(insight)\n                    ON CREATE SET r.relevance = $relevance\n                    ON MATCH SET r.relevance = CASE\n                        WHEN $relevance > r.relevance THEN $relevance\n                        ELSE r.relevance\n                    END\n                    \"\"\", {\n                        \"entity_id\": entity_id,\n                        \"insight_id\": insight_id,\n                        \"relevance\": relevance\n                    })\n                    \n        print(\"Successfully created cross-references\")\n    except Exception as e:\n        raise DatabaseConnectionError(f\"Failed to create cross-references: {e}\")\n\ndef compute_similarity_relationships(session, node_type='Insight', similarity_threshold=0.7, top_n=5):\n    \"\"\"\n    Pre-compute similarity relationships between nodes with embeddings.\n    \n    Args:\n        session: Neo4j session\n        node_type: Type of node to compute similarities for ('Insight', 'Entity', 'Segment')\n        similarity_threshold: Minimum similarity score to create relationship\n        top_n: Maximum number of similar nodes to connect\n    \"\"\"\n    print(f\"Computing similarity relationships for {node_type} nodes...\")\n    \n    # Use built-in gds.similarity.cosine if available, otherwise use custom calculation\n    try:\n        # Try using GDS library\n        result = session.run(f\"\"\"\n        MATCH (n:{node_type})\n        WHERE n.embedding IS NOT NULL\n        WITH n, count(*) as total\n        MATCH (other:{node_type})\n        WHERE other.embedding IS NOT NULL \n          AND n.id < other.id\n          AND n.episode_id = other.episode_id\n        WITH n, other, \n             gds.similarity.cosine(n.embedding, other.embedding) AS similarity\n        WHERE similarity >= $threshold\n        WITH n, other, similarity\n        ORDER BY n.id, similarity DESC\n        WITH n, collect({{id: other.id, similarity: similarity}})[..$top_n] as top_similar\n        UNWIND top_similar as sim_node\n        MATCH (target:{node_type} {{id: sim_node.id}})\n        MERGE (n)-[r:SIMILAR_TO]->(target)\n        SET r.score = sim_node.similarity,\n            r.computed_at = datetime()\n        RETURN count(r) as relationships_created\n        \"\"\", {\"threshold\": similarity_threshold, \"top_n\": top_n})\n        \n    except:\n        # Fallback to manual calculation\n        print(f\"GDS not available, using manual similarity calculation...\")\n        \n        # For each node, find similar nodes\n        nodes_result = session.run(f\"\"\"\n        MATCH (n:{node_type})\n        WHERE n.embedding IS NOT NULL\n        RETURN n.id as id, n.embedding as embedding, n.episode_id as episode_id\n        \"\"\")\n        \n        nodes = list(nodes_result)\n        relationships_created = 0\n        \n        for i, node1 in enumerate(nodes):\n            similar_nodes = []\n            \n            for j, node2 in enumerate(nodes):\n                if i >= j or node1['episode_id'] != node2['episode_id']:\n                    continue\n                    \n                # Calculate cosine similarity\n                embedding1 = node1['embedding']\n                embedding2 = node2['embedding']\n                \n                # Simple cosine similarity calculation\n                dot_product = sum(a * b for a, b in zip(embedding1, embedding2))\n                magnitude1 = sum(a * a for a in embedding1) ** 0.5\n                magnitude2 = sum(a * a for a in embedding2) ** 0.5\n                \n                if magnitude1 * magnitude2 > 0:\n                    similarity = dot_product / (magnitude1 * magnitude2)\n                    \n                    if similarity >= similarity_threshold:\n                        similar_nodes.append({\n                            'id': node2['id'],\n                            'similarity': similarity\n                        })\n            \n            # Sort by similarity and take top N\n            similar_nodes.sort(key=lambda x: x['similarity'], reverse=True)\n            \n            for sim_node in similar_nodes[:top_n]:\n                session.run(f\"\"\"\n                MATCH (n:{node_type} {{id: $id1}})\n                MATCH (other:{node_type} {{id: $id2}})\n                MERGE (n)-[r:SIMILAR_TO]->(other)\n                SET r.score = $similarity,\n                    r.computed_at = datetime()\n                \"\"\", {\n                    \"id1\": node1['id'],\n                    \"id2\": sim_node['id'],\n                    \"similarity\": sim_node['similarity']\n                })\n                relationships_created += 1\n        \n        result = [{'relationships_created': relationships_created}]\n    \n    rel_count = result[0][\"relationships_created\"] if result else 0\n    print(f\"Created {rel_count} similarity relationships for {node_type} nodes\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 8Ô∏è‚É£ Advanced Analytics [ADVANCED FEATURES]\n\n## Comprehensive Content Analysis\n\nThis section provides **advanced analytics capabilities** including:\n\n- **Complexity Analysis**: Determine if content is for laypeople, intermediate, or experts\n- **Information Density**: Measure insights per minute, fact density\n- **Accessibility Scoring**: How easy is the content to understand\n- **Quotability Detection**: Find the most memorable quotes\n- **Best-Of Detection**: Identify highlight-worthy segments\n- **Community Detection**: Find clusters of related topics\n- **Discourse Analysis**: Understand conversational patterns\n\nThese metrics help you understand not just WHAT was said, but HOW it was communicated!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 8.1: Technical Complexity Scoring\n\n**What this does:**\n- Analyzes vocabulary complexity\n- Detects technical jargon and terminology\n- Classifies content as layperson, intermediate, or expert level\n- Calculates technical density metrics\n\n**Use this to:**\n- Understand your audience level\n- Find episodes suitable for different knowledge levels\n- Identify highly technical content",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def analyze_vocabulary_complexity(text):\n    \"\"\"\n    Analyze vocabulary complexity of text.\n    \n    Returns:\n        Dict with complexity metrics\n    \"\"\"\n    from collections import Counter\n    import re\n    \n    # Basic tokenization\n    words = re.findall(r'\\b[a-z]+\\b', text.lower())\n    if not words:\n        return {\n            'avg_word_length': 0,\n            'unique_ratio': 0,\n            'syllable_complexity': 0,\n            'technical_density': 0\n        }\n    \n    # Calculate basic metrics\n    avg_word_length = sum(len(word) for word in words) / len(words)\n    unique_words = set(words)\n    unique_ratio = len(unique_words) / len(words)\n    \n    # Estimate syllable complexity (simple heuristic)\n    def count_syllables(word):\n        vowels = 'aeiouAEIOU'\n        count = 0\n        previous_was_vowel = False\n        for char in word:\n            is_vowel = char in vowels\n            if is_vowel and not previous_was_vowel:\n                count += 1\n            previous_was_vowel = is_vowel\n        return max(1, count)\n    \n    total_syllables = sum(count_syllables(word) for word in words)\n    avg_syllables = total_syllables / len(words)\n    \n    # Use optimized pattern matcher\n    technical_count = pattern_matcher.count_technical_terms(text) if pattern_matcher else 0\n    technical_density = technical_count / len(words) if words else 0\n    \n    return {\n        'avg_word_length': avg_word_length,\n        'unique_ratio': unique_ratio,\n        'syllable_complexity': avg_syllables,\n        'technical_density': technical_density\n    }\n\ndef classify_segment_complexity(text, entities=None):\n    \"\"\"\n    Classify segment complexity as layperson, intermediate, or expert.\n    \n    Args:\n        text: Segment text\n        entities: Optional list of detected entities\n        \n    Returns:\n        Dict with complexity classification and scores\n    \"\"\"\n    # Get vocabulary metrics\n    vocab_metrics = analyze_vocabulary_complexity(text)\n    \n    # Count technical entities if provided\n    technical_entity_types = {\n        'Study', 'Institution', 'Researcher', 'Journal', 'Theory', 'Research_Method',\n        'Medication', 'Condition', 'Treatment', 'Symptom', 'Biological_Process',\n        'Medical_Device', 'Chemical', 'Scientific_Theory', 'Laboratory', 'Experiment',\n        'Discovery'\n    }\n    \n    technical_entity_count = 0\n    if entities:\n        for entity in entities:\n            if entity.get('type') in technical_entity_types:\n                technical_entity_count += 1\n    \n    # Calculate composite score\n    complexity_score = (\n        vocab_metrics['avg_word_length'] * 0.2 +\n        (1 - vocab_metrics['unique_ratio']) * 0.2 +  # Lower unique ratio = more repetition = easier\n        vocab_metrics['syllable_complexity'] * 0.3 +\n        vocab_metrics['technical_density'] * 100 * 0.3  # Scale technical density\n    )\n    \n    # Add entity contribution\n    if entities and len(entities) > 0:\n        entity_ratio = technical_entity_count / len(entities)\n        complexity_score += entity_ratio * 2\n    \n    # Classify based on score\n    if complexity_score < 3:\n        classification = 'layperson'\n    elif complexity_score < 5:\n        classification = 'intermediate'\n    else:\n        classification = 'expert'\n    \n    # Check for specific markers that might override classification\n    if vocab_metrics['technical_density'] > 0.1:  # >10% technical terms\n        classification = 'expert'\n    elif vocab_metrics['technical_density'] > 0.05 and classification == 'layperson':\n        classification = 'intermediate'\n    \n    return {\n        'classification': classification,\n        'complexity_score': complexity_score,\n        'vocab_metrics': vocab_metrics,\n        'technical_entity_count': technical_entity_count,\n        'technical_density': vocab_metrics['technical_density']\n    }\n\ndef calculate_episode_complexity(segments_complexity):\n    \"\"\"\n    Calculate overall episode complexity from segment complexities.\n    \n    Args:\n        segments_complexity: List of segment complexity dicts\n        \n    Returns:\n        Dict with episode-level complexity metrics\n    \"\"\"\n    if not segments_complexity:\n        return {\n            'average_complexity': 0,\n            'dominant_level': 'unknown',\n            'complexity_distribution': {},\n            'technical_density': 0,\n            'complexity_variance': 0\n        }\n    \n    # Calculate average complexity score\n    scores = [seg['complexity_score'] for seg in segments_complexity]\n    avg_score = sum(scores) / len(scores)\n    \n    # Calculate variance to measure consistency\n    variance = sum((score - avg_score) ** 2 for score in scores) / len(scores)\n    \n    # Count distribution of complexity levels\n    distribution = {'layperson': 0, 'intermediate': 0, 'expert': 0}\n    for seg in segments_complexity:\n        distribution[seg['classification']] += 1\n    \n    # Determine dominant level\n    dominant_level = max(distribution.items(), key=lambda x: x[1])[0]\n    \n    # Calculate average technical density\n    tech_densities = [seg['technical_density'] for seg in segments_complexity]\n    avg_tech_density = sum(tech_densities) / len(tech_densities)\n    \n    # Normalize distribution to percentages\n    total_segments = len(segments_complexity)\n    distribution_pct = {\n        level: (count / total_segments) * 100 \n        for level, count in distribution.items()\n    }\n    \n    return {\n        'average_complexity': avg_score,\n        'dominant_level': dominant_level,\n        'complexity_distribution': distribution_pct,\n        'technical_density': avg_tech_density,\n        'complexity_variance': variance,\n        'is_mixed_complexity': variance > 1.5,  # High variance indicates mixed audience\n        'is_technical': avg_tech_density > 0.05  # >5% technical terms\n    }\n\n# Example usage\ndef analyze_complexity_example():\n    \"\"\"Example of complexity analysis.\"\"\"\n    sample_text = \"\"\"\n    The quantum entanglement phenomenon demonstrates non-locality in quantum mechanics.\n    This means that particles can be connected even when separated by vast distances.\n    When you measure one particle, it instantly affects its entangled partner.\n    \"\"\"\n    \n    complexity = classify_segment_complexity(sample_text)\n    print(\"üìä Complexity Analysis Example:\")\n    print(f\"  ‚Ä¢ Classification: {complexity['classification'].upper()}\")\n    print(f\"  ‚Ä¢ Complexity Score: {complexity['complexity_score']:.2f}\")\n    print(f\"  ‚Ä¢ Technical Density: {complexity['technical_density']:.2%}\")\n    print(f\"  ‚Ä¢ Vocabulary Metrics:\")\n    for metric, value in complexity['vocab_metrics'].items():\n        print(f\"    - {metric}: {value:.2f}\")\n    \n    return complexity\n\n# Run example\nprint(\"‚úÖ Complexity analysis functions loaded\")\nanalyze_complexity_example()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 8.2: Information Density & Accessibility Analysis\n\n**What this does:**\n- Measures how much valuable information is packed into content\n- Calculates insights per minute and fact density\n- Scores how accessible/understandable content is\n- Detects explanations, analogies, and examples\n\n**Use this to:**\n- Find information-rich segments\n- Identify episodes with high educational value\n- Ensure content is accessible to your target audience",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def calculate_information_density(text, insights=None, entities=None):\n    \"\"\"\n    Calculate information density metrics for a text segment.\n    \n    Args:\n        text: Segment text\n        insights: Optional list of insights extracted\n        entities: Optional list of entities detected\n        \n    Returns:\n        Dict with information density metrics\n    \"\"\"\n    import re\n    \n    # Basic text metrics\n    words = text.split()\n    word_count = len(words)\n    char_count = len(text)\n    \n    if word_count == 0:\n        return {\n            'insight_density': 0,\n            'entity_density': 0,\n            'fact_density': 0,\n            'information_score': 0,\n            'words_per_minute': 0\n        }\n    \n    # Calculate densities\n    insight_density = len(insights) / word_count * 100 if insights else 0\n    entity_density = len(entities) / word_count * 100 if entities else 0\n    \n    # Use optimized pattern matcher for fact counting\n    fact_count = pattern_matcher.count_facts(text) if pattern_matcher else 0\n    fact_density = fact_count / word_count * 100\n    \n    # Calculate composite information score\n    information_score = (\n        insight_density * 0.4 +\n        entity_density * 0.3 +\n        fact_density * 0.3\n    )\n    \n    # Estimate words per minute (assuming average speaking rate of 150 wpm)\n    avg_wpm = 150\n    duration_estimate = word_count / avg_wpm\n    \n    return {\n        'insight_density': insight_density,\n        'entity_density': entity_density,\n        'fact_density': fact_density,\n        'information_score': information_score,\n        'word_count': word_count,\n        'duration_minutes': duration_estimate,\n        'insights_per_minute': (len(insights) / duration_estimate) if insights and duration_estimate > 0 else 0,\n        'entities_per_minute': (len(entities) / duration_estimate) if entities and duration_estimate > 0 else 0\n    }\n\ndef calculate_accessibility_score(text, complexity_score):\n    \"\"\"\n    Calculate accessibility score based on various readability metrics.\n    \n    Args:\n        text: Text to analyze\n        complexity_score: Complexity score from classify_segment_complexity\n        \n    Returns:\n        Dict with accessibility metrics\n    \"\"\"\n    import re\n    \n    # Split into sentences and words\n    sentences = re.split(r'[.!?]+', text)\n    sentences = [s.strip() for s in sentences if s.strip()]\n    words = text.split()\n    \n    if not sentences or not words:\n        return {\n            'accessibility_score': 0,\n            'avg_sentence_length': 0,\n            'jargon_percentage': 0,\n            'explanation_quality': 0\n        }\n    \n    # Average sentence length\n    avg_sentence_length = len(words) / len(sentences)\n    \n    # Check for jargon and explanations\n    jargon_patterns = [\n        r'\\b[A-Z]{3,}\\b',  # Acronyms\n        r'\\b\\w+(?:ology|itis|osis|ase|ine)\\b',  # Technical suffixes\n        r'\\b(?:neuro|cardio|hypo|anti|meta|poly)\\w+\\b',  # Technical prefixes\n    ]\n    \n    explanation_patterns = [\n        r'\\b(?:which means|in other words|that is|i\\.e\\.|for example|such as)\\b',\n        r'\\b(?:basically|simply put|essentially|in simple terms)\\b',\n        r'\\b(?:imagine|think of it as|like a?|similar to)\\b',  # Analogies\n        r'\\([^)]+\\)',  # Parenthetical explanations\n    ]\n    \n    jargon_count = 0\n    for pattern in jargon_patterns:\n        jargon_count += len(re.findall(pattern, text, re.IGNORECASE))\n    \n    explanation_count = 0\n    for pattern in explanation_patterns:\n        explanation_count += len(re.findall(pattern, text, re.IGNORECASE))\n    \n    jargon_percentage = (jargon_count / len(words)) * 100 if words else 0\n    explanation_quality = min(100, (explanation_count / max(1, jargon_count)) * 100)\n    \n    # Calculate accessibility score (inverse of complexity with adjustments)\n    accessibility_score = 100 - (complexity_score * 10)  # Base from complexity\n    \n    # Adjust for sentence length (longer sentences = less accessible)\n    if avg_sentence_length > 20:\n        accessibility_score -= (avg_sentence_length - 20) * 2\n    \n    # Adjust for jargon vs explanations\n    accessibility_score -= jargon_percentage * 0.5\n    accessibility_score += explanation_quality * 0.3\n    \n    # Ensure score is between 0 and 100\n    accessibility_score = max(0, min(100, accessibility_score))\n    \n    return {\n        'accessibility_score': accessibility_score,\n        'avg_sentence_length': avg_sentence_length,\n        'jargon_percentage': jargon_percentage,\n        'explanation_quality': explanation_quality,\n        'has_analogies': bool(re.search(r'\\b(?:like a?|similar to|imagine|think of it as)\\b', text, re.IGNORECASE)),\n        'has_examples': bool(re.search(r'\\b(?:for example|such as|instance)\\b', text, re.IGNORECASE))\n    }\n\ndef aggregate_episode_metrics(segments_info_density, segments_accessibility):\n    \"\"\"\n    Aggregate segment-level metrics to episode level.\n    \n    Args:\n        segments_info_density: List of information density dicts\n        segments_accessibility: List of accessibility dicts\n        \n    Returns:\n        Dict with episode-level aggregated metrics\n    \"\"\"\n    if not segments_info_density:\n        return {\n            'avg_information_score': 0,\n            'total_insights': 0,\n            'total_entities': 0,\n            'avg_accessibility': 0,\n            'information_variance': 0\n        }\n    \n    # Information density aggregation\n    info_scores = [seg['information_score'] for seg in segments_info_density]\n    avg_info_score = sum(info_scores) / len(info_scores)\n    \n    # Calculate variance to identify episodes with uneven information distribution\n    info_variance = sum((score - avg_info_score) ** 2 for score in info_scores) / len(info_scores)\n    \n    # Total insights and entities\n    total_insights = sum(seg['insight_density'] * seg['word_count'] / 100 for seg in segments_info_density)\n    total_entities = sum(seg['entity_density'] * seg['word_count'] / 100 for seg in segments_info_density)\n    \n    # Accessibility aggregation\n    accessibility_scores = [seg['accessibility_score'] for seg in segments_accessibility] if segments_accessibility else []\n    avg_accessibility = sum(accessibility_scores) / len(accessibility_scores) if accessibility_scores else 0\n    \n    # Find high-value segments (top 20% by information score)\n    sorted_segments = sorted(enumerate(info_scores), key=lambda x: x[1], reverse=True)\n    top_20_percent = int(len(sorted_segments) * 0.2) or 1\n    high_value_segments = [idx for idx, _ in sorted_segments[:top_20_percent]]\n    \n    return {\n        'avg_information_score': avg_info_score,\n        'total_insights': int(total_insights),\n        'total_entities': int(total_entities),\n        'avg_accessibility': avg_accessibility,\n        'information_variance': info_variance,\n        'has_consistent_density': info_variance < 10,  # Low variance = consistent\n        'high_value_segment_indices': high_value_segments\n    }\n\n# Example usage\ndef analyze_density_example():\n    \"\"\"Example of information density analysis.\"\"\"\n    sample_text = \"\"\"\n    Research shows that companies with diverse teams are 35% more likely to outperform.\n    This is because diversity brings different perspectives and problem-solving approaches.\n    For example, a study by McKinsey found that ethnically diverse companies are 36% more\n    profitable than their less diverse counterparts.\n    \"\"\"\n    \n    # Mock insights and entities for the example\n    mock_insights = [\n        {'title': 'Diversity improves performance'},\n        {'title': 'Different perspectives enhance problem-solving'}\n    ]\n    mock_entities = ['McKinsey', 'diverse teams', 'companies']\n    \n    # Calculate information density\n    density = calculate_information_density(sample_text, mock_insights, mock_entities)\n    \n    # Calculate accessibility (using mock complexity score)\n    accessibility = calculate_accessibility_score(sample_text, 3.5)\n    \n    print(\"üìä Information Density Analysis:\")\n    print(f\"  ‚Ä¢ Information Score: {density['information_score']:.2f}\")\n    print(f\"  ‚Ä¢ Insights per minute: {density['insights_per_minute']:.1f}\")\n    print(f\"  ‚Ä¢ Fact density: {density['fact_density']:.1f}%\")\n    print(f\"  ‚Ä¢ Duration estimate: {density['duration_minutes']:.1f} minutes\")\n    \n    print(\"\\n‚ôø Accessibility Analysis:\")\n    print(f\"  ‚Ä¢ Accessibility Score: {accessibility['accessibility_score']:.1f}/100\")\n    print(f\"  ‚Ä¢ Average sentence length: {accessibility['avg_sentence_length']:.1f} words\")\n    print(f\"  ‚Ä¢ Has analogies: {accessibility['has_analogies']}\")\n    print(f\"  ‚Ä¢ Has examples: {accessibility['has_examples']}\")\n    \n    return density, accessibility\n\n# Run example\nprint(\"‚úÖ Information density and accessibility functions loaded\")\nanalyze_density_example()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 8.3: Quotability & Best-Of Detection\n\n**What this does:**\n- Identifies highly quotable segments\n- Detects \"best of\" worthy content for highlight reels\n- Scores memorable phrases and insights\n- Finds key moments and breakthroughs\n\n**Use this to:**\n- Create quote collections\n- Generate podcast highlights\n- Find shareable content\n- Identify the most impactful moments",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def calculate_quotability_score(text, speaker=None):\n    \"\"\"\n    Calculate how quotable a text segment is.\n    \n    Args:\n        text: Text to analyze\n        speaker: Optional speaker name\n        \n    Returns:\n        Dict with quotability metrics\n    \"\"\"\n    import re\n    \n    # Check text length (ideal quotes are 10-30 words)\n    words = text.split()\n    word_count = len(words)\n    \n    if word_count < 5 or word_count > 100:\n        length_score = 0\n    elif 10 <= word_count <= 30:\n        length_score = 100\n    else:\n        # Gradual decrease for longer quotes\n        length_score = max(0, 100 - (word_count - 30) * 2)\n    \n    # Use optimized pattern matcher for quotability\n    pattern_matches = pattern_matcher.get_quotability_matches(text) if pattern_matcher else 0\n    pattern_score = min(100, pattern_matches * 15)\n    \n    # Check for memorable phrasing\n    memorable_indicators = [\n        r'\\b(?:imagine|picture|think about)\\b',  # Vivid imagery\n        r'\\b\\w+\\s+(?:is|are)\\s+like\\b',  # Analogies\n        r'\\b(?:not|n\\'t).*but\\b',  # Contrasts\n        r'[!?]',  # Emotional punctuation\n        r'\\b(?:I|we)\\s+(?:learned|discovered|realized)\\b',  # Personal insights\n    ]\n    \n    memorable_score = 0\n    for indicator in memorable_indicators:\n        if re.search(indicator, text, re.IGNORECASE):\n            memorable_score += 20\n    memorable_score = min(100, memorable_score)\n    \n    # Check for self-contained meaning (doesn't rely on context)\n    context_dependent_words = ['this', 'that', 'these', 'those', 'it', 'they', 'them', 'here', 'there']\n    context_dependency = sum(1 for word in context_dependent_words if word in text.lower().split())\n    self_contained_score = max(0, 100 - (context_dependency * 20))\n    \n    # Calculate composite score\n    quotability_score = (\n        length_score * 0.3 +\n        pattern_score * 0.25 +\n        memorable_score * 0.25 +\n        self_contained_score * 0.2\n    )\n    \n    # Boost score for known speakers (if provided)\n    if speaker and speaker.lower() != 'unknown':\n        quotability_score = min(100, quotability_score * 1.1)\n    \n    return {\n        'quotability_score': quotability_score,\n        'is_highly_quotable': quotability_score >= 70,\n        'length_score': length_score,\n        'pattern_score': pattern_score,\n        'memorable_score': memorable_score,\n        'self_contained_score': self_contained_score,\n        'word_count': word_count\n    }\n\ndef detect_best_of_markers(text, insights=None):\n    \"\"\"\n    Detect if a segment contains \"best of\" worthy content.\n    \n    Args:\n        text: Segment text\n        insights: Optional insights extracted from segment\n        \n    Returns:\n        Dict with best-of detection results\n    \"\"\"\n    import re\n    \n    # Patterns indicating highlight-worthy content\n    highlight_patterns = [\n        # Key moments\n        r'\\b(?:breakthrough|turning point|pivotal|game.?changer)\\b',\n        r'\\b(?:aha|eureka|lightbulb) moment\\b',\n        r'\\b(?:changed everything|life.?changing|transformative)\\b',\n        \n        # Valuable insights\n        r'\\b(?:most important|biggest|key) (?:lesson|insight|takeaway)\\b',\n        r'\\b(?:secret|trick|hack) (?:to|is|for)\\b',\n        r'\\bhere\\'s (?:the|what|how)\\b',\n        \n        # Strong statements\n        r'\\b(?:controversial|unpopular) opinion\\b',\n        r'\\b(?:truth is|fact is|reality is)\\b',\n        r'\\bmyth about\\b',\n        \n        # Expertise markers\n        r'\\b(?:spent|invested) \\d+ (?:years|months|hours)\\b',\n        r'\\b(?:learned|discovered) (?:that|how)\\b',\n        r'\\bafter (?:years|decades) of\\b',\n        \n        # Actionable advice\n        r'\\b(?:step.?by.?step|framework|process|method)\\b',\n        r'\\b\\d+\\s+(?:tips|ways|steps|strategies)\\b',\n        r'\\b(?:how to|guide to|formula for)\\b'\n    ]\n    \n    pattern_matches = 0\n    matched_patterns = []\n    for pattern in highlight_patterns:\n        if re.search(pattern, text, re.IGNORECASE):\n            pattern_matches += 1\n            matched_patterns.append(pattern)\n    \n    # Check insight quality if available\n    high_value_insights = 0\n    if insights:\n        for insight in insights:\n            # Assuming insights have confidence scores\n            if insight.get('confidence', 0) >= 8:\n                high_value_insights += 1\n    \n    # Calculate best-of score\n    pattern_score = min(100, pattern_matches * 25)\n    insight_score = min(100, high_value_insights * 30)\n    \n    best_of_score = (pattern_score * 0.6 + insight_score * 0.4)\n    \n    # Determine category\n    if best_of_score >= 80:\n        category = 'must_include'\n    elif best_of_score >= 60:\n        category = 'highly_recommended'\n    elif best_of_score >= 40:\n        category = 'consider'\n    else:\n        category = 'regular'\n    \n    return {\n        'best_of_score': best_of_score,\n        'category': category,\n        'pattern_matches': pattern_matches,\n        'high_value_insights': high_value_insights,\n        'is_best_of': best_of_score >= 60,\n        'matched_patterns': matched_patterns[:3]  # Top 3 patterns for reference\n    }\n\ndef extract_key_quotes(segments, quotability_scores):\n    \"\"\"\n    Extract the most quotable segments from an episode.\n    \n    Args:\n        segments: List of transcript segments\n        quotability_scores: List of quotability score dicts\n        \n    Returns:\n        List of key quotes with metadata\n    \"\"\"\n    if not segments or not quotability_scores:\n        return []\n    \n    # Combine segments with their scores\n    segment_quotes = []\n    for i, (segment, score) in enumerate(zip(segments, quotability_scores)):\n        if score['is_highly_quotable']:\n            segment_quotes.append({\n                'text': segment['text'],\n                'speaker': segment.get('speaker', 'Unknown'),\n                'start_time': segment['start'],\n                'end_time': segment['end'],\n                'segment_index': i,\n                'quotability_score': score['quotability_score'],\n                'word_count': score['word_count']\n            })\n    \n    # Sort by quotability score\n    segment_quotes.sort(key=lambda x: x['quotability_score'], reverse=True)\n    \n    # Return top quotes (max 10)\n    return segment_quotes[:10]\n\n# Example usage\ndef analyze_quotability_example():\n    \"\"\"Example of quotability analysis.\"\"\"\n    sample_quotes = [\n        {\n            'text': \"The biggest lesson I learned is that success isn't about perfection, it's about persistence.\",\n            'speaker': 'Guest Speaker'\n        },\n        {\n            'text': \"We discovered that by simply changing our morning routine, productivity increased by 40%.\",\n            'speaker': 'Host'\n        },\n        {\n            'text': \"This is really interesting when you think about it in the context of what we discussed earlier.\",\n            'speaker': 'Guest'\n        }\n    ]\n    \n    print(\"üìù Quotability Analysis Examples:\\n\")\n    \n    for i, quote in enumerate(sample_quotes, 1):\n        score = calculate_quotability_score(quote['text'], quote['speaker'])\n        best_of = detect_best_of_markers(quote['text'])\n        \n        print(f\"Quote {i}: \\\"{quote['text'][:60]}...\\\"\")\n        print(f\"  Speaker: {quote['speaker']}\")\n        print(f\"  Quotability Score: {score['quotability_score']:.1f}/100\")\n        print(f\"  Is Highly Quotable: {'YES' if score['is_highly_quotable'] else 'NO'}\")\n        print(f\"  Best-Of Category: {best_of['category'].upper()}\")\n        print(f\"  Word Count: {score['word_count']}\")\n        print()\n    \n    return sample_quotes\n\n# Run example\nprint(\"‚úÖ Quotability and best-of detection functions loaded\")\nanalyze_quotability_example()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def apply_graph_algorithms(neo4j_driver):\n    \"\"\"\n    Apply graph algorithms for pattern discovery using PageRank, community detection, and path analysis.\n    \"\"\"\n    if not neo4j_driver:\n        print(\"Neo4j driver not available. Cannot apply graph algorithms.\")\n        return\n        \n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            print(\"Setting up graph projections...\")\n            \n            # Create graph projection for algorithms\n            try:\n                # First drop existing projection if any\n                session.run(\"CALL gds.graph.drop('complete_knowledge_graph', false)\")\n            except:\n                pass\n                \n            session.run(\"\"\"\n            CALL gds.graph.project(\n              'complete_knowledge_graph',\n              ['Entity', 'Insight', 'Episode', 'Segment'],\n              {\n                MENTIONED_IN: {orientation: 'UNDIRECTED'},\n                MENTIONED_WITH: {orientation: 'UNDIRECTED'},\n                SIMILAR_CONCEPT: {orientation: 'UNDIRECTED'},\n                CONTRADICTS: {orientation: 'UNDIRECTED'},\n                SUPPORTS: {orientation: 'UNDIRECTED'},\n                EXPANDS_ON: {orientation: 'UNDIRECTED'},\n                HAS_INSIGHT: {orientation: 'UNDIRECTED'},\n                HAS_SEGMENT: {orientation: 'UNDIRECTED'},\n                SIMILAR_INSIGHT: {orientation: 'UNDIRECTED'},\n                TOPIC_EVOLUTION: {orientation: 'NATURAL'}\n              }\n            )\n            \"\"\")\n            \n            print(\"Running PageRank algorithm...\")\n            # Run PageRank to find influential nodes\n            session.run(\"\"\"\n            CALL gds.pageRank.write(\n              'complete_knowledge_graph',\n              {\n                writeProperty: 'pagerank',\n                maxIterations: 20,\n                dampingFactor: 0.85\n              }\n            )\n            \"\"\")\n            \n            print(\"Running community detection...\")\n            # Run community detection\n            session.run(\"\"\"\n            CALL gds.louvain.write(\n              'complete_knowledge_graph',\n              {\n                writeProperty: 'community',\n                includeIntermediateCommunities: true\n              }\n            )\n            \"\"\")\n            \n            # Find and connect important entities\n            print(\"Finding key connections between influential entities...\")\n            important_entities = session.run(\"\"\"\n            MATCH (e:Entity)\n            WHERE e.pagerank > 0.01\n            RETURN e.id as id, e.name as name, e.pagerank as score\n            ORDER BY e.pagerank DESC\n            LIMIT 10\n            \"\"\")\n            \n            entities = list(important_entities)\n            \n            # Find paths between top entities\n            connections_created = 0\n            for i in range(len(entities)):\n                for j in range(i+1, len(entities)):\n                    source = entities[i]\n                    target = entities[j]\n                    \n                    paths = session.run(\"\"\"\n                    MATCH path = shortestPath((e1:Entity {id: $source_id})-[*..5]-(e2:Entity {id: $target_id}))\n                    RETURN [node IN nodes(path) | node.name] as node_names,\n                           [rel IN relationships(path) | type(rel)] as rel_types\n                    \"\"\", {\"source_id\": source[\"id\"], \"target_id\": target[\"id\"]})\n                    \n                    if paths.peek():\n                        path_data = paths.single()\n                        # Create KEY_CONNECTION relationships\n                        session.run(\"\"\"\n                        MATCH (e1:Entity {id: $source_id}), (e2:Entity {id: $target_id})\n                        MERGE (e1)-[:KEY_CONNECTION {\n                            path_length: size($path_nodes) - 2,\n                            path_details: $path_nodes,\n                            created_at: datetime()\n                        }]->(e2)\n                        \"\"\", {\n                            \"source_id\": source[\"id\"],\n                            \"target_id\": target[\"id\"],\n                            \"path_nodes\": path_data[\"node_names\"]\n                        })\n                        connections_created += 1\n                        \n            print(f\"Created {connections_created} key connections between influential entities\")\n            \n            # Clean up projection\n            session.run(\"CALL gds.graph.drop('complete_knowledge_graph')\")\n            \n            print(\"‚úì Graph algorithms applied successfully\")\n            \n    except Exception as e:\n        print(f\"Error applying graph algorithms: {e}\")\n        # Continue even if algorithms fail (e.g., GDS not installed)\n\ndef get_influential_entities(neo4j_driver, limit=20):\n    \"\"\"\n    Get the most influential entities based on PageRank scores.\n    \"\"\"\n    if not neo4j_driver:\n        return []\n        \n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            result = session.run(\"\"\"\n            MATCH (e:Entity)\n            WHERE e.pagerank IS NOT NULL\n            RETURN e.name as name, \n                   e.type as type, \n                   e.pagerank as score,\n                   e.description as description,\n                   size((e)-[]-()) as connections\n            ORDER BY e.pagerank DESC\n            LIMIT $limit\n            \"\"\", {\"limit\": limit})\n            \n            entities = []\n            for record in result:\n                entities.append({\n                    \"name\": record[\"name\"],\n                    \"type\": record[\"type\"],\n                    \"pagerank\": record[\"score\"],\n                    \"description\": record[\"description\"],\n                    \"connections\": record[\"connections\"]\n                })\n                \n            return entities\n            \n    except Exception as e:\n        print(f\"Error getting influential entities: {e}\")\n        return []\n\ndef get_community_clusters(neo4j_driver):\n    \"\"\"\n    Get community clusters and their members.\n    \"\"\"\n    if not neo4j_driver:\n        return []\n        \n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            result = session.run(\"\"\"\n            MATCH (n)\n            WHERE n.community IS NOT NULL\n            WITH n.community as community_id,\n                 collect(DISTINCT {\n                     name: n.name,\n                     type: labels(n)[0],\n                     pagerank: n.pagerank\n                 }) as members\n            RETURN community_id, \n                   size(members) as size,\n                   members[..10] as sample_members\n            ORDER BY size DESC\n            LIMIT 20\n            \"\"\")\n            \n            communities = []\n            for record in result:\n                communities.append({\n                    \"id\": record[\"community_id\"],\n                    \"size\": record[\"size\"],\n                    \"sample_members\": record[\"sample_members\"]\n                })\n                \n            return communities\n            \n    except Exception as e:\n        print(f\"Error getting community clusters: {e}\")\n        return []\n\ndef analyze_knowledge_paths(neo4j_driver, start_entity, end_entity=None, max_length=5):\n    \"\"\"\n    Analyze knowledge paths between entities to find connections.\n    \"\"\"\n    if not neo4j_driver:\n        return []\n        \n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            if end_entity:\n                # Find paths between specific entities\n                result = session.run(\"\"\"\n                MATCH (start:Entity {name: $start_name})\n                MATCH (end:Entity {name: $end_name})\n                MATCH path = allShortestPaths((start)-[*..{max_length}]-(end))\n                RETURN [node IN nodes(path) | {\n                    name: node.name,\n                    type: labels(node)[0]\n                }] as path_nodes,\n                [rel IN relationships(path) | type(rel)] as relationships,\n                length(path) as path_length\n                LIMIT 5\n                \"\"\", {\n                    \"start_name\": start_entity,\n                    \"end_name\": end_entity,\n                    \"max_length\": max_length\n                })\n            else:\n                # Find paths from start entity to important entities\n                result = session.run(\"\"\"\n                MATCH (start:Entity {name: $start_name})\n                MATCH (end:Entity)\n                WHERE end.pagerank > 0.01 AND end.name <> start.name\n                MATCH path = shortestPath((start)-[*..{max_length}]-(end))\n                RETURN [node IN nodes(path) | {\n                    name: node.name,\n                    type: labels(node)[0]\n                }] as path_nodes,\n                [rel IN relationships(path) | type(rel)] as relationships,\n                length(path) as path_length,\n                end.name as end_entity,\n                end.pagerank as end_importance\n                ORDER BY end.pagerank DESC\n                LIMIT 10\n                \"\"\", {\n                    \"start_name\": start_entity,\n                    \"max_length\": max_length\n                })\n            \n            paths = []\n            for record in result:\n                paths.append({\n                    \"path\": record[\"path_nodes\"],\n                    \"relationships\": record[\"relationships\"],\n                    \"length\": record[\"path_length\"],\n                    \"end_entity\": record.get(\"end_entity\"),\n                    \"importance\": record.get(\"end_importance\", 0)\n                })\n                \n            return paths\n            \n    except Exception as e:\n        print(f\"Error analyzing knowledge paths: {e}\")\n        return []",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 9.2: Semantic Clustering",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def implement_semantic_clustering(neo4j_driver, llm_client=None):\n    \"\"\"\n    Implement semantic clustering using vector embeddings to group related concepts.\n    \"\"\"\n    if not neo4j_driver:\n        print(\"Neo4j driver not available. Cannot implement semantic clustering.\")\n        return\n        \n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            print(\"Creating vector indexes for semantic search...\")\n            \n            # Create vector indexes\n            try:\n                session.run(\"\"\"\n                CREATE VECTOR INDEX insight_vector IF NOT EXISTS\n                FOR (i:Insight) ON (i.embedding)\n                OPTIONS {\n                  indexConfig: {\n                    `vector.dimensions`: 1536,\n                    `vector.similarity_function`: 'cosine'\n                  }\n                }\n                \"\"\")\n            except:\n                pass  # Index may already exist\n                \n            try:\n                session.run(\"\"\"\n                CREATE VECTOR INDEX entity_vector IF NOT EXISTS\n                FOR (e:Entity) ON (e.embedding)\n                OPTIONS {\n                  indexConfig: {\n                    `vector.dimensions`: 1536,\n                    `vector.similarity_function`: 'cosine'\n                  }\n                }\n                \"\"\")\n            except:\n                pass  # Index may already exist\n            \n            print(\"Creating semantic similarity relationships...\")\n            \n            # Create semantic similarity connections between insights\n            insight_count = session.run(\"\"\"\n            MATCH (i1:Insight)\n            WHERE i1.embedding IS NOT NULL\n            WITH i1, i1.embedding as embedding1\n            MATCH (i2:Insight)\n            WHERE i2.embedding IS NOT NULL \n              AND i1.id < i2.id\n              AND i1.episode_id = i2.episode_id\n            WITH i1, i2, gds.similarity.cosine(i1.embedding, i2.embedding) as similarity\n            WHERE similarity > 0.8\n            MERGE (i1)-[r:SEMANTIC_SIMILARITY]->(i2)\n            SET r.score = similarity\n            RETURN count(r) as relationships_created\n            \"\"\").single()[\"relationships_created\"]\n            \n            print(f\"Created {insight_count} semantic relationships between insights\")\n            \n            # Create semantic similarity connections between entities\n            entity_count = session.run(\"\"\"\n            MATCH (e1:Entity)\n            WHERE e1.embedding IS NOT NULL\n            WITH e1, e1.embedding as embedding1\n            MATCH (e2:Entity)\n            WHERE e2.embedding IS NOT NULL \n              AND e1.id < e2.id\n            WITH e1, e2, gds.similarity.cosine(e1.embedding, e2.embedding) as similarity\n            WHERE similarity > 0.85\n            MERGE (e1)-[r:SEMANTIC_SIMILARITY]->(e2)\n            SET r.score = similarity\n            RETURN count(r) as relationships_created\n            \"\"\").single()[\"relationships_created\"]\n            \n            print(f\"Created {entity_count} semantic relationships between entities\")\n            \n            # Create semantic graph projection\n            print(\"Running semantic clustering algorithm...\")\n            \n            try:\n                # Drop existing projection if any\n                session.run(\"CALL gds.graph.drop('semantic_graph', false)\")\n            except:\n                pass\n                \n            session.run(\"\"\"\n            CALL gds.graph.project(\n              'semantic_graph',\n              ['Insight', 'Entity'],\n              {\n                SEMANTIC_SIMILARITY: {\n                  orientation: 'UNDIRECTED',\n                  properties: ['score']\n                }\n              }\n            )\n            \"\"\")\n            \n            # Run label propagation for clustering\n            session.run(\"\"\"\n            CALL gds.labelPropagation.write(\n              'semantic_graph',\n              {\n                writeProperty: 'semantic_cluster',\n                relationshipWeightProperty: 'score',\n                maxIterations: 20\n              }\n            )\n            \"\"\")\n            \n            # Get cluster statistics\n            cluster_stats = session.run(\"\"\"\n            MATCH (n)\n            WHERE n.semantic_cluster IS NOT NULL\n            RETURN count(DISTINCT n.semantic_cluster) as cluster_count,\n                   avg(size([(n)-[:SEMANTIC_SIMILARITY]-()|1])) as avg_cluster_connectivity\n            \"\"\").single()\n            \n            print(f\"Created {cluster_stats['cluster_count']} semantic clusters\")\n            \n            # Name clusters if LLM is available\n            if llm_client:\n                print(\"Generating cluster names...\")\n                cluster_results = session.run(\"\"\"\n                MATCH (n)\n                WHERE n.semantic_cluster IS NOT NULL\n                WITH n.semantic_cluster as cluster_id,\n                     collect(DISTINCT CASE \n                         WHEN 'Entity' IN labels(n) THEN n.name + ' (' + n.type + ')'\n                         ELSE n.title\n                     END)[..20] as member_names,\n                     count(n) as cluster_size\n                WHERE cluster_size >= 3\n                RETURN cluster_id, member_names, cluster_size\n                ORDER BY cluster_size DESC\n                LIMIT 50\n                \"\"\")\n                \n                for cluster in cluster_results:\n                    cluster_id = cluster[\"cluster_id\"]\n                    member_names = cluster[\"member_names\"]\n                    \n                    # Generate cluster name with LLM\n                    prompt = f\"\"\"\n                    Create a short (2-4 word) descriptive name for a topic cluster containing these concepts:\n                    {', '.join(member_names)}\n                    \n                    The name should capture the common theme. Return only the name.\n                    \"\"\"\n                    \n                    try:\n                        cluster_name = llm_client.invoke(prompt).content.strip()\n                        \n                        # Update nodes with cluster name\n                        session.run(\"\"\"\n                        MATCH (n)\n                        WHERE n.semantic_cluster = $cluster_id\n                        SET n.cluster_name = $cluster_name\n                        \"\"\", {\"cluster_id\": cluster_id, \"cluster_name\": cluster_name})\n                    except:\n                        pass  # Skip if LLM fails\n                        \n            # Clean up projection\n            session.run(\"CALL gds.graph.drop('semantic_graph')\")\n            \n            print(\"‚úì Semantic clustering completed successfully\")\n            \n    except Exception as e:\n        print(f\"Error implementing semantic clustering: {e}\")\n\ndef get_semantic_clusters(neo4j_driver, min_size=3):\n    \"\"\"\n    Get semantic clusters and their characteristics.\n    \"\"\"\n    if not neo4j_driver:\n        return []\n        \n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            result = session.run(\"\"\"\n            MATCH (n)\n            WHERE n.semantic_cluster IS NOT NULL\n            WITH n.semantic_cluster as cluster_id,\n                 n.cluster_name as cluster_name,\n                 collect(DISTINCT {\n                     name: CASE \n                         WHEN 'Entity' IN labels(n) THEN n.name\n                         ELSE n.title\n                     END,\n                     type: labels(n)[0],\n                     pagerank: n.pagerank\n                 }) as members\n            WHERE size(members) >= $min_size\n            RETURN cluster_id,\n                   cluster_name,\n                   size(members) as size,\n                   members[..15] as sample_members,\n                   avg([m IN members | m.pagerank]) as avg_importance\n            ORDER BY size DESC\n            LIMIT 30\n            \"\"\", {\"min_size\": min_size})\n            \n            clusters = []\n            for record in result:\n                clusters.append({\n                    \"id\": record[\"cluster_id\"],\n                    \"name\": record[\"cluster_name\"] or f\"Cluster {record['cluster_id']}\",\n                    \"size\": record[\"size\"],\n                    \"sample_members\": record[\"sample_members\"],\n                    \"importance\": record[\"avg_importance\"] or 0\n                })\n                \n            return clusters\n            \n    except Exception as e:\n        print(f\"Error getting semantic clusters: {e}\")\n        return []\n\ndef find_cross_cluster_connections(neo4j_driver):\n    \"\"\"\n    Find connections between different semantic clusters.\n    \"\"\"\n    if not neo4j_driver:\n        return []\n        \n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            result = session.run(\"\"\"\n            MATCH (n1)-[r]-(n2)\n            WHERE n1.semantic_cluster IS NOT NULL \n              AND n2.semantic_cluster IS NOT NULL\n              AND n1.semantic_cluster < n2.semantic_cluster\n              AND type(r) IN ['SUPPORTS', 'CONTRADICTS', 'EXPANDS_ON', 'KEY_CONNECTION']\n            WITH n1.semantic_cluster as cluster1,\n                 n2.semantic_cluster as cluster2,\n                 n1.cluster_name as name1,\n                 n2.cluster_name as name2,\n                 type(r) as rel_type,\n                 count(*) as connection_count\n            RETURN cluster1, cluster2, name1, name2, \n                   collect({type: rel_type, count: connection_count}) as connections,\n                   sum(connection_count) as total_connections\n            ORDER BY total_connections DESC\n            LIMIT 20\n            \"\"\")\n            \n            cross_connections = []\n            for record in result:\n                cross_connections.append({\n                    \"cluster1\": {\n                        \"id\": record[\"cluster1\"],\n                        \"name\": record[\"name1\"] or f\"Cluster {record['cluster1']}\"\n                    },\n                    \"cluster2\": {\n                        \"id\": record[\"cluster2\"],\n                        \"name\": record[\"name2\"] or f\"Cluster {record['cluster2']}\"\n                    },\n                    \"connections\": record[\"connections\"],\n                    \"total\": record[\"total_connections\"]\n                })\n                \n            return cross_connections\n            \n    except Exception as e:\n        print(f\"Error finding cross-cluster connections: {e}\")\n        return []",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 8.4: Temporal Pattern Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def analyze_temporal_patterns(neo4j_driver):\n    \"\"\"\n    Analyze temporal patterns in podcast content including topic evolution and trend detection.\n    \"\"\"\n    if not neo4j_driver:\n        return {}\n        \n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    patterns = {}\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            # 1. Episode Release Patterns\n            release_patterns = session.run(\"\"\"\n            MATCH (e:Episode)\n            WHERE e.published_date IS NOT NULL\n            WITH e ORDER BY e.published_date\n            WITH collect({\n                date: e.published_date,\n                title: e.title,\n                duration: e.duration_seconds\n            }) as episodes\n            RETURN size(episodes) as total_episodes,\n                   episodes[0].date as first_episode,\n                   episodes[-1].date as last_episode,\n                   avg([e IN episodes | e.duration]) as avg_duration\n            \"\"\").single()\n            \n            patterns['release_info'] = dict(release_patterns)\n            \n            # 2. Topic Evolution Over Time\n            topic_evolution = session.run(\"\"\"\n            MATCH (e:Episode)-[r:HAS_TOPIC]->(t:Topic)\n            WHERE e.published_date IS NOT NULL\n            WITH t.name as topic,\n                 e.published_date as date,\n                 r.score as score\n            ORDER BY date\n            WITH topic, \n                 collect({date: date, score: score}) as timeline\n            WHERE size(timeline) >= 3\n            RETURN topic,\n                   size(timeline) as episode_count,\n                   timeline[0].date as first_mention,\n                   timeline[-1].date as last_mention,\n                   avg([t IN timeline | t.score]) as avg_score,\n                   timeline\n            ORDER BY episode_count DESC\n            LIMIT 20\n            \"\"\")\n            \n            patterns['evolving_topics'] = []\n            for record in topic_evolution:\n                timeline = record['timeline']\n                # Calculate trend (increasing/decreasing scores)\n                if len(timeline) >= 3:\n                    early_avg = sum(t['score'] for t in timeline[:len(timeline)//2]) / (len(timeline)//2)\n                    late_avg = sum(t['score'] for t in timeline[len(timeline)//2:]) / (len(timeline) - len(timeline)//2)\n                    trend = 'increasing' if late_avg > early_avg * 1.1 else 'decreasing' if late_avg < early_avg * 0.9 else 'stable'\n                else:\n                    trend = 'stable'\n                    \n                patterns['evolving_topics'].append({\n                    'topic': record['topic'],\n                    'episodes': record['episode_count'],\n                    'first_mention': record['first_mention'],\n                    'last_mention': record['last_mention'],\n                    'avg_score': record['avg_score'],\n                    'trend': trend\n                })\n            \n            # 3. Entity Appearance Patterns\n            entity_patterns = session.run(\"\"\"\n            MATCH (e:Entity)-[:MENTIONED_IN]->(ep:Episode)\n            WHERE ep.published_date IS NOT NULL\n            WITH e, \n                 count(DISTINCT ep) as episode_count,\n                 collect(DISTINCT ep.published_date) as dates\n            WHERE episode_count >= 2\n            WITH e, episode_count, dates,\n                 duration.between(min(dates), max(dates)).days as span_days\n            RETURN e.name as entity,\n                   e.type as type,\n                   episode_count,\n                   span_days,\n                   CASE \n                       WHEN span_days > 0 THEN episode_count * 30.0 / span_days\n                       ELSE 0\n                   END as mentions_per_month\n            ORDER BY mentions_per_month DESC\n            LIMIT 20\n            \"\"\")\n            \n            patterns['recurring_entities'] = [dict(record) for record in entity_patterns]\n            \n            # 4. Cross-Episode Connections\n            cross_episode = session.run(\"\"\"\n            MATCH (ep1:Episode)-[r:TOPIC_EVOLUTION]->(ep2:Episode)\n            WITH type(r) as evolution_type,\n                 r.relation_type as relation,\n                 count(*) as count\n            RETURN evolution_type, relation, count\n            ORDER BY count DESC\n            \"\"\")\n            \n            patterns['cross_episode_patterns'] = [dict(record) for record in cross_episode]\n            \n            # 5. Insight Category Distribution Over Time\n            insight_timeline = session.run(\"\"\"\n            MATCH (i:Insight)-[:FROM_EPISODE]->(e:Episode)\n            WHERE e.published_date IS NOT NULL\n            WITH e.published_date as date,\n                 i.insight_type as category,\n                 count(i) as count\n            ORDER BY date\n            WITH date,\n                 collect({category: category, count: count}) as categories\n            RETURN date,\n                   [c IN categories | c.count] as counts,\n                   [c IN categories | c.category] as category_names\n            ORDER BY date\n            \"\"\")\n            \n            patterns['insight_timeline'] = [dict(record) for record in insight_timeline]\n            \n            return patterns\n            \n    except Exception as e:\n        print(f\"Error analyzing temporal patterns: {e}\")\n        return patterns\n\ndef find_topic_trajectories(neo4j_driver, min_episodes=3):\n    \"\"\"\n    Find how topics evolve and transform across episodes.\n    \"\"\"\n    if not neo4j_driver:\n        return []\n        \n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            result = session.run(\"\"\"\n            MATCH (t:Topic)<-[r:HAS_TOPIC]-(e:Episode)\n            WHERE e.published_date IS NOT NULL\n            WITH t, \n                 collect({\n                     episode: e.title,\n                     date: e.published_date,\n                     score: r.score,\n                     evidence: r.evidence\n                 }) as episodes\n            WHERE size(episodes) >= $min_episodes\n            WITH t, episodes\n            ORDER BY t.avg_score DESC\n            RETURN t.name as topic,\n                   size(episodes) as episode_count,\n                   episodes,\n                   t.avg_score as overall_score\n            LIMIT 15\n            \"\"\", {\"min_episodes\": min_episodes})\n            \n            trajectories = []\n            for record in result:\n                episodes = sorted(record['episodes'], key=lambda x: x['date'])\n                \n                # Analyze trajectory\n                scores = [ep['score'] for ep in episodes]\n                avg_change = sum(abs(scores[i] - scores[i-1]) for i in range(1, len(scores))) / (len(scores) - 1) if len(scores) > 1 else 0\n                \n                trajectory = {\n                    'topic': record['topic'],\n                    'episode_count': record['episode_count'],\n                    'overall_score': record['overall_score'],\n                    'volatility': avg_change,\n                    'episodes': episodes,\n                    'peak_episode': max(episodes, key=lambda x: x['score'])['episode'],\n                    'peak_score': max(scores)\n                }\n                \n                trajectories.append(trajectory)\n                \n            return trajectories\n            \n    except Exception as e:\n        print(f\"Error finding topic trajectories: {e}\")\n        return []\n\ndef detect_emerging_trends(neo4j_driver, lookback_episodes=5):\n    \"\"\"\n    Detect emerging trends by analyzing recent episodes.\n    \"\"\"\n    if not neo4j_driver:\n        return []\n        \n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            # Get recent episodes\n            recent_episodes = session.run(\"\"\"\n            MATCH (e:Episode)\n            WHERE e.published_date IS NOT NULL\n            RETURN e.id as id\n            ORDER BY e.published_date DESC\n            LIMIT $limit\n            \"\"\", {\"limit\": lookback_episodes}).value()\n            \n            if not recent_episodes:\n                return []\n            \n            # Find entities/topics that appear frequently in recent episodes but not earlier\n            result = session.run(\"\"\"\n            WITH $recent_episodes as recent_ids\n            \n            // Get entities in recent episodes\n            MATCH (e:Entity)-[:MENTIONED_IN]->(ep:Episode)\n            WHERE ep.id IN recent_ids\n            WITH e, count(DISTINCT ep) as recent_count\n            \n            // Get total appearances\n            MATCH (e)-[:MENTIONED_IN]->(all_ep:Episode)\n            WITH e, recent_count, count(DISTINCT all_ep) as total_count\n            \n            // Calculate emergence score\n            WITH e, \n                 recent_count,\n                 total_count,\n                 recent_count * 1.0 / total_count as recency_ratio\n            WHERE recency_ratio > 0.5 AND total_count >= 2\n            \n            RETURN e.name as name,\n                   e.type as type,\n                   recent_count,\n                   total_count,\n                   recency_ratio,\n                   e.pagerank as importance\n            ORDER BY recency_ratio DESC, importance DESC\n            LIMIT 20\n            \"\"\", {\"recent_episodes\": recent_episodes})\n            \n            trends = []\n            for record in result:\n                trends.append({\n                    'entity': record['name'],\n                    'type': record['type'],\n                    'recent_mentions': record['recent_count'],\n                    'total_mentions': record['total_count'],\n                    'emergence_score': record['recency_ratio'],\n                    'importance': record['importance'] or 0\n                })\n                \n            return trends\n            \n    except Exception as e:\n        print(f\"Error detecting emerging trends: {e}\")\n        return []",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 8.5: Knowledge Graph Statistics & Aggregation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def collect_knowledge_graph_stats(neo4j_driver):\n    \"\"\"\n    Collect comprehensive statistics about the knowledge graph for analysis and verification.\n    \"\"\"\n    if not neo4j_driver:\n        print(\"Neo4j driver not available. Cannot collect statistics.\")\n        return None\n    \n    stats = {}\n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            # Node counts by type\n            result = session.run(\"\"\"\n            MATCH (n)\n            WITH labels(n)[0] as nodeType, count(n) as count\n            RETURN nodeType, count\n            ORDER BY count DESC\n            \"\"\")\n            \n            node_counts = {record[\"nodeType\"]: record[\"count\"] for record in result}\n            stats[\"node_counts\"] = node_counts\n            stats[\"total_nodes\"] = sum(node_counts.values())\n            \n            # Relationship counts by type\n            result = session.run(\"\"\"\n            MATCH ()-[r]->() \n            WITH type(r) as relType, count(r) as count\n            RETURN relType, count\n            ORDER BY count DESC\n            \"\"\")\n            \n            rel_counts = {record[\"relType\"]: record[\"count\"] for record in result}\n            stats[\"relationship_counts\"] = rel_counts\n            stats[\"total_relationships\"] = sum(rel_counts.values())\n            \n            # Semantic relationships\n            semantic_rels = [\"SIMILAR_CONCEPT\", \"CONTRADICTS\", \"SUPPORTS\", \"EXPANDS_ON\", \n                            \"MENTIONED_WITH\", \"SEMANTIC_SIMILARITY\", \"TOPIC_EVOLUTION\", \n                            \"KEY_CONNECTION\"]\n            \n            relationship_taxonomy_types = [\"RELATIONSHIP_HIERARCHICAL\", \"RELATIONSHIP_INFLUENTIAL\", \n                                          \"RELATIONSHIP_COMPARATIVE\", \"RELATIONSHIP_TEMPORAL\", \n                                          \"RELATIONSHIP_FUNCTIONAL\"]\n            \n            semantic_rels.extend(relationship_taxonomy_types)\n            \n            semantic_rel_count = sum(rel_counts.get(rel, 0) for rel in semantic_rels)\n            stats[\"semantic_relationship_count\"] = semantic_rel_count\n            \n            # Extract relationship counts by taxonomy type\n            relationship_by_type = {}\n            for rel_type in relationship_taxonomy_types:\n                if rel_type in rel_counts:\n                    relationship_by_type[rel_type.replace(\"RELATIONSHIP_\", \"\")] = rel_counts[rel_type]\n                \n            stats[\"extracted_relationship_types\"] = relationship_by_type\n            \n            # Cluster counts\n            result = session.run(\"\"\"\n            MATCH (n)\n            WHERE n.semantic_cluster IS NOT NULL\n            RETURN count(DISTINCT n.semantic_cluster) as cluster_count\n            \"\"\")\n            \n            if result.peek():\n                stats[\"semantic_cluster_count\"] = result.single()[\"cluster_count\"]\n            else:\n                stats[\"semantic_cluster_count\"] = 0\n            \n            # Cross-episode connections\n            result = session.run(\"\"\"\n            MATCH ()-[r:TOPIC_EVOLUTION]->()\n            RETURN count(r) as count\n            \"\"\")\n            \n            if result.peek():\n                stats[\"cross_episode_count\"] = result.single()[\"count\"]\n            else:\n                stats[\"cross_episode_count\"] = 0\n            \n            # Top entities by PageRank\n            result = session.run(\"\"\"\n            MATCH (e:Entity)\n            WHERE e.pagerank IS NOT NULL\n            RETURN e.name as name, e.type as type, e.pagerank as score\n            ORDER BY e.pagerank DESC\n            LIMIT 10\n            \"\"\")\n            \n            stats[\"top_entities\"] = [{\n                \"name\": record[\"name\"], \n                \"type\": record[\"type\"], \n                \"pagerank\": record[\"score\"]\n            } for record in result]\n            \n            # Content statistics\n            result = session.run(\"\"\"\n            MATCH (e:Episode)\n            WITH count(e) as episode_count,\n                 avg(e.duration_seconds) as avg_duration,\n                 sum(e.duration_seconds) as total_duration\n            RETURN episode_count, avg_duration, total_duration\n            \"\"\")\n            \n            content_stats = result.single()\n            stats[\"content\"] = {\n                \"episodes\": content_stats[\"episode_count\"],\n                \"avg_duration_minutes\": (content_stats[\"avg_duration\"] or 0) / 60,\n                \"total_hours\": (content_stats[\"total_duration\"] or 0) / 3600\n            }\n            \n            # Insight distribution\n            result = session.run(\"\"\"\n            MATCH (i:Insight)\n            WITH i.insight_type as type, count(i) as count\n            RETURN type, count\n            ORDER BY count DESC\n            \"\"\")\n            \n            stats[\"insight_distribution\"] = {\n                record[\"type\"]: record[\"count\"] for record in result\n            }\n            \n            # Graph density metrics\n            result = session.run(\"\"\"\n            MATCH (n)\n            WITH count(n) as node_count\n            MATCH ()-[r]->()\n            WITH node_count, count(r) as rel_count\n            RETURN node_count,\n                   rel_count,\n                   CASE \n                       WHEN node_count > 1 \n                       THEN rel_count * 1.0 / (node_count * (node_count - 1))\n                       ELSE 0\n                   END as density\n            \"\"\")\n            \n            density_stats = result.single()\n            stats[\"graph_metrics\"] = {\n                \"density\": density_stats[\"density\"],\n                \"avg_degree\": 2 * density_stats[\"rel_count\"] / density_stats[\"node_count\"] if density_stats[\"node_count\"] > 0 else 0\n            }\n            \n            return stats\n            \n    except Exception as e:\n        print(f\"Error collecting knowledge graph statistics: {e}\")\n        return None\n\ndef generate_knowledge_summary(stats):\n    \"\"\"\n    Generate a human-readable summary of the knowledge graph.\n    \"\"\"\n    if not stats:\n        return \"No statistics available.\"\n        \n    summary = []\n    summary.append(\"=== Knowledge Graph Summary ===\\n\")\n    \n    # Overall statistics\n    summary.append(f\"Total Nodes: {stats['total_nodes']:,}\")\n    summary.append(f\"Total Relationships: {stats['total_relationships']:,}\")\n    summary.append(f\"Semantic Relationships: {stats['semantic_relationship_count']:,}\")\n    summary.append(f\"Graph Density: {stats['graph_metrics']['density']:.4f}\")\n    summary.append(\"\")\n    \n    # Content overview\n    if 'content' in stats:\n        summary.append(\"Content Overview:\")\n        summary.append(f\"  Episodes: {stats['content']['episodes']}\")\n        summary.append(f\"  Total Hours: {stats['content']['total_hours']:.1f}\")\n        summary.append(f\"  Avg Duration: {stats['content']['avg_duration_minutes']:.1f} minutes\")\n        summary.append(\"\")\n    \n    # Node distribution\n    summary.append(\"Node Distribution:\")\n    for node_type, count in sorted(stats['node_counts'].items(), key=lambda x: x[1], reverse=True):\n        summary.append(f\"  {node_type}: {count:,}\")\n    summary.append(\"\")\n    \n    # Top entities\n    if 'top_entities' in stats and stats['top_entities']:\n        summary.append(\"Most Influential Entities:\")\n        for i, entity in enumerate(stats['top_entities'][:5], 1):\n            summary.append(f\"  {i}. {entity['name']} ({entity['type']}): {entity['pagerank']:.4f}\")\n        summary.append(\"\")\n    \n    # Insight types\n    if 'insight_distribution' in stats:\n        summary.append(\"Insight Distribution:\")\n        total_insights = sum(stats['insight_distribution'].values())\n        for insight_type, count in sorted(stats['insight_distribution'].items(), key=lambda x: x[1], reverse=True):\n            percentage = (count / total_insights * 100) if total_insights > 0 else 0\n            summary.append(f\"  {insight_type}: {count} ({percentage:.1f}%)\")\n        summary.append(\"\")\n    \n    # Clustering info\n    if stats.get('semantic_cluster_count', 0) > 0:\n        summary.append(f\"Semantic Clusters: {stats['semantic_cluster_count']}\")\n        summary.append(f\"Cross-Episode Connections: {stats['cross_episode_count']}\")\n        summary.append(\"\")\n    \n    # Relationship types\n    if 'extracted_relationship_types' in stats and stats['extracted_relationship_types']:\n        summary.append(\"Extracted Relationships:\")\n        for rel_type, count in stats['extracted_relationship_types'].items():\n            summary.append(f\"  {rel_type}: {count}\")\n    \n    return \"\\n\".join(summary)\n\ndef export_graph_metrics(neo4j_driver, output_file='graph_metrics.json'):\n    \"\"\"\n    Export comprehensive graph metrics to a JSON file for external analysis.\n    \"\"\"\n    stats = collect_knowledge_graph_stats(neo4j_driver)\n    \n    if not stats:\n        print(\"No statistics to export.\")\n        return\n        \n    # Add timestamp\n    stats['generated_at'] = datetime.now().isoformat()\n    \n    # Save to file\n    try:\n        with open(output_file, 'w') as f:\n            json.dump(stats, f, indent=2, default=str)\n        print(f\"Graph metrics exported to {output_file}\")\n    except Exception as e:\n        print(f\"Error exporting metrics: {e}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cross-episode relationship building functions\ndef build_cross_episode_relationships(neo4j_driver, podcast_id=None):\n    \"\"\"\n    Build relationships between episodes, entities, and insights across a podcast series.\n    \n    Args:\n        neo4j_driver: Neo4j driver\n        podcast_id: Optional podcast ID to limit scope\n        \n    Returns:\n        Number of relationships created\n    \"\"\"\n    if not neo4j_driver:\n        logger.warning(\"Neo4j driver not available\")\n        return 0\n        \n    database = PodcastConfig.NEO4J_DATABASE\n    relationships_created = 0\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            # Build entity relationships across episodes\n            query = \"\"\"\n            MATCH (e1:Entity)-[:MENTIONED_IN]->(ep1:Episode)\n            MATCH (e2:Entity)-[:MENTIONED_IN]->(ep2:Episode)\n            WHERE e1.normalized_name = e2.normalized_name \n              AND e1.type = e2.type \n              AND ep1.id <> ep2.id\n              AND ep1.podcast_id = ep2.podcast_id\n            \"\"\"\n            \n            if podcast_id:\n                query += \" AND ep1.podcast_id = $podcast_id\"\n                \n            query += \"\"\"\n            MERGE (e1)-[r:SAME_ENTITY_AS]->(e2)\n            ON CREATE SET r.created = datetime()\n            RETURN count(r) as count\n            \"\"\"\n            \n            params = {\"podcast_id\": podcast_id} if podcast_id else {}\n            result = session.run(query, params)\n            count = result.single()[\"count\"]\n            relationships_created += count\n            logger.info(f\"Created {count} same-entity relationships\")\n            \n            # Build temporal relationships between episodes\n            query = \"\"\"\n            MATCH (e1:Episode), (e2:Episode)\n            WHERE e1.podcast_id = e2.podcast_id \n              AND datetime(e1.published_date) < datetime(e2.published_date)\n            \"\"\"\n            \n            if podcast_id:\n                query += \" AND e1.podcast_id = $podcast_id\"\n                \n            query += \"\"\"\n            WITH e1, e2, duration.between(datetime(e1.published_date), datetime(e2.published_date)) as time_between\n            WHERE time_between.days < 90  // Within 3 months\n            MERGE (e1)-[r:PRECEDED_BY {days_apart: time_between.days}]->(e2)\n            RETURN count(r) as count\n            \"\"\"\n            \n            result = session.run(query, params)\n            count = result.single()[\"count\"]\n            relationships_created += count\n            logger.info(f\"Created {count} temporal episode relationships\")\n            \n            # Build thematic relationships between episodes based on shared entities\n            query = \"\"\"\n            MATCH (e1:Episode)<-[:MENTIONED_IN]-(entity:Entity)-[:MENTIONED_IN]->(e2:Episode)\n            WHERE e1.id <> e2.id AND e1.podcast_id = e2.podcast_id\n            \"\"\"\n            \n            if podcast_id:\n                query += \" AND e1.podcast_id = $podcast_id\"\n                \n            query += \"\"\"\n            WITH e1, e2, count(distinct entity) as shared_entities\n            WHERE shared_entities >= 3\n            MERGE (e1)-[r:THEMATICALLY_RELATED {shared_entities: shared_entities}]->(e2)\n            RETURN count(r) as count\n            \"\"\"\n            \n            result = session.run(query, params)\n            count = result.single()[\"count\"]\n            relationships_created += count\n            logger.info(f\"Created {count} thematic episode relationships\")\n            \n        return relationships_created\n        \n    except Exception as e:\n        logger.error(f\"Error building cross-episode relationships: {e}\")\n        return relationships_created\n\ndef discover_cross_episode_patterns(neo4j_driver, min_episodes=3):\n    \"\"\"\n    Discover patterns and themes that span multiple episodes.\n    \n    Args:\n        neo4j_driver: Neo4j driver\n        min_episodes: Minimum episodes for a pattern to be significant\n        \n    Returns:\n        List of discovered patterns\n    \"\"\"\n    if not neo4j_driver:\n        return []\n        \n    database = PodcastConfig.NEO4J_DATABASE\n    patterns = []\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            # Find recurring entities across episodes\n            result = session.run(\"\"\"\n            MATCH (e:Entity)-[:MENTIONED_IN]->(ep:Episode)\n            WITH e, count(distinct ep) as episode_count, collect(distinct ep.title) as episodes\n            WHERE episode_count >= $min_episodes\n            RETURN e.name as entity_name, \n                   e.type as entity_type, \n                   episode_count,\n                   episodes[0..5] as sample_episodes\n            ORDER BY episode_count DESC\n            LIMIT 20\n            \"\"\", {\"min_episodes\": min_episodes})\n            \n            recurring_entities = []\n            for record in result:\n                recurring_entities.append({\n                    \"entity_name\": record[\"entity_name\"],\n                    \"entity_type\": record[\"entity_type\"],\n                    \"episode_count\": record[\"episode_count\"],\n                    \"sample_episodes\": record[\"sample_episodes\"]\n                })\n                \n            if recurring_entities:\n                patterns.append({\n                    \"pattern_type\": \"recurring_entities\",\n                    \"description\": f\"Entities mentioned in {min_episodes}+ episodes\",\n                    \"data\": recurring_entities\n                })\n            \n            # Find insight themes across episodes\n            result = session.run(\"\"\"\n            MATCH (i:Insight)-[:EXTRACTED_FROM]->(ep:Episode)\n            WITH i.insight_type as type, count(distinct ep) as episode_count, \n                 collect(distinct {episode: ep.title, insight: i.title})[0..5] as samples\n            WHERE episode_count >= $min_episodes\n            RETURN type, episode_count, samples\n            ORDER BY episode_count DESC\n            \"\"\", {\"min_episodes\": min_episodes})\n            \n            insight_themes = []\n            for record in result:\n                insight_themes.append({\n                    \"insight_type\": record[\"type\"],\n                    \"episode_count\": record[\"episode_count\"],\n                    \"samples\": record[\"samples\"]\n                })\n                \n            if insight_themes:\n                patterns.append({\n                    \"pattern_type\": \"insight_themes\",\n                    \"description\": f\"Insight types spanning {min_episodes}+ episodes\",\n                    \"data\": insight_themes\n                })\n            \n            # Find entity evolution over time\n            result = session.run(\"\"\"\n            MATCH (e:Entity)-[:MENTIONED_IN]->(ep:Episode)\n            WHERE exists(e.importance)\n            WITH e, ep, e.importance as importance\n            ORDER BY ep.published_date\n            WITH e, collect({date: ep.published_date, importance: importance}) as timeline\n            WHERE size(timeline) >= $min_episodes\n            RETURN e.name as entity_name, \n                   e.type as entity_type,\n                   timeline[0..10] as evolution\n            LIMIT 10\n            \"\"\", {\"min_episodes\": min_episodes})\n            \n            entity_evolution = []\n            for record in result:\n                entity_evolution.append({\n                    \"entity_name\": record[\"entity_name\"],\n                    \"entity_type\": record[\"entity_type\"],\n                    \"evolution\": record[\"evolution\"]\n                })\n                \n            if entity_evolution:\n                patterns.append({\n                    \"pattern_type\": \"entity_evolution\",\n                    \"description\": \"How entity importance changes over time\",\n                    \"data\": entity_evolution\n                })\n                \n        return patterns\n        \n    except Exception as e:\n        logger.error(f\"Error discovering cross-episode patterns: {e}\")\n        return patterns",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 10: Visualization",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 10.1: Knowledge Graph Visualization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport numpy as np\n\ndef visualize_knowledge_graph_stats(neo4j_driver):\n    \"\"\"\n    Create interactive visualizations of knowledge graph statistics using Plotly.\n    \"\"\"\n    if not neo4j_driver:\n        print(\"Neo4j driver not available.\")\n        return\n        \n    stats = collect_knowledge_graph_stats(neo4j_driver)\n    if not stats:\n        print(\"No statistics to visualize.\")\n        return\n        \n    # Create subplots\n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=('Node Distribution', 'Top Entities by Influence', \n                       'Insight Types', 'Relationship Types'),\n        specs=[[{'type': 'domain'}, {'type': 'bar'}],\n               [{'type': 'bar'}, {'type': 'bar'}]]\n    )\n    \n    # 1. Node Distribution Pie Chart\n    node_types = list(stats['node_counts'].keys())\n    node_values = list(stats['node_counts'].values())\n    \n    fig.add_trace(\n        go.Pie(labels=node_types, values=node_values, hole=0.3),\n        row=1, col=1\n    )\n    \n    # 2. Top Entities Bar Chart\n    if stats.get('top_entities'):\n        entities = stats['top_entities'][:10]\n        entity_names = [f\"{e['name']} ({e['type']})\" for e in entities]\n        entity_scores = [e['pagerank'] for e in entities]\n        \n        fig.add_trace(\n            go.Bar(x=entity_scores, y=entity_names, orientation='h'),\n            row=1, col=2\n        )\n    \n    # 3. Insight Types Bar Chart\n    if stats.get('insight_distribution'):\n        insight_types = list(stats['insight_distribution'].keys())\n        insight_counts = list(stats['insight_distribution'].values())\n        \n        fig.add_trace(\n            go.Bar(x=insight_types, y=insight_counts),\n            row=2, col=1\n        )\n    \n    # 4. Relationship Types Bar Chart\n    rel_types = list(stats['relationship_counts'].keys())[:10]\n    rel_counts = [stats['relationship_counts'][t] for t in rel_types]\n    \n    fig.add_trace(\n        go.Bar(x=rel_types, y=rel_counts),\n        row=2, col=2\n    )\n    \n    # Update layout\n    fig.update_layout(\n        title_text=\"Knowledge Graph Overview\",\n        height=800,\n        showlegend=False\n    )\n    \n    # Update axes\n    fig.update_xaxes(title_text=\"PageRank Score\", row=1, col=2)\n    fig.update_xaxes(title_text=\"Insight Type\", row=2, col=1)\n    fig.update_xaxes(title_text=\"Relationship Type\", row=2, col=2, tickangle=-45)\n    fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Count\", row=2, col=2)\n    \n    fig.show()\n    \n    # Print summary\n    print(generate_knowledge_summary(stats))\n\ndef visualize_semantic_clusters(neo4j_driver):\n    \"\"\"\n    Visualize semantic clusters as a network graph.\n    \"\"\"\n    clusters = get_semantic_clusters(neo4j_driver)\n    if not clusters:\n        print(\"No semantic clusters to visualize.\")\n        return\n        \n    # Create network visualization data\n    nodes = []\n    edges = []\n    \n    # Add cluster nodes\n    for i, cluster in enumerate(clusters[:20]):  # Top 20 clusters\n        nodes.append({\n            'id': f\"cluster_{cluster['id']}\",\n            'label': cluster['name'],\n            'size': np.log(cluster['size'] + 1) * 10,\n            'type': 'cluster'\n        })\n        \n        # Add sample members as nodes\n        for member in cluster['sample_members'][:5]:\n            member_id = f\"member_{cluster['id']}_{member['name']}\"\n            nodes.append({\n                'id': member_id,\n                'label': member['name'],\n                'size': 5,\n                'type': member['type']\n            })\n            \n            # Add edge from cluster to member\n            edges.append({\n                'source': f\"cluster_{cluster['id']}\",\n                'target': member_id\n            })\n    \n    # Create network graph\n    edge_trace = []\n    for edge in edges:\n        source_node = next(n for n in nodes if n['id'] == edge['source'])\n        target_node = next(n for n in nodes if n['id'] == edge['target'])\n        \n        edge_trace.append(go.Scatter(\n            x=[source_node.get('x', 0), target_node.get('x', 0), None],\n            y=[source_node.get('y', 0), target_node.get('y', 0), None],\n            mode='lines',\n            line=dict(width=0.5, color='#888'),\n            hoverinfo='none'\n        ))\n    \n    node_trace = go.Scatter(\n        x=[n.get('x', np.random.random()) for n in nodes],\n        y=[n.get('y', np.random.random()) for n in nodes],\n        mode='markers+text',\n        text=[n['label'] for n in nodes],\n        textposition='top center',\n        marker=dict(\n            size=[n['size'] for n in nodes],\n            color=['red' if n['type'] == 'cluster' else 'blue' for n in nodes],\n            line=dict(width=2, color='white')\n        ),\n        hovertext=[f\"{n['label']} ({n['type']})\" for n in nodes],\n        hoverinfo='text'\n    )\n    \n    fig = go.Figure(data=edge_trace + [node_trace])\n    \n    fig.update_layout(\n        title='Semantic Clusters',\n        showlegend=False,\n        hovermode='closest',\n        margin=dict(b=0, l=0, r=0, t=40),\n        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        height=600\n    )\n    \n    fig.show()\n\ndef visualize_temporal_patterns(neo4j_driver):\n    \"\"\"\n    Visualize temporal patterns in the podcast data.\n    \"\"\"\n    patterns = analyze_temporal_patterns(neo4j_driver)\n    if not patterns:\n        print(\"No temporal patterns to visualize.\")\n        return\n        \n    # Create subplots for different temporal visualizations\n    fig = make_subplots(\n        rows=2, cols=2,\n        subplot_titles=('Topic Evolution', 'Entity Frequency Over Time',\n                       'Episode Release Pattern', 'Emerging Trends'),\n        specs=[[{'secondary_y': False}, {'secondary_y': False}],\n               [{'secondary_y': False}, {'type': 'bar'}]]\n    )\n    \n    # 1. Topic Evolution\n    if patterns.get('evolving_topics'):\n        top_topics = patterns['evolving_topics'][:5]\n        for topic_data in top_topics:\n            topic = topic_data['topic']\n            trend = topic_data['trend']\n            color = 'green' if trend == 'increasing' else 'red' if trend == 'decreasing' else 'blue'\n            \n            fig.add_trace(\n                go.Scatter(\n                    x=[topic_data['first_mention'], topic_data['last_mention']],\n                    y=[topic_data['avg_score'], topic_data['avg_score']],\n                    mode='lines+markers',\n                    name=f\"{topic} ({trend})\",\n                    line=dict(color=color, width=2),\n                    marker=dict(size=8)\n                ),\n                row=1, col=1\n            )\n    \n    # 2. Entity Frequency\n    if patterns.get('recurring_entities'):\n        entities_df = pd.DataFrame(patterns['recurring_entities'][:10])\n        \n        fig.add_trace(\n            go.Bar(\n                x=entities_df['entity'],\n                y=entities_df['mentions_per_month'],\n                text=entities_df['type'],\n                textposition='auto',\n            ),\n            row=1, col=2\n        )\n    \n    # 3. Episode Release Pattern (Timeline)\n    if patterns.get('release_info'):\n        # This would need actual episode data\n        fig.add_trace(\n            go.Scatter(\n                x=[patterns['release_info']['first_episode'], \n                   patterns['release_info']['last_episode']],\n                y=[1, patterns['release_info']['total_episodes']],\n                mode='lines+markers',\n                name='Episode Count',\n                line=dict(color='purple', width=3)\n            ),\n            row=2, col=1\n        )\n    \n    # 4. Emerging Trends\n    trends = detect_emerging_trends(neo4j_driver)\n    if trends:\n        trend_df = pd.DataFrame(trends[:10])\n        \n        fig.add_trace(\n            go.Bar(\n                x=trend_df['entity'],\n                y=trend_df['emergence_score'],\n                text=trend_df['type'],\n                textposition='auto',\n                marker_color='orange'\n            ),\n            row=2, col=2\n        )\n    \n    # Update layout\n    fig.update_layout(\n        title_text=\"Temporal Analysis\",\n        height=800,\n        showlegend=True\n    )\n    \n    # Update axes\n    fig.update_xaxes(title_text=\"Date\", row=1, col=1)\n    fig.update_xaxes(title_text=\"Entity\", row=1, col=2, tickangle=-45)\n    fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n    fig.update_xaxes(title_text=\"Emerging Entity\", row=2, col=2, tickangle=-45)\n    \n    fig.update_yaxes(title_text=\"Topic Score\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Mentions/Month\", row=1, col=2)\n    fig.update_yaxes(title_text=\"Episode #\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Emergence Score\", row=2, col=2)\n    \n    fig.show()\n\ndef create_insight_dashboard(neo4j_driver, episode_id=None):\n    \"\"\"\n    Create an interactive dashboard for exploring insights.\n    \"\"\"\n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            # Get insights data\n            if episode_id:\n                query = \"\"\"\n                MATCH (i:Insight)-[:FROM_EPISODE]->(e:Episode {id: $episode_id})\n                RETURN i.title as title, \n                       i.insight_type as type,\n                       i.description as description,\n                       i.confidence as confidence\n                ORDER BY i.confidence DESC\n                \"\"\"\n                params = {\"episode_id\": episode_id}\n            else:\n                query = \"\"\"\n                MATCH (i:Insight)\n                RETURN i.title as title, \n                       i.insight_type as type,\n                       i.description as description,\n                       i.confidence as confidence\n                ORDER BY i.confidence DESC\n                LIMIT 100\n                \"\"\"\n                params = {}\n                \n            result = session.run(query, params)\n            insights = [dict(record) for record in result]\n            \n            if not insights:\n                print(\"No insights found.\")\n                return\n                \n            # Create DataFrame\n            df = pd.DataFrame(insights)\n            \n            # Create visualizations\n            fig = make_subplots(\n                rows=2, cols=2,\n                subplot_titles=('Insight Type Distribution', 'Confidence Distribution',\n                               'Top Insights by Confidence', 'Insight Word Cloud'),\n                specs=[[{'type': 'domain'}, {'type': 'histogram'}],\n                       [{'type': 'bar'}, {'type': 'scatter'}]]\n            )\n            \n            # 1. Type distribution\n            type_counts = df['type'].value_counts()\n            fig.add_trace(\n                go.Pie(labels=type_counts.index, values=type_counts.values),\n                row=1, col=1\n            )\n            \n            # 2. Confidence histogram\n            fig.add_trace(\n                go.Histogram(x=df['confidence'], nbinsx=20),\n                row=1, col=2\n            )\n            \n            # 3. Top insights\n            top_insights = df.nlargest(10, 'confidence')\n            fig.add_trace(\n                go.Bar(\n                    x=top_insights['confidence'],\n                    y=top_insights['title'].str[:50] + '...',\n                    orientation='h'\n                ),\n                row=2, col=1\n            )\n            \n            # 4. Word frequency (simplified)\n            all_text = ' '.join(df['title'] + ' ' + df['description'])\n            words = all_text.lower().split()\n            word_freq = pd.Series(words).value_counts().head(20)\n            \n            fig.add_trace(\n                go.Scatter(\n                    x=list(range(len(word_freq))),\n                    y=word_freq.values,\n                    mode='markers',\n                    marker=dict(size=word_freq.values/10),\n                    text=word_freq.index,\n                    textposition='top center'\n                ),\n                row=2, col=2\n            )\n            \n            fig.update_layout(\n                title_text=f\"Insight Dashboard{' for Episode ' + episode_id if episode_id else ''}\",\n                height=800,\n                showlegend=False\n            )\n            \n            fig.show()\n            \n            # Display sample insights\n            print(\"\\nSample High-Confidence Insights:\")\n            for _, insight in top_insights.head(5).iterrows():\n                print(f\"\\n{insight['type']}: {insight['title']}\")\n                print(f\"Confidence: {insight['confidence']:.2f}\")\n                print(f\"Description: {insight['description'][:200]}...\")\n                \n    except Exception as e:\n        print(f\"Error creating insight dashboard: {e}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 11: Pipeline Orchestration",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 11.1: Master Pipeline Orchestrator",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PodcastKnowledgePipeline:\n    \"\"\"\n    Master orchestrator for the podcast knowledge extraction pipeline.\n    Coordinates all components and manages the end-to-end processing workflow.\n    \"\"\"\n    \n    def __init__(self, config=None):\n        \"\"\"Initialize the pipeline with configuration.\"\"\"\n        self.config = config or PodcastConfig\n        self.neo4j_driver = None\n        self.task_router = None\n        self.embedding_client = None\n        self.checkpoint = ProgressCheckpoint()\n        \n    def initialize_components(self, use_large_context=True):\n        \"\"\"Initialize all pipeline components.\"\"\"\n        try:\n            print(\"Initializing pipeline components...\")\n            \n            # Initialize Neo4j\n            if self.neo4j_driver is None:\n                self.neo4j_driver = connect_to_neo4j(self.config)\n                setup_neo4j_schema(self.neo4j_driver)\n                \n            # Initialize task router for LLM routing\n            self.task_router = TaskRouter()\n            \n            # Initialize embedding client\n            self.embedding_client = initialize_embedding_model()\n            \n            print(\"‚úì All pipeline components initialized successfully\")\n            return True\n            \n        except Exception as e:\n            print(f\"‚úó Failed to initialize pipeline components: {e}\")\n            return False\n            \n    def cleanup(self):\n        \"\"\"Clean up resources and close connections.\"\"\"\n        if self.neo4j_driver:\n            try:\n                self.neo4j_driver.close()\n                print(\"Neo4j connection closed\")\n            except Exception as e:\n                print(f\"Warning: Error closing Neo4j connection: {e}\")\n                \n        # Clean up memory\n        cleanup_memory()\n        \n    def process_episode(self, podcast_config, episode, segmenter_config=None, \n                       output_dir=\"processed_podcasts\", use_large_context=True):\n        \"\"\"\n        Process a single episode through the pipeline with checkpoint support.\n        \"\"\"\n        print(f\"\\n{'='*50}\")\n        print(f\"PROCESSING EPISODE: {episode['title']}\")\n        print(f\"{'='*50}\\n\")\n        \n        episode_id = episode['id']\n        \n        # Check if episode already completed\n        completed_episodes = self.checkpoint.get_completed_episodes()\n        if episode_id in completed_episodes:\n            print(f\"Episode {episode_id} already completed, skipping\")\n            return None\n        \n        # Create output directory\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Initialize segmenter with optimized settings for large context\n        if use_large_context and segmenter_config:\n            segmenter_config = segmenter_config.copy()\n            segmenter_config['min_segment_tokens'] = segmenter_config.get('min_segment_tokens', 150)\n            segmenter_config['max_segment_tokens'] = segmenter_config.get('max_segment_tokens', 800)\n            \n        segmenter = EnhancedPodcastSegmenter(segmenter_config)\n        \n        # Download episode audio\n        audio_path = download_episode_audio(episode, podcast_config[\"id\"])\n        if not audio_path:\n            print(f\"Failed to download audio for episode {episode['id']}\")\n            return None\n        \n        # Check for transcript checkpoint\n        transcript_segments = self.checkpoint.load_episode_progress(episode_id, 'transcript')\n        \n        if transcript_segments is None:\n            # Process the audio\n            print(\"Transcribing audio...\")\n            processing_result = segmenter.process_audio(audio_path)\n            transcript_segments = processing_result['transcript']\n            \n            # Save transcript checkpoint\n            self.checkpoint.save_episode_progress(episode_id, 'transcript', transcript_segments)\n            print(f\"Created {len(transcript_segments)} segments\")\n        else:\n            print(f\"Loaded {len(transcript_segments)} segments from checkpoint\")\n        \n        # Process segments and extract knowledge\n        results = self._extract_knowledge(\n            podcast_config, episode, transcript_segments, use_large_context\n        )\n        \n        # Save to knowledge graph\n        if results and self.neo4j_driver:\n            self._save_to_knowledge_graph(\n                podcast_config, episode, results, transcript_segments\n            )\n        \n        # Mark episode as complete\n        self.checkpoint.save_episode_progress(episode_id, 'complete', {\n            'completed_at': datetime.now().isoformat(),\n            'insights_count': len(results.get('insights', [])),\n            'entities_count': len(results.get('entities', []))\n        })\n        \n        # Clean up intermediate checkpoints\n        self.checkpoint.clean_episode_checkpoints(episode_id)\n        \n        print(\"\\n‚úì Episode processing complete\")\n        return results\n    \n    def _extract_knowledge(self, podcast_config, episode, transcript_segments, use_large_context):\n        \"\"\"Extract insights, entities, and other knowledge from transcript.\"\"\"\n        episode_id = episode['id']\n        \n        # Check for extraction checkpoint\n        extraction_data = self.checkpoint.load_episode_progress(episode_id, 'extraction')\n        \n        if extraction_data is None:\n            print(\"\\nExtracting knowledge from transcript...\")\n            \n            # Convert transcript for LLM processing\n            full_transcript = convert_transcript_for_llm(transcript_segments)\n            \n            if use_large_context:\n                # Use large context model for full-episode extraction\n                print(\"Using large context model for extraction\")\n                \n                try:\n                    # Build combined extraction prompt\n                    combined_prompt = build_combined_extraction_prompt(\n                        podcast_config[\"name\"], episode[\"title\"], full_transcript, True\n                    )\n                    \n                    # Route request through task router\n                    result = self.task_router.route_request('insights', combined_prompt)\n                    \n                    # Parse response\n                    parsed_data = parse_combined_llm_response(result['response'])\n                    \n                    # Validate extracted data\n                    insights = extraction_validator.validate_insights(parsed_data.get('insights', []))\n                    entities = extraction_validator.validate_entities(parsed_data.get('entities', []))\n                    quotes = parsed_data.get('quotes', [])\n                    \n                    extraction_data = {\n                        'insights': insights,\n                        'entities': entities,\n                        'quotes': quotes,\n                        'model_used': result['model_used']\n                    }\n                    \n                    print(f\"Model used: {result['model_used']} (fallback: {result['fallback']})\")\n                    print(f\"Extracted {len(insights)} insights, {len(entities)} entities, {len(quotes)} quotes\")\n                    \n                except Exception as e:\n                    print(f\"Error during extraction: {e}\")\n                    extraction_data = {'insights': [], 'entities': [], 'quotes': []}\n            else:\n                # Process segments individually (traditional approach)\n                extraction_data = self._extract_segment_by_segment(\n                    podcast_config, episode, transcript_segments\n                )\n            \n            # Save extraction checkpoint\n            self.checkpoint.save_episode_progress(episode_id, 'extraction', extraction_data)\n        else:\n            print(f\"Loaded extraction from checkpoint\")\n            \n        return extraction_data\n    \n    def _extract_segment_by_segment(self, podcast_config, episode, transcript_segments):\n        \"\"\"Extract knowledge segment by segment (for smaller context models).\"\"\"\n        all_insights = []\n        all_entities = []\n        \n        for i, segment in enumerate(transcript_segments):\n            print(f\"\\rProcessing segment {i+1}/{len(transcript_segments)}\", end='')\n            \n            # Skip advertisements\n            if segment.get('is_advertisement', False):\n                continue\n            \n            segment_text = segment['text']\n            \n            # Extract insights for segment\n            prompt = build_insight_extraction_prompt(\n                podcast_config[\"name\"], episode[\"title\"], segment_text\n            )\n            result = self.task_router.route_request('insights', prompt)\n            insights = parse_insights_from_response(result['response'])\n            all_insights.extend(insights)\n            \n            # Extract entities for segment\n            entity_prompt = build_entity_extraction_prompt(segment_text)\n            result = self.task_router.route_request('entities', entity_prompt)\n            entities = parse_entities_from_response(result['response'])\n            all_entities.extend(entities)\n        \n        print()  # New line after progress\n        return {\n            'insights': all_insights,\n            'entities': all_entities,\n            'quotes': []\n        }\n    \n    def _save_to_knowledge_graph(self, podcast_config, episode, results, transcript_segments):\n        \"\"\"Save all extracted knowledge to Neo4j.\"\"\"\n        print(\"\\nSaving to knowledge graph...\")\n        \n        # Process segments in batches for memory efficiency\n        BATCH_SIZE = 20\n        \n        for batch_start in range(0, len(transcript_segments), BATCH_SIZE):\n            batch_end = min(batch_start + BATCH_SIZE, len(transcript_segments))\n            batch_segments = transcript_segments[batch_start:batch_end]\n            \n            # Calculate metrics for batch\n            batch_metrics = []\n            for segment in batch_segments:\n                segment_entities = [e for e in results['entities'] \n                                  if e['name'].lower() in segment['text'].lower()]\n                \n                # Calculate all metrics\n                complexity = classify_segment_complexity(segment['text'], segment_entities)\n                info_density = calculate_information_density(\n                    segment['text'], results['insights'], segment_entities\n                )\n                accessibility = calculate_accessibility_score(\n                    segment['text'], complexity['complexity_score']\n                )\n                \n                batch_metrics.append({\n                    'complexity': complexity,\n                    'info_density': info_density,\n                    'accessibility': accessibility\n                })\n            \n            # Save batch to Neo4j\n            save_segment_batch_to_neo4j(\n                self.neo4j_driver, episode, batch_segments, batch_start,\n                [m['complexity'] for m in batch_metrics],\n                [m['info_density'] for m in batch_metrics],\n                [m['accessibility'] for m in batch_metrics],\n                [], [],  # quotability and best_of calculated elsewhere\n                self.embedding_client\n            )\n            \n            # Clean up memory\n            del batch_metrics\n            cleanup_memory()\n        \n        # Save episode-level knowledge\n        save_episode_knowledge_to_neo4j(\n            podcast_config, episode, results['insights'], results['entities'],\n            self.neo4j_driver, self.embedding_client, \n            None, None, True, transcript_segments, self.task_router,\n            results.get('quotes'), None\n        )\n        \n        print(\"‚úì Knowledge saved to graph\")\n    \n    def run_pipeline(self, podcast_config, max_episodes=1, segmenter_config=None, \n                    use_large_context=True, enhance_graph=True):\n        \"\"\"\n        Run the complete pipeline for a podcast.\n        \"\"\"\n        print(\"üöÄ Starting Knowledge Graph Pipeline\")\n        print(f\"Configuration: max_episodes={max_episodes}, use_large_context={use_large_context}\")\n        \n        try:\n            # Initialize components\n            if not self.initialize_components(use_large_context):\n                raise Exception(\"Failed to initialize pipeline\")\n            \n            # Fetch podcast feed\n            print(f\"\\nFetching podcast feed: {podcast_config['name']}\")\n            podcast_info = fetch_podcast_feed(podcast_config, max_episodes)\n            \n            if not podcast_info or not podcast_info.get(\"episodes\"):\n                raise Exception(\"No episodes found to process\")\n            \n            # Process episodes\n            episodes = podcast_info[\"episodes\"]\n            results = []\n            \n            for i, episode in enumerate(episodes):\n                print(f\"\\n--- Episode {i+1}/{len(episodes)} ---\")\n                \n                result = self.process_episode(\n                    podcast_config, episode, segmenter_config, \n                    use_large_context=use_large_context\n                )\n                \n                if result:\n                    results.append(result)\n                \n                # Monitor memory\n                monitor_memory()\n            \n            # Apply graph enhancements if requested\n            if enhance_graph and results:\n                print(\"\\nüîß Applying knowledge graph enhancements...\")\n                self._enhance_knowledge_graph(podcast_info, results)\n            \n            # Generate final report\n            self._generate_final_report(results)\n            \n            return results\n            \n        finally:\n            self.cleanup()\n    \n    def _enhance_knowledge_graph(self, podcast_info, results):\n        \"\"\"Apply advanced graph algorithms and enhancements.\"\"\"\n        try:\n            # Extract relationships\n            print(\"Extracting relationship network...\")\n            relationship_count = extract_relationship_network(\n                self.neo4j_driver, self.task_router, podcast_info.get(\"id\")\n            )\n            print(f\"Extracted {relationship_count} relationships\")\n            \n            # Apply graph algorithms\n            print(\"Applying graph algorithms...\")\n            apply_graph_algorithms(self.neo4j_driver)\n            \n            # Implement semantic clustering\n            print(\"Implementing semantic clustering...\")\n            implement_semantic_clustering(self.neo4j_driver, self.task_router)\n            \n            print(\"‚úì Knowledge graph enhancements completed\")\n            \n        except Exception as e:\n            print(f\"Warning: Failed to apply some enhancements: {e}\")\n    \n    def _generate_final_report(self, results):\n        \"\"\"Generate and display final processing report.\"\"\"\n        print(\"\\n\" + \"=\"*50)\n        print(\"PIPELINE EXECUTION COMPLETE\")\n        print(\"=\"*50)\n        \n        # Collect and display statistics\n        stats = collect_knowledge_graph_stats(self.neo4j_driver)\n        if stats:\n            print(generate_knowledge_summary(stats))\n        \n        # Model usage report\n        usage_report = self.task_router.get_usage_report()\n        print(\"\\nModel Usage Report:\")\n        for model, stats in usage_report['model_status'].items():\n            print(f\"  {model}: {stats['requests_today']} requests\")\n        \n        # Validation report\n        validation_report = extraction_validator.get_validation_report()\n        if validation_report:\n            print(\"\\nValidation Report:\")\n            for stat, count in validation_report.items():\n                print(f\"  {stat}: {count}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 11.2: Pipeline Helper Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def save_segment_batch_to_neo4j(neo4j_driver, episode, batch_segments, batch_start,\n                                batch_complexity, batch_info_density, batch_accessibility,\n                                batch_quotability, batch_best_of, embedding_client):\n    \"\"\"\n    Save a batch of segments to Neo4j with their metrics.\n    Optimized for memory efficiency by processing in batches.\n    \"\"\"\n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            for i, segment in enumerate(batch_segments):\n                segment_idx = batch_start + i\n                segment_id = f\"segment_{episode['id']}_{segment_idx}\"\n                \n                # Generate embedding for segment\n                embedding = None\n                if embedding_client and len(segment['text']) > 50:\n                    try:\n                        embedding = generate_embeddings(segment['text'], embedding_client)\n                    except:\n                        pass  # Skip if embedding fails\n                \n                # Prepare segment data\n                segment_data = {\n                    \"id\": segment_id,\n                    \"episode_id\": episode['id'],\n                    \"segment_number\": segment_idx,\n                    \"text\": segment['text'][:5000],  # Limit text length\n                    \"start_time\": segment.get('start', 0),\n                    \"end_time\": segment.get('end', 0),\n                    \"speaker\": segment.get('speaker', 'Unknown'),\n                    \"embedding\": embedding\n                }\n                \n                # Add metrics if available\n                if i < len(batch_complexity):\n                    segment_data.update({\n                        \"complexity_score\": batch_complexity[i].get('complexity_score', 0),\n                        \"complexity_level\": batch_complexity[i].get('classification', 'unknown')\n                    })\n                \n                if i < len(batch_info_density):\n                    segment_data.update({\n                        \"info_density\": batch_info_density[i].get('information_score', 0),\n                        \"info_density_level\": batch_info_density[i].get('density_level', 'unknown')\n                    })\n                \n                if i < len(batch_accessibility):\n                    segment_data.update({\n                        \"accessibility_score\": batch_accessibility[i].get('accessibility_score', 0)\n                    })\n                \n                # Save segment\n                session.run(\"\"\"\n                MERGE (s:Segment {id: $id})\n                SET s.episode_id = $episode_id,\n                    s.segment_number = $segment_number,\n                    s.text = $text,\n                    s.start_time = $start_time,\n                    s.end_time = $end_time,\n                    s.speaker = $speaker,\n                    s.embedding = $embedding,\n                    s.complexity_score = coalesce($complexity_score, 0),\n                    s.complexity_level = coalesce($complexity_level, 'unknown'),\n                    s.info_density = coalesce($info_density, 0),\n                    s.info_density_level = coalesce($info_density_level, 'unknown'),\n                    s.accessibility_score = coalesce($accessibility_score, 0)\n                WITH s\n                MATCH (e:Episode {id: $episode_id})\n                MERGE (e)-[:HAS_SEGMENT]->(s)\n                \"\"\", segment_data)\n                \n    except Exception as e:\n        print(f\"Error saving segment batch: {e}\")\n\ndef save_episode_knowledge_to_neo4j(podcast_config, episode, insights, entities,\n                                  neo4j_driver, embedding_client, episode_complexity,\n                                  episode_metrics, use_large_context, transcript_segments,\n                                  task_router, quotes=None, topics=None):\n    \"\"\"\n    Save episode-level knowledge to Neo4j including podcast, episode, insights, and entities.\n    \"\"\"\n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            # Save podcast node\n            session.run(\"\"\"\n            MERGE (p:Podcast {id: $id})\n            SET p.name = $name,\n                p.author = $author,\n                p.description = $description,\n                p.language = $language,\n                p.categories = $categories,\n                p.website = $website,\n                p.feed_url = $feed_url,\n                p.image_url = $image_url\n            \"\"\", {\n                \"id\": podcast_config[\"id\"],\n                \"name\": podcast_config[\"name\"],\n                \"author\": podcast_config.get(\"author\", \"\"),\n                \"description\": podcast_config.get(\"description\", \"\"),\n                \"language\": podcast_config.get(\"language\", \"en\"),\n                \"categories\": podcast_config.get(\"categories\", []),\n                \"website\": podcast_config.get(\"website\", \"\"),\n                \"feed_url\": podcast_config[\"feed_url\"],\n                \"image_url\": podcast_config.get(\"image_url\", \"\")\n            })\n            \n            # Save episode node with metrics\n            episode_data = {\n                \"id\": episode[\"id\"],\n                \"podcast_id\": podcast_config[\"id\"],\n                \"title\": episode[\"title\"],\n                \"description\": episode.get(\"description\", \"\"),\n                \"published_date\": episode.get(\"published_date\", \"\"),\n                \"duration_seconds\": episode.get(\"duration_seconds\", 0),\n                \"audio_url\": episode.get(\"audio_url\", \"\"),\n                \"episode_number\": episode.get(\"episode_number\", 0),\n                \"season_number\": episode.get(\"season_number\", 0)\n            }\n            \n            # Add complexity metrics if available\n            if episode_complexity:\n                episode_data.update({\n                    \"complexity_level\": episode_complexity.get(\"dominant_level\", \"unknown\"),\n                    \"avg_complexity\": episode_complexity.get(\"average_complexity\", 0)\n                })\n            \n            if episode_metrics:\n                episode_data.update({\n                    \"avg_info_density\": episode_metrics.get(\"avg_info_density\", 0),\n                    \"avg_accessibility\": episode_metrics.get(\"avg_accessibility\", 0)\n                })\n            \n            session.run(\"\"\"\n            MERGE (e:Episode {id: $id})\n            SET e.podcast_id = $podcast_id,\n                e.title = $title,\n                e.description = $description,\n                e.published_date = $published_date,\n                e.duration_seconds = $duration_seconds,\n                e.audio_url = $audio_url,\n                e.episode_number = $episode_number,\n                e.season_number = $season_number,\n                e.complexity_level = coalesce($complexity_level, 'unknown'),\n                e.avg_complexity = coalesce($avg_complexity, 0),\n                e.avg_info_density = coalesce($avg_info_density, 0),\n                e.avg_accessibility = coalesce($avg_accessibility, 0),\n                e.processed_at = datetime()\n            WITH e\n            MATCH (p:Podcast {id: $podcast_id})\n            MERGE (p)-[:HAS_EPISODE]->(e)\n            \"\"\", episode_data)\n            \n            # Save insights\n            if insights:\n                print(f\"Saving {len(insights)} insights...\")\n                create_insight_nodes(session, insights, podcast_config, episode, \n                                   embedding_client, use_large_context)\n            \n            # Save entities with deduplication\n            if entities:\n                print(f\"Saving {len(entities)} entities...\")\n                create_entity_nodes(session, entities, podcast_config, episode, \n                                  embedding_client, use_large_context)\n            \n            # Extract and save topics if not provided\n            if topics is None and task_router:\n                print(\"Extracting episode topics...\")\n                full_transcript = convert_transcript_for_llm(transcript_segments)\n                \n                topic_prompt = f\"\"\"\n                Analyze this podcast episode and identify the main topics discussed.\n                \n                Episode: {episode['title']}\n                Transcript: {full_transcript[:50000]}\n                \n                Return a JSON list of topics with scores (0-1) indicating prominence:\n                [\n                  {{\"name\": \"topic name\", \"score\": 0.8, \"evidence\": \"brief explanation\"}},\n                  ...\n                ]\n                \n                Focus on 5-10 main topics. Be specific but not overly granular.\n                \"\"\"\n                \n                try:\n                    result = task_router.route_request('topics', topic_prompt)\n                    topics = json.loads(result['response'])\n                    \n                    # Save topics\n                    if topics:\n                        created_topics = create_topic_nodes(\n                            session, topics, episode['id'], podcast_config['id']\n                        )\n                        update_episode_with_topics(session, episode['id'], topics)\n                        print(f\"Created {len(created_topics)} topic relationships\")\n                except:\n                    print(\"Failed to extract topics\")\n            \n            # Save quotes if provided\n            if quotes:\n                print(f\"Saving {len(quotes)} key quotes...\")\n                for segment in transcript_segments:\n                    segment_quotes = [q for q in quotes \n                                    if q['text'] in segment['text']]\n                    if segment_quotes:\n                        segment_id = f\"segment_{episode['id']}_{segment['segment_number']}\"\n                        create_quote_nodes(\n                            session, segment_quotes, segment_id, \n                            episode['id'], embedding_client\n                        )\n            \n            # Create cross-references\n            if use_large_context and insights and entities:\n                print(\"Creating cross-references...\")\n                create_cross_references(session, entities, insights, \n                                      podcast_config, episode, use_large_context)\n            \n            print(\"‚úì Episode knowledge saved to Neo4j\")\n            \n    except Exception as e:\n        print(f\"Error saving episode knowledge: {e}\")\n        raise\n\ndef calculate_episode_metrics_from_db(neo4j_driver, episode_id):\n    \"\"\"\n    Calculate episode-level metrics by querying saved segments from database.\n    Used when processing in batches to avoid keeping all data in memory.\n    \"\"\"\n    database = os.environ.get(\"NEO4J_DATABASE\", \"neo4j\")\n    \n    try:\n        with neo4j_driver.session(database=database) as session:\n            # Get complexity metrics\n            complexity_result = session.run(\"\"\"\n            MATCH (e:Episode {id: $episode_id})-[:HAS_SEGMENT]->(s:Segment)\n            WITH s.complexity_level as level, \n                 avg(s.complexity_score) as avg_score,\n                 count(s) as count\n            RETURN level, avg_score, count\n            ORDER BY count DESC\n            \"\"\", {\"episode_id\": episode_id})\n            \n            complexity_data = list(complexity_result)\n            if complexity_data:\n                dominant_level = complexity_data[0]['level']\n                total_segments = sum(d['count'] for d in complexity_data)\n                avg_complexity = sum(d['avg_score'] * d['count'] for d in complexity_data) / total_segments\n                \n                episode_complexity = {\n                    'dominant_level': dominant_level,\n                    'average_complexity': avg_complexity,\n                    'distribution': {d['level']: d['count'] for d in complexity_data}\n                }\n            else:\n                episode_complexity = None\n            \n            # Get other metrics\n            metrics_result = session.run(\"\"\"\n            MATCH (e:Episode {id: $episode_id})-[:HAS_SEGMENT]->(s:Segment)\n            RETURN avg(s.info_density) as avg_info_density,\n                   avg(s.accessibility_score) as avg_accessibility,\n                   max(s.info_density) as max_info_density,\n                   min(s.accessibility_score) as min_accessibility\n            \"\"\", {\"episode_id\": episode_id})\n            \n            metrics = metrics_result.single()\n            episode_metrics = {\n                'avg_info_density': metrics['avg_info_density'] or 0,\n                'avg_accessibility': metrics['avg_accessibility'] or 0,\n                'max_info_density': metrics['max_info_density'] or 0,\n                'min_accessibility': metrics['min_accessibility'] or 100\n            }\n            \n            # Get key quotes\n            quotes_result = session.run(\"\"\"\n            MATCH (e:Episode {id: $episode_id})<-[:QUOTED_IN]-(q:Quote)\n            RETURN q.text as text,\n                   q.speaker as speaker,\n                   q.impact_score as score,\n                   q.quote_type as type\n            ORDER BY q.impact_score DESC\n            LIMIT 10\n            \"\"\", {\"episode_id\": episode_id})\n            \n            key_quotes = [dict(record) for record in quotes_result]\n            \n            return episode_complexity, episode_metrics, key_quotes\n            \n    except Exception as e:\n        print(f\"Error calculating episode metrics: {e}\")\n        return None, None, []\n\ndef run_simple_pipeline(podcast_url, max_episodes=1, use_large_context=True):\n    \"\"\"\n    Simplified pipeline runner for quick testing.\n    \n    Args:\n        podcast_url: RSS feed URL of the podcast\n        max_episodes: Number of episodes to process\n        use_large_context: Whether to use large context models\n        \n    Returns:\n        Processing results\n    \"\"\"\n    # Create podcast config from URL\n    podcast_config = {\n        \"id\": hashlib.md5(podcast_url.encode()).hexdigest()[:16],\n        \"name\": \"Podcast\",  # Will be updated from feed\n        \"feed_url\": podcast_url\n    }\n    \n    # Fetch feed to get podcast name\n    feed_data = feedparser.parse(podcast_url)\n    if feed_data.feed:\n        podcast_config[\"name\"] = feed_data.feed.get(\"title\", \"Unknown Podcast\")\n        podcast_config[\"author\"] = feed_data.feed.get(\"author\", \"\")\n        podcast_config[\"description\"] = feed_data.feed.get(\"description\", \"\")\n    \n    print(f\"Processing: {podcast_config['name']}\")\n    \n    # Create and run pipeline\n    pipeline = PodcastKnowledgePipeline()\n    \n    try:\n        results = pipeline.run_pipeline(\n            podcast_config,\n            max_episodes=max_episodes,\n            use_large_context=use_large_context,\n            enhance_graph=True\n        )\n        \n        return results\n        \n    except Exception as e:\n        print(f\"Pipeline error: {e}\")\n        return None",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 12: Batch Processing & Seeding",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 12.1: Batch Processing for Multiple Podcasts",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def seed_podcasts(podcast_configs, max_episodes_each=10, neo4j_config=None):\n    \"\"\"\n    Seed knowledge graph with multiple podcasts efficiently.\n    \n    Args:\n        podcast_configs: List of podcast configurations or single config dict\n        max_episodes_each: Episodes to process per podcast\n        neo4j_config: Override Neo4j configuration\n        \n    Returns:\n        Summary dict with processing statistics\n    \"\"\"\n    # Ensure podcast_configs is a list\n    if isinstance(podcast_configs, dict):\n        podcast_configs = [podcast_configs]\n    \n    # Initialize pipeline\n    pipeline = PodcastKnowledgePipeline()\n    \n    # Summary statistics\n    summary = {\n        'total_podcasts': len(podcast_configs),\n        'total_episodes': 0,\n        'successful_episodes': 0,\n        'failed_episodes': 0,\n        'total_segments': 0,\n        'total_insights': 0,\n        'total_entities': 0,\n        'start_time': datetime.now(),\n        'errors': []\n    }\n    \n    try:\n        # Initialize components once\n        if not pipeline.initialize_components(use_large_context=True):\n            raise Exception(\"Failed to initialize pipeline components\")\n        \n        # Process each podcast\n        for i, podcast_config in enumerate(podcast_configs):\n            try:\n                print(f\"\\n{'='*60}\")\n                print(f\"Processing Podcast {i+1}/{len(podcast_configs)}: {podcast_config['name']}\")\n                print(f\"{'='*60}\")\n                \n                # Ensure required fields\n                if 'feed_url' not in podcast_config and 'rss_url' in podcast_config:\n                    podcast_config['feed_url'] = podcast_config['rss_url']\n                \n                # Fetch and process episodes\n                podcast_info = fetch_podcast_feed(podcast_config, max_episodes_each)\n                \n                if not podcast_info or not podcast_info.get(\"episodes\"):\n                    print(f\"No episodes found for {podcast_config['name']}\")\n                    continue\n                \n                episodes = podcast_info[\"episodes\"]\n                podcast_results = []\n                \n                for j, episode in enumerate(episodes):\n                    try:\n                        print(f\"\\nEpisode {j+1}/{len(episodes)}: {episode['title']}\")\n                        \n                        result = pipeline.process_episode(\n                            podcast_config, \n                            episode, \n                            use_large_context=True\n                        )\n                        \n                        if result:\n                            podcast_results.append(result)\n                            summary['successful_episodes'] += 1\n                            summary['total_segments'] += len(result.get('segments', []))\n                            summary['total_insights'] += len(result.get('insights', []))\n                            summary['total_entities'] += len(result.get('entities', []))\n                        else:\n                            summary['failed_episodes'] += 1\n                            \n                    except Exception as e:\n                        print(f\"Error processing episode: {e}\")\n                        summary['failed_episodes'] += 1\n                        summary['errors'].append({\n                            'podcast': podcast_config['name'],\n                            'episode': episode['title'],\n                            'error': str(e)\n                        })\n                    \n                    # Clean up memory after each episode\n                    cleanup_memory()\n                \n                # Apply graph enhancements for this podcast\n                if podcast_results:\n                    print(f\"\\nApplying graph enhancements for {podcast_config['name']}...\")\n                    pipeline._enhance_knowledge_graph(podcast_info, podcast_results)\n                \n                summary['total_episodes'] += len(episodes)\n                \n            except Exception as e:\n                print(f\"Failed to process podcast {podcast_config['name']}: {e}\")\n                summary['errors'].append({\n                    'podcast': podcast_config['name'],\n                    'error': str(e)\n                })\n    \n    finally:\n        # Calculate duration\n        summary['end_time'] = datetime.now()\n        summary['duration_seconds'] = (summary['end_time'] - summary['start_time']).total_seconds()\n        \n        # Cleanup\n        pipeline.cleanup()\n        \n        # Print summary\n        print(f\"\\n{'='*60}\")\n        print(\"BATCH PROCESSING COMPLETE\")\n        print(f\"{'='*60}\")\n        print(f\"Total Podcasts: {summary['total_podcasts']}\")\n        print(f\"Total Episodes: {summary['total_episodes']}\")\n        print(f\"Successful: {summary['successful_episodes']}\")\n        print(f\"Failed: {summary['failed_episodes']}\")\n        print(f\"Total Insights: {summary['total_insights']}\")\n        print(f\"Total Entities: {summary['total_entities']}\")\n        print(f\"Duration: {summary['duration_seconds']/60:.1f} minutes\")\n        \n        if summary['errors']:\n            print(f\"\\nErrors ({len(summary['errors'])}):\")\n            for error in summary['errors'][:5]:  # Show first 5 errors\n                print(f\"  - {error}\")\n    \n    return summary\n\ndef seed_knowledge_graph_batch(rss_urls, max_episodes_each=10):\n    \"\"\"\n    Convenience function to seed knowledge graph from RSS URLs.\n    \n    Args:\n        rss_urls: List of RSS feed URLs or dict mapping names to URLs\n        max_episodes_each: Episodes to process per podcast\n        \n    Returns:\n        Summary dict with processing statistics\n        \n    Examples:\n        # From list of URLs\n        urls = [\n            \"https://feeds.example.com/podcast1.xml\",\n            \"https://feeds.example.com/podcast2.xml\"\n        ]\n        summary = seed_knowledge_graph_batch(urls, max_episodes_each=5)\n        \n        # From dict with names\n        podcasts = {\n            \"Tech Podcast\": \"https://feeds.example.com/tech.xml\",\n            \"Science Show\": \"https://feeds.example.com/science.xml\"\n        }\n        summary = seed_knowledge_graph_batch(podcasts, max_episodes_each=3)\n    \"\"\"\n    # Convert RSS URLs to podcast configs\n    podcast_configs = []\n    \n    if isinstance(rss_urls, dict):\n        # Dict format: {\"podcast_name\": \"rss_url\"}\n        for name, url in rss_urls.items():\n            podcast_configs.append({\n                \"id\": hashlib.md5(url.encode()).hexdigest()[:16],\n                \"name\": name,\n                \"feed_url\": url,\n                \"description\": f\"Podcast: {name}\"\n            })\n    else:\n        # List format: [\"url1\", \"url2\"]\n        for i, url in enumerate(rss_urls):\n            # Try to get name from feed\n            try:\n                feed = feedparser.parse(url)\n                name = feed.feed.get('title', f'Podcast {i+1}')\n            except:\n                name = f'Podcast {i+1}'\n                \n            podcast_configs.append({\n                \"id\": hashlib.md5(url.encode()).hexdigest()[:16],\n                \"name\": name,\n                \"feed_url\": url,\n                \"description\": f\"Podcast from {url}\"\n            })\n    \n    return seed_podcasts(podcast_configs, max_episodes_each)\n\ndef process_podcast_csv(csv_file_path, max_episodes_each=5):\n    \"\"\"\n    Process podcasts from a CSV file.\n    \n    CSV format:\n    name,rss_url,category\n    \"Podcast Name\",\"https://feed.url\",\"Technology\"\n    \n    Args:\n        csv_file_path: Path to CSV file\n        max_episodes_each: Episodes per podcast\n        \n    Returns:\n        Processing summary\n    \"\"\"\n    import csv\n    \n    podcast_configs = []\n    \n    with open(csv_file_path, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            if 'rss_url' in row or 'feed_url' in row:\n                url = row.get('rss_url') or row.get('feed_url')\n                name = row.get('name', 'Unknown Podcast')\n                \n                config = {\n                    \"id\": hashlib.md5(url.encode()).hexdigest()[:16],\n                    \"name\": name,\n                    \"feed_url\": url,\n                    \"description\": row.get('description', ''),\n                    \"category\": row.get('category', 'General')\n                }\n                \n                podcast_configs.append(config)\n    \n    print(f\"Loaded {len(podcast_configs)} podcasts from CSV\")\n    return seed_podcasts(podcast_configs, max_episodes_each)\n\ndef get_batch_progress():\n    \"\"\"\n    Get current batch processing progress from checkpoints.\n    \n    Returns:\n        Dict with progress information\n    \"\"\"\n    checkpoint = ProgressCheckpoint()\n    completed_episodes = checkpoint.get_completed_episodes()\n    \n    # Get more detailed progress from Neo4j\n    driver = connect_to_neo4j()\n    if driver:\n        try:\n            with driver.session() as session:\n                result = session.run(\"\"\"\n                MATCH (p:Podcast)-[:HAS_EPISODE]->(e:Episode)\n                WITH p.name as podcast, count(e) as episode_count\n                RETURN podcast, episode_count\n                ORDER BY episode_count DESC\n                \"\"\")\n                \n                podcast_progress = [dict(record) for record in result]\n                \n                # Get overall stats\n                stats_result = session.run(\"\"\"\n                MATCH (e:Episode)\n                WITH count(e) as total_episodes\n                MATCH (i:Insight)\n                WITH total_episodes, count(i) as total_insights\n                MATCH (n:Entity)\n                RETURN total_episodes, total_insights, count(n) as total_entities\n                \"\"\")\n                \n                overall_stats = stats_result.single()\n                \n                driver.close()\n                \n                return {\n                    'completed_episodes': list(completed_episodes),\n                    'podcast_progress': podcast_progress,\n                    'overall_stats': dict(overall_stats) if overall_stats else {}\n                }\n                \n        except Exception as e:\n            print(f\"Error getting progress: {e}\")\n            driver.close()\n    \n    return {\n        'completed_episodes': list(completed_episodes),\n        'podcast_progress': [],\n        'overall_stats': {}\n    }",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 13: Colab Integration",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 13.1: Colab-Specific Setup and Utilities",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def setup_colab_environment():\n    \"\"\"\n    Complete setup for Google Colab environment including GPU, Drive, and dependencies.\n    \"\"\"\n    print(\"Setting up Google Colab environment...\")\n    \n    # Check if running in Colab\n    try:\n        import google.colab\n        IN_COLAB = True\n    except ImportError:\n        IN_COLAB = False\n        print(\"Not running in Google Colab. Skipping Colab-specific setup.\")\n        return False\n    \n    # 1. Mount Google Drive for persistence\n    print(\"Mounting Google Drive...\")\n    from google.colab import drive\n    drive.mount('/content/drive')\n    \n    # 2. Check GPU availability\n    import tensorflow as tf\n    if tf.config.list_physical_devices('GPU'):\n        print(\"‚úì GPU available:\", tf.config.list_physical_devices('GPU')[0].name)\n    else:\n        print(\"‚ö† No GPU detected. Processing will be slower.\")\n    \n    # 3. Install required packages\n    print(\"\\nInstalling required packages...\")\n    packages = [\n        \"openai-whisper\",\n        \"neo4j\",\n        \"google-generativeai\",\n        \"openai\",\n        \"feedparser\",\n        \"pydub\",\n        \"python-dotenv\",\n        \"plotly\",\n        \"pyannote.audio\"\n    ]\n    \n    for package in packages:\n        os.system(f\"pip install -q {package}\")\n    \n    # 4. Create directories\n    print(\"\\nCreating working directories...\")\n    directories = [\n        \"/content/podcasts\",\n        \"/content/processed_podcasts\",\n        \"/content/drive/MyDrive/podcast_knowledge\",\n        \"/content/drive/MyDrive/podcast_knowledge/checkpoints\",\n        \"/content/drive/MyDrive/podcast_knowledge/exports\"\n    ]\n    \n    for directory in directories:\n        os.makedirs(directory, exist_ok=True)\n    \n    # 5. Setup environment variables\n    print(\"\\nSetting up environment variables...\")\n    setup_env_file = \"/content/drive/MyDrive/podcast_knowledge/.env\"\n    \n    if not os.path.exists(setup_env_file):\n        print(\"\\n‚ö† Environment file not found. Creating template...\")\n        env_template = \"\"\"# Podcast Knowledge Graph Environment Variables\n\n# Google Gemini API\nGOOGLE_API_KEY=your_gemini_api_key_here\n\n# OpenAI API (for embeddings)\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Neo4j Database\nNEO4J_URI=neo4j+s://your-neo4j-instance.databases.neo4j.io\nNEO4J_USERNAME=neo4j\nNEO4J_PASSWORD=your_password_here\nNEO4J_DATABASE=neo4j\n\n# Hugging Face (for pyannote speaker diarization)\nHUGGINGFACE_TOKEN=your_hf_token_here\n\"\"\"\n        with open(setup_env_file, 'w') as f:\n            f.write(env_template)\n        \n        print(f\"Created template at: {setup_env_file}\")\n        print(\"Please edit this file with your API keys before continuing.\")\n        return False\n    \n    # Load environment variables\n    load_dotenv(setup_env_file)\n    print(\"‚úì Environment variables loaded\")\n    \n    # 6. Test connections\n    print(\"\\nTesting connections...\")\n    \n    # Test Gemini\n    try:\n        import google.generativeai as genai\n        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n        model = genai.GenerativeModel('gemini-1.5-flash')\n        response = model.generate_content(\"Say 'API connected'\")\n        print(\"‚úì Gemini API connected\")\n    except Exception as e:\n        print(f\"‚úó Gemini API error: {e}\")\n    \n    # Test Neo4j\n    try:\n        driver = connect_to_neo4j()\n        if driver:\n            driver.close()\n            print(\"‚úì Neo4j connected\")\n        else:\n            print(\"‚úó Neo4j connection failed\")\n    except Exception as e:\n        print(f\"‚úó Neo4j error: {e}\")\n    \n    print(\"\\n‚úì Colab environment setup complete!\")\n    return True\n\ndef mount_drive_with_timeout(timeout=30):\n    \"\"\"Mount Google Drive with timeout to handle authentication issues.\"\"\"\n    import threading\n    from google.colab import drive\n    \n    def mount():\n        drive.mount('/content/drive', force_remount=True)\n    \n    thread = threading.Thread(target=mount)\n    thread.start()\n    thread.join(timeout)\n    \n    if thread.is_alive():\n        print(\"Drive mounting timed out. Please try manually.\")\n        return False\n    return True\n\ndef save_to_drive(data, filename, subfolder=\"exports\"):\n    \"\"\"\n    Save data to Google Drive for persistence.\n    \n    Args:\n        data: Data to save (dict, list, etc.)\n        filename: Name of file\n        subfolder: Subfolder in podcast_knowledge directory\n    \"\"\"\n    drive_path = f\"/content/drive/MyDrive/podcast_knowledge/{subfolder}\"\n    os.makedirs(drive_path, exist_ok=True)\n    \n    filepath = os.path.join(drive_path, filename)\n    \n    # Save based on file extension\n    if filename.endswith('.json'):\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2, default=str)\n    elif filename.endswith('.pkl'):\n        import pickle\n        with open(filepath, 'wb') as f:\n            pickle.dump(data, f)\n    else:\n        # Save as text\n        with open(filepath, 'w') as f:\n            f.write(str(data))\n    \n    print(f\"Saved to: {filepath}\")\n    return filepath\n\ndef load_from_drive(filename, subfolder=\"exports\"):\n    \"\"\"\n    Load data from Google Drive.\n    \n    Args:\n        filename: Name of file\n        subfolder: Subfolder in podcast_knowledge directory\n        \n    Returns:\n        Loaded data\n    \"\"\"\n    filepath = f\"/content/drive/MyDrive/podcast_knowledge/{subfolder}/{filename}\"\n    \n    if not os.path.exists(filepath):\n        print(f\"File not found: {filepath}\")\n        return None\n    \n    # Load based on file extension\n    if filename.endswith('.json'):\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    elif filename.endswith('.pkl'):\n        import pickle\n        with open(filepath, 'rb') as f:\n            return pickle.load(f)\n    else:\n        with open(filepath, 'r') as f:\n            return f.read()\n\ndef monitor_colab_resources():\n    \"\"\"Monitor and display Colab resource usage.\"\"\"\n    import psutil\n    \n    # CPU usage\n    cpu_percent = psutil.cpu_percent(interval=1)\n    \n    # Memory usage\n    memory = psutil.virtual_memory()\n    memory_used_gb = memory.used / (1024**3)\n    memory_total_gb = memory.total / (1024**3)\n    memory_percent = memory.percent\n    \n    # Disk usage\n    disk = psutil.disk_usage('/')\n    disk_used_gb = disk.used / (1024**3)\n    disk_total_gb = disk.total / (1024**3)\n    disk_percent = disk.percent\n    \n    # GPU usage (if available)\n    gpu_info = \"\"\n    try:\n        import subprocess\n        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', \n                               '--format=csv,noheader,nounits'], \n                              capture_output=True, text=True)\n        if result.returncode == 0:\n            gpu_util, gpu_mem_used, gpu_mem_total = result.stdout.strip().split(', ')\n            gpu_info = f\"\\nGPU: {gpu_util}% | Memory: {gpu_mem_used}MB / {gpu_mem_total}MB\"\n    except:\n        pass\n    \n    print(f\"\"\"\nResource Usage:\nCPU: {cpu_percent}%\nRAM: {memory_used_gb:.1f}GB / {memory_total_gb:.1f}GB ({memory_percent}%)\nDisk: {disk_used_gb:.1f}GB / {disk_total_gb:.1f}GB ({disk_percent}%)\n{gpu_info}\n\"\"\")\n\ndef create_colab_widgets():\n    \"\"\"Create interactive widgets for Colab notebook.\"\"\"\n    from IPython.display import display, HTML\n    import ipywidgets as widgets\n    \n    # Podcast URL input\n    url_input = widgets.Text(\n        value='',\n        placeholder='Enter podcast RSS feed URL',\n        description='Podcast URL:',\n        style={'description_width': 'initial'},\n        layout=widgets.Layout(width='500px')\n    )\n    \n    # Episodes slider\n    episodes_slider = widgets.IntSlider(\n        value=3,\n        min=1,\n        max=20,\n        step=1,\n        description='Episodes:',\n        style={'description_width': 'initial'}\n    )\n    \n    # Process button\n    process_button = widgets.Button(\n        description='Process Podcast',\n        button_style='success',\n        icon='play'\n    )\n    \n    # Output area\n    output = widgets.Output()\n    \n    def on_process_click(b):\n        with output:\n            output.clear_output()\n            if url_input.value:\n                print(f\"Processing: {url_input.value}\")\n                print(f\"Episodes: {episodes_slider.value}\")\n                \n                # Run pipeline\n                results = run_simple_pipeline(\n                    url_input.value, \n                    max_episodes=episodes_slider.value\n                )\n                \n                if results:\n                    print(\"\\n‚úì Processing complete!\")\n                    # Save results\n                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n                    save_to_drive(results, f\"results_{timestamp}.json\")\n            else:\n                print(\"Please enter a podcast URL\")\n    \n    process_button.on_click(on_process_click)\n    \n    # Display widgets\n    display(HTML(\"<h3>Podcast Knowledge Extractor</h3>\"))\n    display(widgets.VBox([\n        url_input,\n        episodes_slider,\n        process_button,\n        output\n    ]))\n\ndef setup_colab_neo4j_browser():\n    \"\"\"Setup Neo4j browser access from Colab.\"\"\"\n    neo4j_uri = os.getenv(\"NEO4J_URI\", \"\")\n    \n    if neo4j_uri:\n        # Extract browser URL from connection URI\n        browser_url = neo4j_uri.replace(\"neo4j+s://\", \"https://\").replace(\"neo4j://\", \"http://\")\n        browser_url = browser_url.split(\".databases.neo4j.io\")[0] + \".databases.neo4j.io\"\n        \n        from IPython.display import IFrame, display, HTML\n        \n        display(HTML(f\"\"\"\n        <h3>Neo4j Browser Access</h3>\n        <p>Open Neo4j Browser in a new tab: <a href=\"{browser_url}\" target=\"_blank\">{browser_url}</a></p>\n        <p>Username: {os.getenv(\"NEO4J_USERNAME\", \"neo4j\")}</p>\n        <p>Use the password from your .env file</p>\n        \"\"\"))\n        \n        # Useful Cypher queries\n        display(HTML(\"\"\"\n        <h4>Useful Queries:</h4>\n        <pre>\n// View schema\nCALL db.schema.visualization()\n\n// Count nodes by type\nMATCH (n)\nRETURN labels(n)[0] as type, count(n) as count\nORDER BY count DESC\n\n// Find top entities\nMATCH (e:Entity)\nRETURN e.name, e.type, e.pagerank\nORDER BY e.pagerank DESC\nLIMIT 20\n\n// View episode insights\nMATCH (ep:Episode)-[:HAS_INSIGHT]->(i:Insight)\nWHERE ep.title CONTAINS 'your search'\nRETURN ep.title, i.title, i.description\n        </pre>\n        \"\"\"))\n\n# Auto-setup when cell is run\nif 'google.colab' in str(get_ipython()):\n    print(\"üöÄ Google Colab detected! Run setup_colab_environment() to initialize.\")\nelse:\n    print(\"üìù Not running in Colab. Local environment assumed.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Section 14: Usage Examples",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 14.1: Single Podcast Processing Example",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 1: Process a single podcast\npodcast_url = \"https://example.com/podcast/feed.xml\"  # Replace with actual RSS feed URL\n\n# Option A: Using the simple pipeline runner\nresults = run_simple_pipeline(\n    podcast_url,\n    max_episodes=3,\n    use_large_context=True\n)\n\n# Option B: Using the full pipeline with custom configuration\npodcast_config = {\n    \"id\": \"my-podcast\",\n    \"name\": \"My Favorite Podcast\",\n    \"feed_url\": podcast_url,\n    \"description\": \"A great podcast about technology\"\n}\n\npipeline = PodcastKnowledgePipeline()\nresults = pipeline.run_pipeline(\n    podcast_config,\n    max_episodes=5,\n    use_large_context=True,\n    enhance_graph=True\n)\n\n# View results\nif results:\n    print(f\"\\nProcessed {len(results)} episodes\")\n    print(f\"Total insights: {sum(len(r.get('insights', [])) for r in results)}\")\n    print(f\"Total entities: {sum(len(r.get('entities', [])) for r in results)}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 14.2: Batch Processing Multiple Podcasts",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 13.2: Visual Progress Display",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def display_progress_notebook(current, total, message=\"Processing\"):\n    \"\"\"\n    Display progress in Colab/Jupyter notebook with HTML progress bar.\n    \n    Args:\n        current: Current progress value\n        total: Total value\n        message: Progress message to display\n    \"\"\"\n    if 'IPython' in sys.modules:\n        from IPython.display import display, HTML, clear_output\n        \n        progress_percent = (current / total) * 100 if total > 0 else 0\n        bar_length = 50\n        filled_length = int(bar_length * current / total) if total > 0 else 0\n        \n        html = f\"\"\"\n        <div style=\"margin: 10px 0;\">\n            <div style=\"font-weight: bold; margin-bottom: 5px;\">\n                {message}: {current}/{total} ({progress_percent:.1f}%)\n            </div>\n            <div style=\"background-color: #f0f0f0; border-radius: 5px; overflow: hidden;\">\n                <div style=\"background-color: #4CAF50; width: {progress_percent}%; \n                            padding: 5px 0; border-radius: 5px; text-align: center; \n                            color: white; font-weight: bold; min-height: 20px;\">\n                    {progress_percent:.0f}%\n                </div>\n            </div>\n        </div>\n        \"\"\"\n        \n        clear_output(wait=True)\n        display(HTML(html))\n    else:\n        # Fallback to text progress\n        print(f\"{message}: {current}/{total} ({(current/total)*100:.1f}%)\")\n\ndef display_rate_limit_countdown(wait_seconds):\n    \"\"\"\n    Display visual countdown for rate limit waits in notebooks.\n    \n    Args:\n        wait_seconds: Number of seconds to wait\n    \"\"\"\n    if COLAB_MODE and 'IPython' in sys.modules:\n        from IPython.display import clear_output\n        import time\n        \n        for remaining in range(wait_seconds, 0, -1):\n            clear_output(wait=True)\n            print(f\"‚è≥ Rate limit cooldown: {remaining} seconds remaining...\")\n            \n            # Create visual progress bar\n            bar_length = min(50, remaining)\n            bar = '‚ñà' * bar_length\n            print(bar)\n            \n            time.sleep(1)\n            \n        clear_output(wait=True)\n        print(\"‚úÖ Ready to continue!\")\n    else:\n        # Simple wait without visual feedback\n        time.sleep(wait_seconds)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example 2: Batch process multiple podcasts\n\n# Method 1: From a dictionary of names and URLs\npodcasts = {\n    \"Tech Podcast\": \"https://example.com/tech/feed.xml\",\n    \"Science Show\": \"https://example.com/science/feed.xml\",\n    \"History Hour\": \"https://example.com/history/feed.xml\"\n}\n\nsummary = seed_knowledge_graph_batch(\n    podcasts,\n    max_episodes_each=3\n)\n\n# Method 2: From a list of URLs\npodcast_urls = [\n    \"https://example.com/podcast1/feed.xml\",\n    \"https://example.com/podcast2/feed.xml\",\n    \"https://example.com/podcast3/feed.xml\"\n]\n\nsummary = seed_knowledge_graph_batch(\n    podcast_urls,\n    max_episodes_each=5\n)\n\n# Method 3: From a CSV file\n# Create a sample CSV first\nsample_csv_content = \"\"\"name,rss_url,category\n\"The Tech Talk\",\"https://example.com/tech/feed.xml\",\"Technology\"\n\"Science Weekly\",\"https://example.com/science/feed.xml\",\"Science\"\n\"History Decoded\",\"https://example.com/history/feed.xml\",\"History\"\n\"\"\"\n\nwith open('podcasts.csv', 'w') as f:\n    f.write(sample_csv_content)\n\n# Process from CSV\nsummary = process_podcast_csv('podcasts.csv', max_episodes_each=3)\n\n# View batch processing results\nprint(f\"\\nBatch Processing Summary:\")\nprint(f\"Total Podcasts: {summary['total_podcasts']}\")\nprint(f\"Successful Episodes: {summary['successful_episodes']}/{summary['total_episodes']}\")\nprint(f\"Total Insights: {summary['total_insights']}\")\nprint(f\"Total Entities: {summary['total_entities']}\")\nprint(f\"Processing Time: {summary['duration_seconds']/60:.1f} minutes\")\n\nif summary['errors']:\n    print(f\"\\nErrors encountered: {len(summary['errors'])}\")\n    for error in summary['errors'][:3]:\n        print(f\"  - {error}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 14.3: Visualizing and Analyzing Results",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 3: Visualize and analyze the knowledge graph\n\n# Initialize Neo4j connection\ndriver = connect_to_neo4j()\n\nif driver:\n    # 1. Visualize overall statistics\n    visualize_knowledge_graph_stats(driver)\n    \n    # 2. View semantic clusters\n    visualize_semantic_clusters(driver)\n    \n    # 3. Analyze temporal patterns\n    visualize_temporal_patterns(driver)\n    \n    # 4. Create insight dashboard for a specific episode\n    # First, get an episode ID\n    with driver.session() as session:\n        result = session.run(\"\"\"\n        MATCH (e:Episode)\n        RETURN e.id as id, e.title as title\n        ORDER BY e.processed_at DESC\n        LIMIT 1\n        \"\"\")\n        \n        if result.peek():\n            episode = result.single()\n            print(f\"\\nCreating dashboard for: {episode['title']}\")\n            create_insight_dashboard(driver, episode['id'])\n    \n    # 5. Get influential entities\n    print(\"\\n=== Most Influential Entities ===\")\n    entities = get_influential_entities(driver, limit=10)\n    for i, entity in enumerate(entities, 1):\n        print(f\"{i}. {entity['name']} ({entity['type']})\")\n        print(f\"   PageRank: {entity['pagerank']:.4f}\")\n        print(f\"   Connections: {entity['connections']}\")\n        if entity.get('description'):\n            print(f\"   Description: {entity['description'][:100]}...\")\n        print()\n    \n    # 6. Find semantic clusters\n    print(\"\\n=== Semantic Clusters ===\")\n    clusters = get_semantic_clusters(driver)\n    for cluster in clusters[:5]:\n        print(f\"\\nCluster: {cluster['name']} (Size: {cluster['size']})\")\n        print(\"Sample members:\")\n        for member in cluster['sample_members'][:5]:\n            print(f\"  - {member['name']} ({member['type']})\")\n    \n    # 7. Analyze knowledge paths\n    print(\"\\n=== Knowledge Paths ===\")\n    # Find paths from a major entity\n    if entities:\n        start_entity = entities[0]['name']\n        print(f\"Paths from '{start_entity}':\")\n        paths = analyze_knowledge_paths(driver, start_entity, max_length=3)\n        \n        for i, path in enumerate(paths[:3], 1):\n            print(f\"\\nPath {i} (Length: {path['length']}):\")\n            for node in path['path']:\n                print(f\"  ‚Üí {node['name']} ({node['type']})\")\n            if path.get('end_entity'):\n                print(f\"  Importance: {path['importance']:.4f}\")\n    \n    # 8. Export comprehensive metrics\n    export_graph_metrics(driver, 'knowledge_graph_metrics.json')\n    \n    # Close driver\n    driver.close()\nelse:\n    print(\"Could not connect to Neo4j\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 14.4: Querying the Knowledge Graph",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 4: Query the knowledge graph with Cypher\n\ndriver = connect_to_neo4j()\n\nif driver:\n    with driver.session() as session:\n        \n        # Query 1: Find episodes discussing a specific topic\n        print(\"=== Episodes about AI ===\")\n        result = session.run(\"\"\"\n        MATCH (e:Episode)-[:HAS_SEGMENT]->(s:Segment)\n        WHERE toLower(s.text) CONTAINS 'artificial intelligence' \n           OR toLower(s.text) CONTAINS ' ai '\n        RETURN DISTINCT e.title as episode, e.published_date as date\n        ORDER BY e.published_date DESC\n        LIMIT 5\n        \"\"\")\n        \n        for record in result:\n            print(f\"- {record['episode']} ({record['date']})\")\n        \n        # Query 2: Find insights about a specific entity\n        print(\"\\n=== Insights about Machine Learning ===\")\n        result = session.run(\"\"\"\n        MATCH (e:Entity)-[:RELATED_TO]->(i:Insight)\n        WHERE toLower(e.name) CONTAINS 'machine learning'\n        RETURN i.title as insight, i.insight_type as type, i.confidence as conf\n        ORDER BY i.confidence DESC\n        LIMIT 5\n        \"\"\")\n        \n        for record in result:\n            print(f\"- [{record['type']}] {record['insight']} (conf: {record['conf']:.2f})\")\n        \n        # Query 3: Find connected concepts\n        print(\"\\n=== Concepts connected to 'Data Science' ===\")\n        result = session.run(\"\"\"\n        MATCH (e1:Entity {name: 'Data Science'})-[r]-(e2:Entity)\n        WHERE type(r) IN ['MENTIONED_WITH', 'SEMANTIC_SIMILARITY', 'KEY_CONNECTION']\n        RETURN DISTINCT e2.name as connected_entity, type(r) as relationship\n        LIMIT 10\n        \"\"\")\n        \n        for record in result:\n            print(f\"- {record['connected_entity']} ({record['relationship']})\")\n        \n        # Query 4: Episode complexity analysis\n        print(\"\\n=== Episode Complexity Distribution ===\")\n        result = session.run(\"\"\"\n        MATCH (e:Episode)\n        WHERE e.complexity_level IS NOT NULL\n        RETURN e.complexity_level as level, count(e) as count\n        ORDER BY count DESC\n        \"\"\")\n        \n        for record in result:\n            print(f\"- {record['level']}: {record['count']} episodes\")\n        \n        # Query 5: Most quoted speakers\n        print(\"\\n=== Most Quoted Speakers ===\")\n        result = session.run(\"\"\"\n        MATCH (q:Quote)\n        WHERE q.speaker IS NOT NULL AND q.speaker <> 'Unknown'\n        RETURN q.speaker as speaker, count(q) as quote_count, avg(q.impact_score) as avg_impact\n        ORDER BY quote_count DESC\n        LIMIT 5\n        \"\"\")\n        \n        for record in result:\n            print(f\"- {record['speaker']}: {record['quote_count']} quotes (avg impact: {record['avg_impact']:.2f})\")\n        \n        # Query 6: Topic evolution over time\n        print(\"\\n=== Topic Evolution ===\")\n        result = session.run(\"\"\"\n        MATCH (ep1:Episode)-[r:TOPIC_EVOLUTION]->(ep2:Episode)\n        WHERE r.entity IS NOT NULL\n        RETURN r.entity as topic, r.relation_type as evolution_type, \n               ep1.title as from_episode, ep2.title as to_episode\n        LIMIT 5\n        \"\"\")\n        \n        for record in result:\n            print(f\"- '{record['topic']}' {record['evolution_type']}:\")\n            print(f\"  From: {record['from_episode']}\")\n            print(f\"  To: {record['to_episode']}\")\n        \n        # Query 7: Knowledge graph summary\n        print(\"\\n=== Knowledge Graph Summary ===\")\n        result = session.run(\"\"\"\n        MATCH (n)\n        WITH count(n) as total_nodes\n        MATCH ()-[r]->()\n        WITH total_nodes, count(r) as total_relationships\n        MATCH (p:Podcast)\n        WITH total_nodes, total_relationships, count(p) as podcast_count\n        MATCH (e:Episode)\n        WITH total_nodes, total_relationships, podcast_count, count(e) as episode_count\n        MATCH (i:Insight)\n        WITH total_nodes, total_relationships, podcast_count, episode_count, count(i) as insight_count\n        MATCH (ent:Entity)\n        RETURN total_nodes, total_relationships, podcast_count, episode_count, \n               insight_count, count(ent) as entity_count\n        \"\"\")\n        \n        summary = result.single()\n        if summary:\n            print(f\"Total Nodes: {summary['total_nodes']:,}\")\n            print(f\"Total Relationships: {summary['total_relationships']:,}\")\n            print(f\"Podcasts: {summary['podcast_count']}\")\n            print(f\"Episodes: {summary['episode_count']}\")\n            print(f\"Insights: {summary['insight_count']}\")\n            print(f\"Entities: {summary['entity_count']}\")\n    \n    driver.close()\nelse:\n    print(\"Could not connect to Neo4j\")\n\n# Example of using the data for RAG (Retrieval Augmented Generation)\ndef query_knowledge_for_rag(question, driver):\n    \"\"\"\n    Query the knowledge graph to provide context for answering questions.\n    \"\"\"\n    with driver.session() as session:\n        # Search for relevant insights\n        insights_result = session.run(\"\"\"\n        MATCH (i:Insight)\n        WHERE toLower(i.title) CONTAINS toLower($query)\n           OR toLower(i.description) CONTAINS toLower($query)\n        RETURN i.title as title, i.description as description, i.confidence as confidence\n        ORDER BY i.confidence DESC\n        LIMIT 5\n        \"\"\", {\"query\": question})\n        \n        insights = [dict(record) for record in insights_result]\n        \n        # Search for relevant segments\n        segments_result = session.run(\"\"\"\n        MATCH (s:Segment)\n        WHERE toLower(s.text) CONTAINS toLower($query)\n        RETURN s.text as text, s.speaker as speaker\n        LIMIT 3\n        \"\"\", {\"query\": question})\n        \n        segments = [dict(record) for record in segments_result]\n        \n        return {\n            \"insights\": insights,\n            \"segments\": segments\n        }\n\n# Example usage\n# context = query_knowledge_for_rag(\"What was said about machine learning?\", driver)\n# Use this context with an LLM to generate informed answers",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "\"\"\"\nThis notebook contains a complete production-ready podcast knowledge extraction system with the following capabilities:\n\nüéØ CORE FEATURES:\n1. Audio Processing - Transcription with Whisper, speaker diarization, intelligent segmentation\n2. Knowledge Extraction - Insights, entities, relationships, quotes, and topics using Gemini 1.5\n3. Advanced Analytics - Complexity scoring, information density, accessibility, quotability\n4. Knowledge Graph - Neo4j storage with semantic relationships and graph algorithms\n5. Visualization - Interactive dashboards with Plotly for insights and patterns\n\nüöÄ KEY IMPROVEMENTS:\n- Multi-model rate limiting with automatic fallback\n- Memory-efficient batch processing\n- Checkpoint/resume for long-running processes\n- Entity deduplication and resolution\n- 1M token context window optimization\n- PageRank and semantic clustering\n- Cross-episode relationship tracking\n- Temporal pattern analysis\n\nüìä SECTIONS OVERVIEW:\n1. Configuration & Setup - Environment setup, API keys, Neo4j connection\n2. Core Infrastructure - Rate limiting, memory management, checkpointing\n3. Audio Processing - Transcription, diarization, segmentation\n4. Knowledge Extraction - LLM prompts, parsing, validation\n5. Entity & Relationship - Entity resolution, relationship extraction\n6. Neo4j Integration - Schema, data persistence, querying\n7. Complexity Analysis - Content complexity, information density\n8. Advanced Analytics - Quotability, temporal patterns, aggregation\n9. Graph Algorithms - PageRank, community detection, clustering\n10. Visualization - Interactive charts and dashboards\n11. Pipeline Orchestration - End-to-end processing workflow\n12. Batch Processing - Multi-podcast seeding capabilities\n13. Colab Integration - Google Colab specific features\n14. Usage Examples - Complete working examples\n\nüéÆ QUICK START:\n1. Set up environment variables (API keys, Neo4j credentials)\n2. Run setup_colab_environment() if using Google Colab\n3. Process a single podcast: run_simple_pipeline(rss_url, max_episodes=3)\n4. Batch process: seed_knowledge_graph_batch(podcast_dict, max_episodes_each=5)\n5. Visualize results: visualize_knowledge_graph_stats(driver)\n\nüí° USE CASES:\n- Podcast content analysis and summarization\n- Knowledge graph construction for RAG systems\n- Content recommendation and discovery\n- Research and educational insights\n- Trend analysis across podcast episodes\n- Speaker influence and topic evolution tracking\n\nThis notebook represents a fully-featured implementation suitable for production use,\nwith robust error handling, scalability features, and comprehensive analytics.\n\"\"\"\n\nprint(\"‚úÖ Notebook loaded successfully!\")\nprint(\"üìö Total cells: 92\")\nprint(\"üîß All functionality from podcast_knowledge_system_enhanced.py has been implemented\")\nprint(\"\\nüöÄ Ready to process podcasts! Start with the examples in Section 14.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 9.1: PageRank & Influence Analysis\n\n**What this does:**\n- Calculates PageRank scores for entities and insights\n- Identifies the most influential concepts in your knowledge graph\n- Finds central ideas that connect many other concepts\n\n**Use this to:**\n- Find the most important entities across episodes\n- Identify key insights that influence many others\n- Discover central themes in your podcast collection",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Complete Error Handling Classes\nclass PodcastProcessingError(Exception):\n    \"\"\"Base exception for podcast processing errors.\"\"\"\n    pass\n\nclass DatabaseConnectionError(PodcastProcessingError):\n    \"\"\"Raised when Neo4j connection fails.\"\"\"\n    pass\n\nclass AudioProcessingError(PodcastProcessingError):\n    \"\"\"Raised when audio transcription or diarization fails.\"\"\"\n    pass\n\nclass LLMProcessingError(PodcastProcessingError):\n    \"\"\"Raised when LLM processing fails.\"\"\"\n    pass\n\nclass ConfigurationError(PodcastProcessingError):\n    \"\"\"Raised when configuration is invalid.\"\"\"\n    pass\n\nclass CheckpointError(PodcastProcessingError):\n    \"\"\"Raised when checkpoint operations fail.\"\"\"\n    pass\n\nclass RateLimitError(PodcastProcessingError):\n    \"\"\"Raised when API rate limits are exceeded.\"\"\"\n    pass\n\n# Neo4j Connection Manager with Enhanced Features\nclass Neo4jManager:\n    \"\"\"Context manager for Neo4j connections with retry logic.\"\"\"\n    \n    def __init__(self, config=None):\n        self.config = config or PodcastConfig\n        self.driver = None\n        self.retry_count = 0\n        self.max_retries = 3\n        \n    def __enter__(self):\n        try:\n            self.driver = GraphDatabase.driver(\n                self.config.NEO4J_URI,\n                auth=(self.config.NEO4J_USERNAME, self.config.NEO4J_PASSWORD)\n            )\n            \n            # Verify connection with retry\n            while self.retry_count < self.max_retries:\n                try:\n                    with self.driver.session(database=self.config.NEO4J_DATABASE) as session:\n                        result = session.run(\"RETURN 'Connected!' AS result\")\n                        message = result.single()[\"result\"]\n                        print(f\"‚úÖ Neo4j connection: {message}\")\n                        break\n                except Exception as e:\n                    self.retry_count += 1\n                    if self.retry_count >= self.max_retries:\n                        raise\n                    print(f\"‚ö†Ô∏è Connection attempt {self.retry_count} failed, retrying...\")\n                    time.sleep(2 ** self.retry_count)  # Exponential backoff\n                    \n            return self.driver\n            \n        except Exception as e:\n            raise DatabaseConnectionError(f\"Failed to connect to Neo4j: {e}\")\n            \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.driver:\n            try:\n                self.driver.close()\n                print(\"‚úÖ Neo4j connection closed\")\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Warning: Error closing Neo4j connection: {e}\")\n\n# Enhanced Memory Management\ndef cleanup_memory(force=False):\n    \"\"\"Enhanced memory cleanup for Colab and large processing jobs.\"\"\"\n    if psutil:\n        # Get memory usage before cleanup\n        process = psutil.Process()\n        mem_before = process.memory_info().rss / 1024 / 1024\n        \n    # Standard cleanup\n    gc.collect()\n    \n    # GPU cleanup\n    if torch and torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n    \n    # Matplotlib cleanup\n    if plt:\n        plt.close('all')\n    \n    # Force cleanup if memory usage is high\n    if force or (psutil and mem_before > PodcastConfig.MEMORY_THRESHOLD_MB):\n        # Clear module caches\n        if hasattr(sys, 'modules'):\n            modules_to_clear = ['transformers', 'whisper', 'pyannote']\n            for module in modules_to_clear:\n                if module in sys.modules:\n                    del sys.modules[module]\n        \n        # Additional aggressive cleanup\n        gc.collect(2)  # Full collection\n        \n    # Log memory freed\n    if psutil:\n        mem_after = process.memory_info().rss / 1024 / 1024\n        if mem_before - mem_after > 100:  # If freed more than 100MB\n            print(f\"üíæ Memory cleanup freed {mem_before - mem_after:.0f}MB\")\n\ndef monitor_memory():\n    \"\"\"Monitor current memory usage with enhanced metrics.\"\"\"\n    try:\n        if psutil:\n            memory = psutil.virtual_memory()\n            print(f\"üíæ RAM: {memory.percent:.1f}% ({memory.used // (1024**3):.1f}GB / {memory.total // (1024**3):.1f}GB)\")\n            \n            # Process-specific memory\n            process = psutil.Process()\n            process_memory = process.memory_info().rss / (1024**3)\n            print(f\"üìä Process memory: {process_memory:.1f}GB\")\n            \n        # GPU memory monitoring\n        if torch and torch.cuda.is_available():\n            gpu_memory = torch.cuda.memory_allocated() / (1024**3)\n            gpu_reserved = torch.cuda.memory_reserved() / (1024**3)\n            gpu_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n            print(f\"üéÆ GPU: {gpu_memory:.1f}GB used, {gpu_reserved:.1f}GB reserved / {gpu_total:.1f}GB total\")\n            \n            # GPU utilization\n            if torch.cuda.is_available():\n                print(f\"‚ö° GPU utilization: {torch.cuda.utilization()}%\")\n                \n    except Exception as e:\n        print(f\"Error monitoring memory: {e}\")\n\nprint(\"‚úÖ Core error handling and resource management loaded\")\nprint(\"  ‚Ä¢ Custom exceptions for better debugging\")\nprint(\"  ‚Ä¢ Safe database connection management\") \nprint(\"  ‚Ä¢ Memory cleanup and monitoring\")\n\n# Test memory monitoring\nmonitor_memory()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 4Ô∏è‚É£ Core Components Setup [REQUIRED]\n\n## What are Core Components?\n\nThese are the building blocks that make everything work:\n- **Error Handling**: Graceful error management\n- **Memory Management**: Keeps your notebook from crashing\n- **Database Manager**: Handles Neo4j connections safely\n- **Rate Limiting**: Prevents hitting API limits\n\n**This section is technical but required** - just run the cells!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Custom Error Classes\nclass PodcastProcessingError(Exception):\n    \"\"\"Base error for podcast processing.\"\"\"\n    pass\n\nclass DatabaseConnectionError(PodcastProcessingError):\n    \"\"\"When Neo4j connection fails.\"\"\"\n    pass\n\nclass AudioProcessingError(PodcastProcessingError):\n    \"\"\"When audio transcription fails.\"\"\"\n    pass\n\nclass LLMProcessingError(PodcastProcessingError):\n    \"\"\"When AI processing fails.\"\"\"\n    pass\n\n# Neo4j Connection Manager\nclass Neo4jManager:\n    \"\"\"Safely manages database connections.\"\"\"\n    \n    def __init__(self, config=None):\n        self.config = config or PodcastConfig\n        self.driver = None\n        \n    def __enter__(self):\n        try:\n            self.driver = GraphDatabase.driver(\n                self.config.NEO4J_URI,\n                auth=(self.config.NEO4J_USERNAME, self.config.NEO4J_PASSWORD)\n            )\n            return self.driver\n        except Exception as e:\n            raise DatabaseConnectionError(f\"Failed to connect: {e}\")\n            \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.driver:\n            self.driver.close()\n\n# Memory Cleanup Function\ndef cleanup_memory():\n    \"\"\"Frees up memory to prevent crashes.\"\"\"\n    gc.collect()\n    if torch and torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    if 'plt' in globals():\n        plt.close('all')\n    gc.collect()\n\nprint(\"‚úÖ Core components loaded!\")\nprint(\"  ‚Ä¢ Error handling ready\")\nprint(\"  ‚Ä¢ Database manager ready\")\nprint(\"  ‚Ä¢ Memory management ready\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 4.1: API Rate Limiting [REQUIRED]\n\n**What this does:**\n- Prevents hitting API rate limits (like speed limits for API calls)\n- Automatically switches between AI models if one is busy\n- Tracks your API usage\n\n**Why you need it:**\n- Google's Gemini API has limits on how fast you can make requests\n- This ensures your processing doesn't get blocked\n- Saves you from errors and retries\n\n**You don't need to modify this** - it works automatically!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from collections import deque\nimport time\n\nclass HybridRateLimiter:\n    \"\"\"Smart rate limiter for AI models.\"\"\"\n    \n    def __init__(self):\n        # Rate limits for different models\n        self.limits = {\n            'gemini-1.5-flash': {\n                'rpm': 15,      # Requests per minute\n                'tpm': 1000000, # Tokens per minute  \n                'rpd': 1500     # Requests per day\n            },\n            'gemini-1.5-pro': {\n                'rpm': 10,\n                'tpm': 250000,\n                'rpd': 500\n            }\n        }\n        \n        # Track usage per model\n        self.requests = {\n            'gemini-1.5-flash': {\n                'minute': deque(),\n                'day': deque(),\n                'tokens_minute': deque()\n            },\n            'gemini-1.5-pro': {\n                'minute': deque(),\n                'day': deque(),\n                'tokens_minute': deque()\n            }\n        }\n        \n        self.error_counts = defaultdict(int)\n        self.current_model = 'gemini-1.5-flash'\n        \n    def can_use_model(self, model_name, estimated_tokens=0):\n        \"\"\"Check if we can use a model without hitting limits.\"\"\"\n        current_time = time.time()\n        \n        if model_name not in self.limits:\n            return False\n            \n        limits = self.limits[model_name]\n        usage = self.requests[model_name]\n        \n        # Clean old entries\n        self._clean_old_entries(usage, current_time)\n        \n        # Check limits\n        rpm_count = len(usage['minute'])\n        if rpm_count >= limits['rpm']:\n            return False\n            \n        tokens_used = sum(t[1] for t in usage['tokens_minute'])\n        if tokens_used + estimated_tokens > limits['tpm']:\n            return False\n            \n        rpd_count = len(usage['day'])\n        if rpd_count >= limits['rpd']:\n            return False\n            \n        return True\n    \n    def _clean_old_entries(self, usage, current_time):\n        \"\"\"Remove old tracking entries.\"\"\"\n        # Clean minute entries (older than 60 seconds)\n        while usage['minute'] and usage['minute'][0] < current_time - 60:\n            usage['minute'].popleft()\n            \n        # Clean token entries  \n        while usage['tokens_minute'] and usage['tokens_minute'][0][0] < current_time - 60:\n            usage['tokens_minute'].popleft()\n            \n        # Clean day entries (older than 24 hours)\n        while usage['day'] and usage['day'][0] < current_time - 86400:\n            usage['day'].popleft()\n    \n    def record_usage(self, model_name, tokens_used):\n        \"\"\"Record that we used the API.\"\"\"\n        current_time = time.time()\n        usage = self.requests[model_name]\n        \n        usage['minute'].append(current_time)\n        usage['day'].append(current_time)\n        usage['tokens_minute'].append((current_time, tokens_used))\n        \n    def get_best_model(self, estimated_tokens=0):\n        \"\"\"Get the best available model.\"\"\"\n        # Try preferred model first\n        if self.can_use_model(self.current_model, estimated_tokens):\n            return self.current_model\n            \n        # Try alternatives\n        for model in self.limits.keys():\n            if model != self.current_model and self.can_use_model(model, estimated_tokens):\n                print(f\"üìä Switching to {model} due to rate limits\")\n                return model\n                \n        # All models at limit\n        wait_time = self._get_wait_time()\n        print(f\"‚è≥ Rate limit reached. Please wait {wait_time} seconds...\")\n        time.sleep(wait_time)\n        return self.current_model\n        \n    def _get_wait_time(self):\n        \"\"\"Calculate how long to wait.\"\"\"\n        current_time = time.time()\n        min_wait = float('inf')\n        \n        for model_name, usage in self.requests.items():\n            if usage['minute']:\n                wait = 61 - (current_time - usage['minute'][0])\n                min_wait = min(min_wait, wait)\n                \n        return max(1, int(min_wait))\n\n# Create global rate limiter\nrate_limiter = HybridRateLimiter()\nprint(\"‚úÖ Rate limiter configured!\")\nprint(\"  ‚Ä¢ Prevents API overuse\")\nprint(\"  ‚Ä¢ Automatically switches models if needed\")\nprint(\"  ‚Ä¢ Tracks usage across sessions\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 5Ô∏è‚É£ Choose Your Podcast [REQUIRED]\n\n## Cell 5.1: Select Podcast to Process\n\n**What this does:**\n- Lets you choose which podcast to analyze\n- You can use preset podcasts or add your own\n\n**Popular Podcast Options:**\n- **My First Million**: Business and startup ideas\n- **All-In Podcast**: Tech, economics, politics, and venture capital\n- **Lex Fridman**: Deep conversations about AI, science, philosophy\n- **Tim Ferriss Show**: Productivity, learning, high performers\n- **Joe Rogan Experience**: Wide-ranging conversations\n- **Huberman Lab**: Science-based health and performance\n\n**How to add your own podcast:**\n1. Find the podcast's RSS feed URL\n2. Add it to the custom podcast section\n3. Give it a short name\n\n**Tip:** Start with just 1-2 episodes to test!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Podcast Selection\n# Pre-configured popular podcasts\nPODCAST_FEEDS = {\n    'my-first-million': 'https://feeds.megaphone.fm/HSW7835889191',\n    'all-in': 'https://feeds.megaphone.fm/all-in-with-chamath-jason-sacks-friedberg',\n    'lex-fridman': 'https://lexfridman.com/feed/podcast/',\n    'tim-ferriss': 'https://rss.art19.com/tim-ferriss-show',\n    'huberman-lab': 'https://feeds.megaphone.fm/hubermanlab',\n    'joe-rogan': 'https://spotifeed.timdorr.com/4rOoJ6Egrf8K2IrywzwOMk',\n    'masters-of-scale': 'https://rss.art19.com/masters-of-scale',\n    'how-i-built-this': 'https://feeds.npr.org/510313/podcast.xml',\n    'planet-money': 'https://feeds.npr.org/510289/podcast.xml',\n    'freakonomics': 'https://feeds.simplecast.com/Y8lFbOT4'\n}\n\n# Display available podcasts\nprint(\"üìª Available Podcasts:\")\nprint(\"-\" * 40)\nfor i, (key, name) in enumerate(PODCAST_FEEDS.items(), 1):\n    print(f\"{i}. {key}\")\n\n# Choose podcast\nprint(\"\\nüéØ Which podcast would you like to process?\")\nprint(\"Enter the podcast name from above, or 'custom' to add your own RSS feed\")\n\npodcast_choice = input(\"\\nYour choice: \").strip().lower()\n\nif podcast_choice == 'custom':\n    print(\"\\nüìù Enter custom podcast details:\")\n    custom_name = input(\"Podcast name (short, no spaces): \").strip()\n    custom_rss = input(\"RSS feed URL: \").strip()\n    PODCAST_FEEDS[custom_name] = custom_rss\n    podcast_choice = custom_name\n\n# Validate choice\nif podcast_choice in PODCAST_FEEDS:\n    SELECTED_PODCAST = podcast_choice\n    SELECTED_RSS = PODCAST_FEEDS[podcast_choice]\n    print(f\"\\n‚úÖ Selected: {SELECTED_PODCAST}\")\n    print(f\"üì° RSS Feed: {SELECTED_RSS}\")\nelse:\n    print(\"‚ùå Invalid choice. Using 'my-first-million' as default\")\n    SELECTED_PODCAST = 'my-first-million'\n    SELECTED_RSS = PODCAST_FEEDS['my-first-million']\n\n# How many episodes?\nprint(f\"\\nüìä How many episodes to process? (1-{PodcastConfig.MAX_EPISODES})\")\nnum_episodes = input(f\"Number of episodes [default: 1]: \").strip()\ntry:\n    NUM_EPISODES = min(int(num_episodes), PodcastConfig.MAX_EPISODES) if num_episodes else 1\nexcept:\n    NUM_EPISODES = 1\n\nprint(f\"\\nüéØ Will process {NUM_EPISODES} episode(s) from '{SELECTED_PODCAST}'\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 6Ô∏è‚É£ Quick Process Functions [REQUIRED]\n\n## Simple Processing Functions\n\n**Instead of loading all the complex classes**, we'll create simplified functions that:\n- Download and process your selected podcast\n- Extract insights and store them in Neo4j\n- Show you progress along the way\n\n**This approach is:**\n- ‚úÖ Easier to understand\n- ‚úÖ Less likely to have errors\n- ‚úÖ Perfect for getting started\n\n**The functions below will:**\n1. Download podcast episodes\n2. Transcribe them (if audio processing is enabled)\n3. Extract insights using AI\n4. Store everything in your Neo4j database",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def download_episode_metadata(rss_url, max_episodes=5):\n    \"\"\"Download podcast episode information.\"\"\"\n    print(f\"üì• Fetching podcast feed...\")\n    \n    try:\n        feed = feedparser.parse(rss_url)\n        episodes = []\n        \n        for i, entry in enumerate(feed.entries[:max_episodes]):\n            episode = {\n                'title': entry.get('title', f'Episode {i+1}'),\n                'url': entry.enclosures[0].href if entry.get('enclosures') else None,\n                'description': entry.get('summary', ''),\n                'published': entry.get('published', ''),\n                'duration': entry.get('itunes_duration', 'Unknown'),\n                'episode_number': i + 1\n            }\n            episodes.append(episode)\n            print(f\"  ‚úì Found: {episode['title'][:50]}...\")\n            \n        return episodes\n    except Exception as e:\n        print(f\"‚ùå Error fetching feed: {e}\")\n        return []\n\ndef simple_transcribe(audio_file):\n    \"\"\"Simple transcription using Whisper.\"\"\"\n    print(f\"üéØ Transcribing: {os.path.basename(audio_file)}\")\n    \n    try:\n        if WhisperModel:\n            # Use faster-whisper\n            model = WhisperModel(\"base\", device=\"cuda\" if PodcastConfig.USE_GPU else \"cpu\")\n            segments, _ = model.transcribe(audio_file)\n            \n            transcript = []\n            for segment in segments:\n                transcript.append({\n                    'text': segment.text,\n                    'start': segment.start,\n                    'end': segment.end\n                })\n            return transcript\n        else:\n            print(\"‚ö†Ô∏è Whisper not available, using mock transcript\")\n            return [{'text': 'Mock transcript for testing', 'start': 0, 'end': 60}]\n            \n    except Exception as e:\n        print(f\"‚ùå Transcription error: {e}\")\n        return []\n\ndef extract_simple_insights(transcript_text, episode_title):\n    \"\"\"Extract insights using Gemini AI.\"\"\"\n    print(\"üß† Extracting insights with AI...\")\n    \n    try:\n        if not ChatGoogleGenerativeAI:\n            print(\"‚ö†Ô∏è AI not available, using mock insights\")\n            return {\n                'insights': ['This is a test insight about the podcast'],\n                'entities': ['Test Company', 'Test Person'],\n                'topics': ['business', 'technology']\n            }\n            \n        # Use Gemini to extract insights\n        model_name = rate_limiter.get_best_model(estimated_tokens=len(transcript_text)//4)\n        llm = ChatGoogleGenerativeAI(\n            model=model_name,\n            temperature=0.3,\n            google_api_key=os.environ.get('GOOGLE_API_KEY')\n        )\n        \n        # Simple prompt\n        prompt = f\\\"\\\"\\\"\n        Analyze this podcast transcript and extract:\n        1. Key insights (main ideas, lessons, advice)\n        2. Important entities (people, companies, products)\n        3. Main topics discussed\n        \n        Episode: {episode_title}\n        Transcript: {transcript_text[:4000]}...\n        \n        Return as JSON with keys: insights, entities, topics\n        \\\"\\\"\\\"\n        \n        response = llm.invoke(prompt)\n        \n        # Parse response (simple approach)\n        try:\n            # Try to extract JSON from response\n            import re\n            json_match = re.search(r'\\\\{.*\\\\}', response.content, re.DOTALL)\n            if json_match:\n                return json.loads(json_match.group())\n            else:\n                # Fallback parsing\n                return {\n                    'insights': [response.content[:200]],\n                    'entities': [],\n                    'topics': []\n                }\n        except:\n            return {\n                'insights': [response.content[:200]],\n                'entities': [],\n                'topics': []\n            }\n            \n    except Exception as e:\n        print(f\"‚ùå AI extraction error: {e}\")\n        return {\n            'insights': ['Error extracting insights'],\n            'entities': [],\n            'topics': []\n        }\n\ndef store_in_neo4j(episode_data, insights_data):\n    \\\"\\\"\\\"Store episode and insights in Neo4j database.\\\"\\\"\\\"\n    print(\"üíæ Storing in knowledge graph...\")\n    \n    try:\n        with Neo4jManager() as driver:\n            with driver.session() as session:\n                # Create episode node\n                episode_query = \\\"\\\"\\\"\n                MERGE (e:Episode {episode_id: $episode_id})\n                SET e.title = $title,\n                    e.podcast_name = $podcast_name,\n                    e.description = $description,\n                    e.published = $published,\n                    e.processed_at = datetime()\n                RETURN e\n                \\\"\\\"\\\"\n                \n                episode_id = f\"{SELECTED_PODCAST}_{episode_data['episode_number']}\"\n                session.run(episode_query, \n                           episode_id=episode_id,\n                           title=episode_data['title'],\n                           podcast_name=SELECTED_PODCAST,\n                           description=episode_data['description'][:500],\n                           published=episode_data['published'])\n                \n                # Store insights\n                for insight in insights_data.get('insights', []):\n                    insight_query = \\\"\\\"\\\"\n                    CREATE (i:Insight {\n                        title: $title,\n                        description: $title,\n                        episode_id: $episode_id\n                    })\n                    WITH i\n                    MATCH (e:Episode {episode_id: $episode_id})\n                    CREATE (i)-[:FROM_EPISODE]->(e)\n                    \\\"\\\"\\\"\n                    session.run(insight_query, \n                               title=insight[:200],\n                               episode_id=episode_id)\n                \n                # Store entities\n                for entity in insights_data.get('entities', []):\n                    entity_query = \\\"\\\"\\\"\n                    MERGE (ent:Entity {name: $name})\n                    WITH ent\n                    MATCH (e:Episode {episode_id: $episode_id})\n                    MERGE (ent)-[:MENTIONED_IN]->(e)\n                    \\\"\\\"\\\"\n                    session.run(entity_query,\n                               name=entity,\n                               episode_id=episode_id)\n                \n                print(f\"  ‚úì Stored {len(insights_data.get('insights', []))} insights\")\n                print(f\"  ‚úì Stored {len(insights_data.get('entities', []))} entities\")\n                \n    except Exception as e:\n        print(f\"‚ùå Database storage error: {e}\")\n\nprint(\"‚úÖ Processing functions ready!\")\nprint(\"  ‚Ä¢ Download episodes\")\nprint(\"  ‚Ä¢ Transcribe audio (if enabled)\")\nprint(\"  ‚Ä¢ Extract insights with AI\")\nprint(\"  ‚Ä¢ Store in knowledge graph\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 7Ô∏è‚É£ Process Your Podcast! [MAIN EXECUTION]\n\n## This is where the magic happens!\n\n**What this cell does:**\n1. Downloads your selected podcast episodes\n2. Processes each episode through the pipeline\n3. Extracts insights and stores them in Neo4j\n4. Shows you progress along the way\n\n**Processing time:**\n- With transcription: ~5-10 minutes per episode\n- Without transcription: ~1-2 minutes per episode\n\n**What you'll see:**\n- Progress updates for each step\n- Summary of what was found\n- Any errors (usually recoverable)\n\n**Ready? Run the cell below to start!** üöÄ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def process_podcast_simple():\n    \"\"\"Main function to process your selected podcast.\"\"\"\n    \n    print(f\"üéôÔ∏è STARTING PODCAST PROCESSING\")\n    print(f\"üìª Podcast: {SELECTED_PODCAST}\")\n    print(f\"üìä Episodes to process: {NUM_EPISODES}\")\n    print(f\"‚öôÔ∏è Audio processing: {'ON' if ENABLE_AUDIO_PROCESSING else 'OFF'}\")\n    print(\"-\" * 50)\n    \n    # Get episode metadata\n    episodes = download_episode_metadata(SELECTED_RSS, NUM_EPISODES)\n    \n    if not episodes:\n        print(\"‚ùå No episodes found. Check the RSS feed URL.\")\n        return\n    \n    print(f\"\\n‚úÖ Found {len(episodes)} episodes\")\n    \n    # Process each episode\n    total_insights = 0\n    total_entities = 0\n    \n    for i, episode in enumerate(episodes, 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"üìù Processing Episode {i}/{len(episodes)}: {episode['title'][:60]}...\")\n        print(f\"{'='*60}\")\n        \n        try:\n            # Option 1: Use transcript from description (faster)\n            if not ENABLE_AUDIO_PROCESSING:\n                print(\"üìÑ Using episode description as text source...\")\n                transcript_text = episode['description']\n            \n            # Option 2: Download and transcribe audio (slower but better)\n            else:\n                print(\"üéµ Audio processing enabled - this will take longer...\")\n                # For demo, we'll use description as fallback\n                # In full implementation, this would download and transcribe\n                transcript_text = episode['description']\n                print(\"  ‚ÑπÔ∏è Using description as demo (full audio processing not implemented in simple version)\")\n            \n            # Extract insights\n            insights = extract_simple_insights(transcript_text, episode['title'])\n            \n            # Store in Neo4j\n            store_in_neo4j(episode, insights)\n            \n            # Track totals\n            total_insights += len(insights.get('insights', []))\n            total_entities += len(insights.get('entities', []))\n            \n            print(f\"\\n‚úÖ Episode {i} complete!\")\n            \n            # Clean memory periodically\n            if i % 2 == 0:\n                cleanup_memory()\n                \n        except Exception as e:\n            print(f\"\\n‚ö†Ô∏è Error processing episode {i}: {e}\")\n            print(\"Continuing with next episode...\")\n            continue\n    \n    # Final summary\n    print(f\"\\n{'='*60}\")\n    print(f\"üéâ PROCESSING COMPLETE!\")\n    print(f\"{'='*60}\")\n    print(f\"üìä Summary:\")\n    print(f\"  ‚Ä¢ Episodes processed: {len(episodes)}\")\n    print(f\"  ‚Ä¢ Total insights extracted: {total_insights}\")\n    print(f\"  ‚Ä¢ Total entities found: {total_entities}\")\n    print(f\"  ‚Ä¢ Podcast: {SELECTED_PODCAST}\")\n    print(f\"\\nüí° Your knowledge graph has been updated!\")\n    print(f\"üîç You can now query your Neo4j database to explore the insights\")\n\n# Run the processing!\nprint(\"üöÄ Starting processing in 3 seconds...\")\nprint(\"   (Press Ctrl+C to cancel)\\n\")\ntime.sleep(3)\n\nprocess_podcast_simple()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 8Ô∏è‚É£ Explore Your Results [OPTIONAL]\n\n## Now that your podcasts are processed, let's explore what we found!\n\n### Cell 8.1: View Stored Episodes\n\n**What this does:**\n- Shows all episodes stored in your knowledge graph\n- Lists basic information about each episode\n\n**Why use it:**\n- Verify your episodes were processed\n- See what's in your database\n- Get episode IDs for further queries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# View stored episodes\nprint(\"üìö Episodes in your knowledge graph:\\n\")\n\ntry:\n    with Neo4jManager() as driver:\n        with driver.session() as session:\n            # Query all episodes\n            result = session.run(\"\"\"\n                MATCH (e:Episode)\n                RETURN e.title as title, \n                       e.podcast_name as podcast,\n                       e.episode_id as id,\n                       e.processed_at as processed_at\n                ORDER BY e.processed_at DESC\n                LIMIT 20\n            \"\"\")\n            \n            episodes = list(result)\n            \n            if episodes:\n                for i, record in enumerate(episodes, 1):\n                    print(f\"{i}. {record['title'][:60]}...\")\n                    print(f\"   üìª Podcast: {record['podcast']}\")\n                    print(f\"   üÜî ID: {record['id']}\")\n                    print(f\"   üìÖ Processed: {record['processed_at']}\")\n                    print()\n            else:\n                print(\"No episodes found. Process a podcast first!\")\n                \nexcept Exception as e:\n    print(f\"Error querying database: {e}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Cell 8.2: View Top Insights\n\n**What this does:**\n- Shows the most recent insights extracted from podcasts\n- Displays which episode each insight came from\n\n**This helps you:**\n- See the key takeaways found\n- Verify the AI extraction is working\n- Find interesting insights quickly",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# View top insights\nprint(\"üí° Recent Insights:\\n\")\n\ntry:\n    with Neo4jManager() as driver:\n        with driver.session() as session:\n            # Query insights with their episodes\n            result = session.run(\"\"\"\n                MATCH (i:Insight)-[:FROM_EPISODE]->(e:Episode)\n                RETURN i.title as insight,\n                       e.title as episode,\n                       e.podcast_name as podcast\n                ORDER BY e.processed_at DESC\n                LIMIT 10\n            \"\"\")\n            \n            insights = list(result)\n            \n            if insights:\n                for i, record in enumerate(insights, 1):\n                    print(f\"{i}. üí≠ {record['insight'][:100]}...\")\n                    print(f\"   üìù From: {record['episode'][:50]}...\")\n                    print(f\"   üìª Podcast: {record['podcast']}\")\n                    print()\n            else:\n                print(\"No insights found yet. Process a podcast to see insights!\")\n                \nexcept Exception as e:\n    print(f\"Error querying insights: {e}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Cell 8.3: Find Connected Entities\n\n**What this does:**\n- Shows people, companies, and topics mentioned in podcasts\n- Displays how many times each was mentioned\n\n**This helps you discover:**\n- Most discussed companies or people\n- Common topics across episodes\n- Potential connections between episodes",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# View top entities\nprint(\"üè¢ Most Mentioned Entities:\\n\")\n\ntry:\n    with Neo4jManager() as driver:\n        with driver.session() as session:\n            # Query entities and their mention count\n            result = session.run(\"\"\"\n                MATCH (ent:Entity)-[:MENTIONED_IN]->(e:Episode)\n                RETURN ent.name as entity,\n                       COUNT(DISTINCT e) as mention_count,\n                       COLLECT(DISTINCT e.podcast_name)[0..3] as podcasts\n                ORDER BY mention_count DESC\n                LIMIT 15\n            \"\"\")\n            \n            entities = list(result)\n            \n            if entities:\n                for i, record in enumerate(entities, 1):\n                    print(f\"{i}. {record['entity']}\")\n                    print(f\"   üìä Mentioned in {record['mention_count']} episode(s)\")\n                    print(f\"   üìª Podcasts: {', '.join(record['podcasts'])}\")\n                    print()\n            else:\n                print(\"No entities found yet. Process a podcast to see entities!\")\n                \nexcept Exception as e:\n    print(f\"Error querying entities: {e}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# 9Ô∏è‚É£ Custom Queries [ADVANCED]\n\n## Write your own Neo4j queries!\n\n**If you know Cypher (Neo4j's query language)**, you can write custom queries here.\n\n**Example queries:**\n- Find all insights about a specific topic\n- Find episodes where two people are mentioned together\n- Find the most connected entities\n\n**New to Cypher?** Check out:\n- [Neo4j Cypher Manual](https://neo4j.com/docs/cypher-manual/current/)\n- [Cypher Cheat Sheet](https://neo4j.com/docs/cypher-cheat-sheet/current/)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Custom query example - modify as needed!\ncustom_query = \"\"\"\n// Example: Find insights containing specific keywords\nMATCH (i:Insight)-[:FROM_EPISODE]->(e:Episode)\nWHERE toLower(i.title) CONTAINS 'business' \n   OR toLower(i.title) CONTAINS 'startup'\nRETURN i.title as insight, e.title as episode\nLIMIT 10\n\"\"\"\n\nprint(\"üîç Running custom query...\\n\")\n\ntry:\n    with Neo4jManager() as driver:\n        with driver.session() as session:\n            result = session.run(custom_query)\n            \n            records = list(result)\n            if records:\n                for record in records:\n                    print(f\"‚Ä¢ {dict(record)}\")\n            else:\n                print(\"No results found for this query.\")\n                \nexcept Exception as e:\n    print(f\"Query error: {e}\")\n    \nprint(\"\\nüí° Tip: Modify the 'custom_query' variable above to run your own queries!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n# üéâ Congratulations!\n\n## You've successfully set up a podcast knowledge extraction system!\n\n### What you've accomplished:\n‚úÖ Connected to Neo4j knowledge graph  \n‚úÖ Configured AI services for insight extraction  \n‚úÖ Processed podcast episodes  \n‚úÖ Extracted and stored insights  \n‚úÖ Created a searchable knowledge base  \n\n### Next Steps:\n\n1. **Process More Podcasts**\n   - Go back to Cell 5.1 to select different podcasts\n   - Increase the number of episodes to process\n\n2. **Explore Your Data**\n   - Use the query cells to find interesting patterns\n   - Open Neo4j Browser to visualize your graph\n\n3. **Enhance the System**\n   - Enable audio transcription for better results\n   - Add more sophisticated insight extraction\n   - Create custom visualizations\n\n4. **Share Your Knowledge**\n   - Export insights to share with others\n   - Build applications on top of your knowledge graph\n\n### Useful Resources:\n- üìö [Neo4j Documentation](https://neo4j.com/docs/)\n- ü§ñ [Google AI Studio](https://makersuite.google.com/)\n- üéôÔ∏è [Podcast RSS Feeds Directory](https://podcastaddict.com/submit)\n\n### Troubleshooting:\n- **Rate Limits**: Wait a few minutes if you hit API limits\n- **Memory Issues**: Restart runtime and process fewer episodes\n- **Connection Errors**: Check your internet and API keys\n\n### Need Help?\n- Check the error messages for specific guidance\n- Most issues can be resolved by re-running cells\n- Make sure all required cells have been run in order\n\n**Thank you for using the Podcast Knowledge System!** üöÄ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Complete System Imports [REQUIRED]\n# This cell imports ALL libraries needed for the full podcast knowledge system\n\n# Standard Library Imports\nimport os\nimport re\nimport json\nimport time\nimport hashlib\nimport urllib.request\nfrom datetime import datetime, timedelta\nfrom urllib.parse import urlparse\nfrom collections import defaultdict, deque\nimport argparse\nimport gc\nimport logging\nimport sys\nimport difflib  # For fuzzy matching in entity resolution\nfrom itertools import combinations\nimport math\nfrom typing import List, Dict, Any, Optional, Tuple, Set\nimport subprocess  # For GPU monitoring\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Core Dependencies\ntry:\n    import torch\n    print(\"‚úÖ PyTorch loaded (GPU acceleration available)\")\n    # Check GPU availability\n    if torch.cuda.is_available():\n        print(f\"  üöÄ GPU detected: {torch.cuda.get_device_name(0)}\")\n        print(f\"  üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    else:\n        print(\"  ‚ÑπÔ∏è No GPU detected, using CPU\")\nexcept ImportError:\n    print(\"‚ö†Ô∏è PyTorch not available. Some features may be limited.\")\n    torch = None\n\ntry:\n    from neo4j import GraphDatabase\n    print(\"‚úÖ Neo4j driver loaded\")\nexcept ImportError:\n    raise ImportError(\"‚ùå Neo4j driver required. Please run the installation cell.\")\n\ntry:\n    from dotenv import load_dotenv\n    load_dotenv()\n    print(\"‚úÖ Environment variables loaded\")\nexcept ImportError:\n    print(\"‚ÑπÔ∏è python-dotenv not available. Using environment variables directly.\")\n\n# Scientific Computing\ntry:\n    import numpy as np\n    from scipy.stats import entropy\n    from scipy.spatial.distance import cosine\n    import networkx as nx\n    from networkx.algorithms import community\n    print(\"‚úÖ Scientific computing libraries loaded\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Some scientific libraries missing: {e}\")\n\n# Machine Learning\ntry:\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.cluster import KMeans, DBSCAN\n    from sklearn.decomposition import PCA\n    print(\"‚úÖ Machine learning libraries loaded\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Some ML libraries missing: {e}\")\n\n# AI and LLM Dependencies\ntry:\n    from langchain_google_genai import ChatGoogleGenerativeAI\n    from langchain.prompts import ChatPromptTemplate\n    from langchain.schema import SystemMessage, HumanMessage\n    print(\"‚úÖ Google AI libraries loaded\")\nexcept ImportError:\n    print(\"‚ö†Ô∏è Google AI libraries not available\")\n    ChatGoogleGenerativeAI = None\n\ntry:\n    from openai import OpenAI\n    print(\"‚úÖ OpenAI library loaded\")\nexcept ImportError:\n    print(\"‚ÑπÔ∏è OpenAI not available (optional for embeddings)\")\n    OpenAI = None\n\n# Audio Processing\ntry:\n    from faster_whisper import WhisperModel\n    print(\"‚úÖ Whisper (speech-to-text) loaded\")\n    WhisperModel = WhisperModel\nexcept ImportError:\n    try:\n        import whisper\n        print(\"‚úÖ Alternative Whisper loaded\")\n        WhisperModel = None\n    except:\n        print(\"‚ö†Ô∏è No speech-to-text available\")\n        WhisperModel = None\n        whisper = None\n\ntry:\n    from pyannote.audio import Pipeline\n    print(\"‚úÖ Speaker diarization loaded\")\nexcept ImportError:\n    print(\"‚ö†Ô∏è Speaker diarization not available\")\n    Pipeline = None\n\n# Data Processing\ntry:\n    import feedparser\n    print(\"‚úÖ RSS feed parser loaded\")\nexcept ImportError:\n    print(\"‚ö†Ô∏è Feed parser not available\")\n    feedparser = None\n\ntry:\n    import pandas as pd\n    print(\"‚úÖ Pandas loaded\")\nexcept ImportError:\n    print(\"‚ÑπÔ∏è Pandas not available (optional)\")\n    pd = None\n\n# Visualization\ntry:\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n    from matplotlib.colors import LinearSegmentedColormap\n    import seaborn as sns\n    print(\"‚úÖ Visualization libraries loaded\")\n    # Set style for better-looking plots\n    plt.style.use('seaborn-v0_8-darkgrid')\n    sns.set_palette(\"husl\")\nexcept ImportError:\n    print(\"‚ÑπÔ∏è Visualization not available (optional)\")\n    plt = None\n    patches = None\n    LinearSegmentedColormap = None\n    sns = None\n\n# Progress tracking\ntry:\n    from tqdm import tqdm\n    # Use notebook version if in Colab\n    if 'google.colab' in sys.modules:\n        from tqdm.notebook import tqdm\n    print(\"‚úÖ Progress bars loaded\")\nexcept ImportError:\n    print(\"‚ÑπÔ∏è Progress bars not available\")\n    # Fallback progress tracker\n    class tqdm:\n        def __init__(self, iterable=None, desc=\"\", total=None):\n            self.iterable = iterable\n            self.desc = desc\n            self.total = total or (len(iterable) if iterable else 0)\n            self.current = 0\n            \n        def __iter__(self):\n            for item in self.iterable:\n                yield item\n                self.current += 1\n                if self.current % max(1, self.total // 10) == 0:\n                    print(f\"{self.desc}: {self.current}/{self.total}\")\n                    \n        def update(self, n=1):\n            self.current += n\n            if self.current % max(1, self.total // 10) == 0:\n                print(f\"{self.desc}: {self.current}/{self.total}\")\n\n# Memory monitoring\ntry:\n    import psutil\n    print(\"‚úÖ System monitoring loaded\")\nexcept ImportError:\n    print(\"‚ÑπÔ∏è System monitoring not available\")\n    psutil = None\n\n# Additional utilities\ntry:\n    import regex\n    print(\"‚úÖ Advanced regex loaded\")\nexcept ImportError:\n    print(\"‚ÑπÔ∏è Using standard regex\")\n    regex = re\n\ntry:\n    from rapidfuzz import fuzz\n    print(\"‚úÖ Fuzzy matching loaded\")\nexcept ImportError:\n    print(\"‚ÑπÔ∏è Fuzzy matching not available\")\n    fuzz = None\n\ntry:\n    from tenacity import retry, stop_after_attempt, wait_exponential\n    print(\"‚úÖ Retry logic loaded\")\nexcept ImportError:\n    print(\"‚ÑπÔ∏è Retry logic not available\")\n    retry = None\n\nprint(\"\\nüéâ All imports completed successfully!\")\nprint(\"üìä System ready for podcast knowledge extraction\")"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}