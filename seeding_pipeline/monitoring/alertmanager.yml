global:
  # Global configuration
  resolve_timeout: 5m
  
  # Slack configuration
  slack_api_url: '${SLACK_WEBHOOK_URL}'
  
  # PagerDuty configuration
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

# Templates for notifications
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree
route:
  # Default receiver
  receiver: 'default-receiver'
  
  # Group alerts by these labels
  group_by: ['alertname', 'severity', 'team']
  
  # Wait before sending initial notification
  group_wait: 30s
  
  # Wait before sending notification about new alerts in group
  group_interval: 5m
  
  # Wait before re-sending notification
  repeat_interval: 4h
  
  # Child routes
  routes:
    # Critical alerts go to PagerDuty
    - match:
        severity: critical
      receiver: critical-receiver
      continue: true
      
    # Platform team alerts
    - match:
        team: platform
      receiver: platform-receiver
      routes:
        - match:
            severity: critical
          receiver: platform-pagerduty
          
    # Data team alerts
    - match:
        team: data
      receiver: data-receiver
      
    # SLO alerts
    - match_re:
        alertname: '^SLO.*'
      receiver: slo-receiver

# Notification receivers
receivers:
  - name: 'default-receiver'
    slack_configs:
      - channel: '#alerts'
        title: 'Podcast KG Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        send_resolved: true

  - name: 'critical-receiver'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
        description: '{{ .GroupLabels.alertname }}: {{ .Annotations.summary }}'
        severity: 'critical'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          description: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    slack_configs:
      - channel: '#alerts-critical'
        title: 'ðŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        color: 'danger'

  - name: 'platform-receiver'
    slack_configs:
      - channel: '#platform-alerts'
        title: 'Platform Alert: {{ .GroupLabels.alertname }}'
        text: |
          *Summary:* {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
          *Description:* {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
          *Runbook:* {{ range .Alerts }}{{ .Annotations.runbook }}{{ end }}
        send_resolved: true
        actions:
          - type: button
            text: 'View Dashboard'
            url: 'https://grafana.example.com/d/podcast-kg'
          - type: button
            text: 'Runbook'
            url: '{{ range .Alerts }}{{ .Annotations.runbook }}{{ end }}'

  - name: 'platform-pagerduty'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_PLATFORM_KEY}'
        description: '{{ .GroupLabels.alertname }}'

  - name: 'data-receiver'
    slack_configs:
      - channel: '#data-alerts'
        title: 'Data Pipeline Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
    email_configs:
      - to: 'data-team@example.com'
        from: 'alerts@example.com'
        smarthost: 'smtp.example.com:587'
        auth_username: 'alerts@example.com'
        auth_password: '${SMTP_PASSWORD}'
        headers:
          Subject: 'Podcast KG Alert: {{ .GroupLabels.alertname }}'

  - name: 'slo-receiver'
    slack_configs:
      - channel: '#slo-alerts'
        title: 'SLO Alert: {{ .GroupLabels.alertname }}'
        text: |
          *SLO:* {{ .GroupLabels.slo }}
          *Current Value:* {{ .Alerts.Firing.Value }}
          *Description:* {{ range .Alerts }}{{ .Annotations.description }}{{ end }}
        color: 'warning'

# Inhibition rules
inhibit_rules:
  # Inhibit warning alerts if critical alert is firing for same alertname
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'team']
    
  # Inhibit all alerts if service is down
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*'
    equal: ['job']