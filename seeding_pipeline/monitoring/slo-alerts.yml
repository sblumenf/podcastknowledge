# SLO-based Alert Rules
# These alerts focus on error budget burn rates and SLO violations

groups:
  - name: slo-error-budget-alerts
    interval: 1m
    rules:
      # Multi-window multi-burn rate alerts (recommended by Google SRE)
      
      # Fast burn alerts (Page immediately)
      - alert: ErrorBudgetFastBurn
        expr: |
          (
            error_budget:availability:burn_rate_1h > 0.02
            and
            error_budget:availability:burn_rate_6h > 0.01
          )
        for: 2m
        labels:
          severity: critical
          team: on-call
          slo: availability
          burn_type: fast
        annotations:
          summary: "Fast error budget burn for availability SLO"
          description: |
            Error budget is burning fast:
            - 1h burn rate: {{ $value | humanizePercentage }}
            - Consuming {{ $value | humanize }}% of monthly budget per hour
            - At this rate, budget exhausted in {{ 100 / $value }} hours
          runbook: "https://wiki.example.com/runbooks/slo/fast-burn"
          dashboard: "https://grafana.example.com/d/podcast-kg-slo"

      # Slow burn alerts (Create ticket)
      - alert: ErrorBudgetSlowBurn
        expr: |
          (
            error_budget:availability:burn_rate_6h > 0.05
            and
            error_budget:availability:burn_rate_24h > 0.02
          )
        for: 30m
        labels:
          severity: warning
          team: data
          slo: availability
          burn_type: slow
        annotations:
          summary: "Slow error budget burn for availability SLO"
          description: |
            Error budget is burning slowly but consistently:
            - 6h burn rate: {{ $value | humanizePercentage }}
            - 24h burn rate: {{ $labels.burn_rate_24h | humanizePercentage }}
          runbook: "https://wiki.example.com/runbooks/slo/slow-burn"

      # Budget exhaustion alerts
      - alert: ErrorBudgetLow
        expr: error_budget:availability:remaining_percent < 20
        for: 5m
        labels:
          severity: warning
          team: data
          slo: availability
        annotations:
          summary: "Low error budget remaining ({{ $value }}%)"
          description: |
            Only {{ $value }}% of error budget remains for this month.
            Non-critical changes should be frozen.
          action: "Freeze non-critical deployments"
          runbook: "https://wiki.example.com/runbooks/slo/low-budget"

      - alert: ErrorBudgetCriticallyLow
        expr: error_budget:availability:remaining_percent < 10
        for: 2m
        labels:
          severity: critical
          team: data
          slo: availability
        annotations:
          summary: "Critically low error budget ({{ $value }}%)"
          description: |
            Error budget is critically low at {{ $value }}%.
            All changes should be frozen except emergency fixes.
          action: "Freeze all non-emergency changes"
          runbook: "https://wiki.example.com/runbooks/slo/critical-budget"

      - alert: ErrorBudgetExhausted
        expr: error_budget:availability:remaining_percent <= 0
        for: 1m
        labels:
          severity: page
          team: on-call
          slo: availability
        annotations:
          summary: "Error budget exhausted for availability SLO"
          description: |
            Error budget has been completely exhausted.
            Immediate action required to restore service reliability.
          action: "Initiate incident response, freeze all changes"
          runbook: "https://wiki.example.com/runbooks/slo/budget-exhausted"

  - name: slo-latency-alerts
    interval: 30s
    rules:
      # Episode processing latency SLO
      - alert: EpisodeProcessingLatencySLOViolation
        expr: |
          (
            sum(rate(podcast_kg_processing_duration_seconds_bucket{stage="full_episode",le="300"}[5m])) / 
            sum(rate(podcast_kg_processing_duration_seconds_count{stage="full_episode"}[5m]))
          ) < 0.95
        for: 10m
        labels:
          severity: warning
          team: data
          slo: episode_latency
        annotations:
          summary: "Episode processing latency SLO violation"
          description: |
            Less than 95% of episodes are being processed within 5 minutes.
            Current success rate: {{ $value | humanizePercentage }}
          runbook: "https://wiki.example.com/runbooks/slo/latency-violation"

      # API latency SLO
      - alert: APILatencySLOViolation
        expr: |
          (
            sum(rate(http_request_duration_seconds_bucket{job="podcast-kg-pipeline",le="1"}[5m])) / 
            sum(rate(http_request_duration_seconds_count{job="podcast-kg-pipeline"}[5m]))
          ) < 0.99
        for: 5m
        labels:
          severity: warning
          team: platform
          slo: api_latency
        annotations:
          summary: "API latency SLO violation"
          description: |
            Less than 99% of API requests are completing within 1 second.
            Current success rate: {{ $value | humanizePercentage }}
          runbook: "https://wiki.example.com/runbooks/slo/api-latency"

  - name: slo-throughput-alerts
    interval: 1m
    rules:
      - alert: ThroughputSLOViolation
        expr: |
          sum(rate(podcast_kg_episodes_processed_total[1h])) * 3600 < 50
        for: 30m
        labels:
          severity: warning
          team: data
          slo: throughput
        annotations:
          summary: "Throughput below SLO target"
          description: |
            Processing throughput is {{ $value }} episodes/hour, below target of 50.
            This may indicate processing bottlenecks or resource constraints.
          runbook: "https://wiki.example.com/runbooks/slo/low-throughput"

  - name: slo-quality-alerts
    interval: 5m
    rules:
      # Transcription quality SLO
      - alert: TranscriptionQualitySLOViolation
        expr: |
          histogram_quantile(0.5,
            sum(rate(podcast_kg_extraction_quality_score_bucket{type="transcription"}[1h]))
            by (le)
          ) < 0.85
        for: 1h
        labels:
          severity: warning
          team: data
          slo: transcription_quality
        annotations:
          summary: "Transcription quality below SLO"
          description: |
            Median transcription quality score is {{ $value }}, below target of 0.85.
            This may indicate issues with audio quality or transcription service.
          runbook: "https://wiki.example.com/runbooks/slo/quality-degradation"

      # Entity extraction SLO
      - alert: EntityExtractionSLOViolation
        expr: |
          (
            sum(rate(podcast_kg_entities_extracted_total[1h])) /
            sum(rate(podcast_kg_episodes_processed_total[1h]))
          ) < 5
        for: 2h
        labels:
          severity: warning
          team: data
          slo: entity_extraction
        annotations:
          summary: "Entity extraction below SLO target"
          description: |
            Average entities per episode is {{ $value }}, below target of 5.
            This may indicate issues with LLM processing or prompt effectiveness.
          runbook: "https://wiki.example.com/runbooks/slo/entity-extraction"

  - name: slo-provider-alerts
    interval: 30s
    rules:
      - alert: ProviderReliabilitySLOViolation
        expr: |
          sli:provider_reliability:by_provider < 0.98
        for: 10m
        labels:
          severity: warning
          team: platform
          slo: provider_reliability
        annotations:
          summary: "Provider {{ $labels.provider }} reliability below SLO"
          description: |
            Provider {{ $labels.provider }} success rate is {{ $value | humanizePercentage }}.
            This is below the 98% SLO target.
          runbook: "https://wiki.example.com/runbooks/slo/provider-reliability"

      # Provider-specific critical thresholds
      - alert: Neo4jReliabilityCritical
        expr: |
          sli:provider_reliability:by_provider{provider="neo4j"} < 0.995
        for: 5m
        labels:
          severity: critical
          team: platform
          slo: provider_reliability
          provider: neo4j
        annotations:
          summary: "Neo4j reliability critically low"
          description: |
            Neo4j success rate is {{ $value | humanizePercentage }}, below critical threshold.
            Database connectivity issues will prevent episode processing.
          runbook: "https://wiki.example.com/runbooks/neo4j-critical"

  - name: slo-composite-alerts
    interval: 1m
    rules:
      - alert: OverallReliabilitySLOViolation
        expr: slo:composite:overall_reliability < 0.99
        for: 15m
        labels:
          severity: warning
          team: data
          slo: overall_reliability
        annotations:
          summary: "Overall service reliability below SLO"
          description: |
            Composite reliability score is {{ $value | humanizePercentage }}.
            This combines availability (60%), episode latency (30%), and API latency (10%).
          dashboard: "https://grafana.example.com/d/podcast-kg-slo"

      - alert: DataQualitySLOViolation
        expr: slo:composite:data_quality < 0.925
        for: 2h
        labels:
          severity: warning
          team: data
          slo: data_quality
        annotations:
          summary: "Overall data quality below SLO"
          description: |
            Composite data quality score is {{ $value | humanizePercentage }}.
            This combines transcription quality and entity extraction effectiveness.
          action: "Review recent LLM/transcription changes"

  - name: slo-freshness-alerts
    interval: 5m
    rules:
      - alert: DataFreshnessSLOViolation
        expr: |
          (max(time() - podcast_kg_last_processed_timestamp) / 3600) > 24
        for: 30m
        labels:
          severity: warning
          team: data
          slo: data_freshness
        annotations:
          summary: "Data freshness SLO violation"
          description: |
            Latest processed episode is {{ $value }} hours old, exceeding 24-hour target.
            This may indicate feed processing issues or backlog.
          runbook: "https://wiki.example.com/runbooks/slo/data-freshness"