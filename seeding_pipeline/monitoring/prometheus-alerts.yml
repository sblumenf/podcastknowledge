groups:
  - name: podcast-kg-alerts
    interval: 30s
    rules:
      # High-level service alerts
      - alert: ServiceDown
        expr: up{job="podcast-kg-pipeline"} == 0
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Podcast KG Pipeline service is down"
          description: "The podcast knowledge graph pipeline service has been down for more than 2 minutes."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/service-down"

      - alert: HighErrorRate
        expr: |
          rate(podcast_kg_episodes_failed_total[5m]) / 
          (rate(podcast_kg_episodes_processed_total[5m]) + rate(podcast_kg_episodes_failed_total[5m])) > 0.1
        for: 5m
        labels:
          severity: warning
          team: data
        annotations:
          summary: "High episode processing error rate ({{ $value | humanizePercentage }})"
          description: "More than 10% of episodes are failing to process in the last 5 minutes."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/high-error-rate"

      # Resource alerts
      - alert: HighMemoryUsage
        expr: |
          podcast_kg_memory_usage_bytes / (1024 * 1024 * 1024) > 6
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High memory usage ({{ $value | humanize }}GB)"
          description: "Memory usage is above 6GB for more than 5 minutes."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/high-memory"

      - alert: CriticalMemoryUsage
        expr: |
          podcast_kg_memory_usage_bytes / (1024 * 1024 * 1024) > 7.5
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Critical memory usage ({{ $value | humanize }}GB)"
          description: "Memory usage is above 7.5GB. OOM kill imminent."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/critical-memory"

      - alert: HighCPUUsage
        expr: podcast_kg_cpu_usage_percent > 80
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High CPU usage ({{ $value | humanize }}%)"
          description: "CPU usage has been above 80% for more than 10 minutes."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/high-cpu"

      # Provider alerts
      - alert: ProviderHighErrorRate
        expr: |
          rate(podcast_kg_provider_errors_total[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
          team: data
        annotations:
          summary: "High provider error rate for {{ $labels.provider }}"
          description: "Provider {{ $labels.provider }} is experiencing more than 0.5 errors per second."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/provider-errors"

      - alert: ProviderHighLatency
        expr: |
          histogram_quantile(0.95, rate(podcast_kg_provider_latency_seconds_bucket[5m])) > 5
        for: 5m
        labels:
          severity: warning
          team: data
        annotations:
          summary: "High provider latency for {{ $labels.provider }}"
          description: "95th percentile latency for provider {{ $labels.provider }} is above 5 seconds."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/provider-latency"

      # Processing performance alerts
      - alert: SlowProcessing
        expr: |
          histogram_quantile(0.95, rate(podcast_kg_processing_duration_seconds_bucket{stage="full_episode"}[5m])) > 300
        for: 10m
        labels:
          severity: warning
          team: data
        annotations:
          summary: "Slow episode processing (p95: {{ $value | humanizeDuration }})"
          description: "95th percentile episode processing time is above 5 minutes."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/slow-processing"

      - alert: ProcessingQueueBacklog
        expr: podcast_kg_queue_size > 100
        for: 30m
        labels:
          severity: warning
          team: data
        annotations:
          summary: "Large processing queue backlog ({{ $value }} episodes)"
          description: "Processing queue has more than 100 episodes waiting for more than 30 minutes."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/queue-backlog"

      # Database alerts
      - alert: Neo4jConnectionFailures
        expr: |
          rate(podcast_kg_provider_errors_total{provider="neo4j"}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Neo4j connection failures"
          description: "Neo4j is experiencing connection failures ({{ $value | humanize }} errors/sec)."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/neo4j-errors"

      # Quality alerts
      - alert: LowExtractionQuality
        expr: |
          histogram_quantile(0.5, rate(podcast_kg_extraction_quality_score_bucket[1h])) < 0.6
        for: 1h
        labels:
          severity: warning
          team: data
        annotations:
          summary: "Low extraction quality scores"
          description: "Median extraction quality score is below 0.6 for the last hour."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/low-quality"

      # Checkpoint alerts
      - alert: CheckpointAge
        expr: |
          time() - podcast_kg_last_checkpoint_timestamp > 3600
        for: 10m
        labels:
          severity: warning
          team: data
        annotations:
          summary: "Stale checkpoint data"
          description: "No checkpoint has been saved in the last hour."
          runbook: "https://wiki.example.com/runbooks/podcast-kg/checkpoint-age"

      # Capacity alerts
      - alert: HighNodeCreationRate
        expr: |
          rate(podcast_kg_nodes_created_total[5m]) > 1000
        for: 10m
        labels:
          severity: info
          team: data
        annotations:
          summary: "High graph node creation rate"
          description: "Creating more than 1000 nodes per second. Check if this is expected."

  - name: podcast-kg-slo-alerts
    interval: 1m
    rules:
      # SLO-based alerts
      - alert: SLOBudgetBurn
        expr: |
          (
            1 - (
              rate(podcast_kg_episodes_processed_total[1h]) /
              (rate(podcast_kg_episodes_processed_total[1h]) + rate(podcast_kg_episodes_failed_total[1h]))
            )
          ) > 0.01
        for: 5m
        labels:
          severity: warning
          team: data
          slo: availability
        annotations:
          summary: "SLO budget burn rate high"
          description: "Error budget is being consumed at {{ $value | humanizePercentage }} per hour."
          
      - alert: SLOViolation
        expr: |
          (
            rate(podcast_kg_episodes_processed_total[24h]) /
            (rate(podcast_kg_episodes_processed_total[24h]) + rate(podcast_kg_episodes_failed_total[24h]))
          ) < 0.99
        for: 10m
        labels:
          severity: critical
          team: data
          slo: availability
        annotations:
          summary: "SLO violation - availability below 99%"
          description: "Service availability is {{ $value | humanizePercentage }} over the last 24 hours."